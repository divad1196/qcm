<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="data:," />
    <!-- <link rel="icon" type="image/png" href="/favicon.ico"> -->
    <title>AWS Certification QCM</title>
    <script type="module" crossorigin>(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const s of document.querySelectorAll('link[rel="modulepreload"]'))n(s);new MutationObserver(s=>{for(const a of s)if(a.type==="childList")for(const i of a.addedNodes)i.tagName==="LINK"&&i.rel==="modulepreload"&&n(i)}).observe(document,{childList:!0,subtree:!0});function o(s){const a={};return s.integrity&&(a.integrity=s.integrity),s.referrerPolicy&&(a.referrerPolicy=s.referrerPolicy),s.crossOrigin==="use-credentials"?a.credentials="include":s.crossOrigin==="anonymous"?a.credentials="omit":a.credentials="same-origin",a}function n(s){if(s.ep)return;s.ep=!0;const a=o(s);fetch(s.href,a)}})();/**
* @vue/shared v3.5.16
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**//*! #__NO_SIDE_EFFECTS__ */function Oo(e){const t=Object.create(null);for(const o of e.split(","))t[o]=1;return o=>o in t}const q={},nt=[],Te=()=>{},Ys=()=>!1,Zt=e=>e.charCodeAt(0)===111&&e.charCodeAt(1)===110&&(e.charCodeAt(2)>122||e.charCodeAt(2)<97),Uo=e=>e.startsWith("onUpdate:"),ae=Object.assign,Lo=(e,t)=>{const o=e.indexOf(t);o>-1&&e.splice(o,1)},Hs=Object.prototype.hasOwnProperty,F=(e,t)=>Hs.call(e,t),T=Array.isArray,st=e=>Dt(e)==="[object Map]",Xt=e=>Dt(e)==="[object Set]",sn=e=>Dt(e)==="[object Date]",P=e=>typeof e=="function",Q=e=>typeof e=="string",De=e=>typeof e=="symbol",H=e=>e!==null&&typeof e=="object",Ln=e=>(H(e)||P(e))&&P(e.then)&&P(e.catch),Fn=Object.prototype.toString,Dt=e=>Fn.call(e),js=e=>Dt(e).slice(8,-1),zn=e=>Dt(e)==="[object Object]",Fo=e=>Q(e)&&e!=="NaN"&&e[0]!=="-"&&""+parseInt(e,10)===e,yt=Oo(",key,ref,ref_for,ref_key,onVnodeBeforeMount,onVnodeMounted,onVnodeBeforeUpdate,onVnodeUpdated,onVnodeBeforeUnmount,onVnodeUnmounted"),eo=e=>{const t=Object.create(null);return o=>t[o]||(t[o]=e(o))},Ns=/-(\w)/g,Ke=eo(e=>e.replace(Ns,(t,o)=>o?o.toUpperCase():"")),Gs=/\B([A-Z])/g,Xe=eo(e=>e.replace(Gs,"-$1").toLowerCase()),Rn=eo(e=>e.charAt(0).toUpperCase()+e.slice(1)),ho=eo(e=>e?`on${Rn(e)}`:""),_e=(e,t)=>!Object.is(e,t),Rt=(e,...t)=>{for(let o=0;o<e.length;o++)e[o](...t)},qn=(e,t,o,n=!1)=>{Object.defineProperty(e,t,{configurable:!0,enumerable:!1,writable:n,value:o})},Co=e=>{const t=parseFloat(e);return isNaN(t)?e:t};let an;const to=()=>an||(an=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});function zo(e){if(T(e)){const t={};for(let o=0;o<e.length;o++){const n=e[o],s=Q(n)?Zs(n):zo(n);if(s)for(const a in s)t[a]=s[a]}return t}else if(Q(e)||H(e))return e}const $s=/;(?![^(]*\))/g,Qs=/:([^]+)/,Js=/\/\*[^]*?\*\//g;function Zs(e){const t={};return e.replace(Js,"").split($s).forEach(o=>{if(o){const n=o.split(Qs);n.length>1&&(t[n[0].trim()]=n[1].trim())}}),t}function Ro(e){let t="";if(Q(e))t=e;else if(T(e))for(let o=0;o<e.length;o++){const n=Ro(e[o]);n&&(t+=n+" ")}else if(H(e))for(const o in e)e[o]&&(t+=o+" ");return t.trim()}const Xs="itemscope,allowfullscreen,formnovalidate,ismap,nomodule,novalidate,readonly",ea=Oo(Xs);function _n(e){return!!e||e===""}function ta(e,t){if(e.length!==t.length)return!1;let o=!0;for(let n=0;o&&n<e.length;n++)o=oo(e[n],t[n]);return o}function oo(e,t){if(e===t)return!0;let o=sn(e),n=sn(t);if(o||n)return o&&n?e.getTime()===t.getTime():!1;if(o=De(e),n=De(t),o||n)return e===t;if(o=T(e),n=T(t),o||n)return o&&n?ta(e,t):!1;if(o=H(e),n=H(t),o||n){if(!o||!n)return!1;const s=Object.keys(e).length,a=Object.keys(t).length;if(s!==a)return!1;for(const i in e){const r=e.hasOwnProperty(i),l=t.hasOwnProperty(i);if(r&&!l||!r&&l||!oo(e[i],t[i]))return!1}}return String(e)===String(t)}function Kn(e,t){return e.findIndex(o=>oo(o,t))}const Vn=e=>!!(e&&e.__v_isRef===!0),he=e=>Q(e)?e:e==null?"":T(e)||H(e)&&(e.toString===Fn||!P(e.toString))?Vn(e)?he(e.value):JSON.stringify(e,Yn,2):String(e),Yn=(e,t)=>Vn(t)?Yn(e,t.value):st(t)?{[`Map(${t.size})`]:[...t.entries()].reduce((o,[n,s],a)=>(o[po(n,a)+" =>"]=s,o),{})}:Xt(t)?{[`Set(${t.size})`]:[...t.values()].map(o=>po(o))}:De(t)?po(t):H(t)&&!T(t)&&!zn(t)?String(t):t,po=(e,t="")=>{var o;return De(e)?`Symbol(${(o=e.description)!=null?o:t})`:e};/**
* @vue/reactivity v3.5.16
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/let ce;class oa{constructor(t=!1){this.detached=t,this._active=!0,this._on=0,this.effects=[],this.cleanups=[],this._isPaused=!1,this.parent=ce,!t&&ce&&(this.index=(ce.scopes||(ce.scopes=[])).push(this)-1)}get active(){return this._active}pause(){if(this._active){this._isPaused=!0;let t,o;if(this.scopes)for(t=0,o=this.scopes.length;t<o;t++)this.scopes[t].pause();for(t=0,o=this.effects.length;t<o;t++)this.effects[t].pause()}}resume(){if(this._active&&this._isPaused){this._isPaused=!1;let t,o;if(this.scopes)for(t=0,o=this.scopes.length;t<o;t++)this.scopes[t].resume();for(t=0,o=this.effects.length;t<o;t++)this.effects[t].resume()}}run(t){if(this._active){const o=ce;try{return ce=this,t()}finally{ce=o}}}on(){++this._on===1&&(this.prevScope=ce,ce=this)}off(){this._on>0&&--this._on===0&&(ce=this.prevScope,this.prevScope=void 0)}stop(t){if(this._active){this._active=!1;let o,n;for(o=0,n=this.effects.length;o<n;o++)this.effects[o].stop();for(this.effects.length=0,o=0,n=this.cleanups.length;o<n;o++)this.cleanups[o]();if(this.cleanups.length=0,this.scopes){for(o=0,n=this.scopes.length;o<n;o++)this.scopes[o].stop(!0);this.scopes.length=0}if(!this.detached&&this.parent&&!t){const s=this.parent.scopes.pop();s&&s!==this&&(this.parent.scopes[this.index]=s,s.index=this.index)}this.parent=void 0}}}function na(){return ce}let Y;const fo=new WeakSet;class Hn{constructor(t){this.fn=t,this.deps=void 0,this.depsTail=void 0,this.flags=5,this.next=void 0,this.cleanup=void 0,this.scheduler=void 0,ce&&ce.active&&ce.effects.push(this)}pause(){this.flags|=64}resume(){this.flags&64&&(this.flags&=-65,fo.has(this)&&(fo.delete(this),this.trigger()))}notify(){this.flags&2&&!(this.flags&32)||this.flags&8||Nn(this)}run(){if(!(this.flags&1))return this.fn();this.flags|=2,rn(this),Gn(this);const t=Y,o=ye;Y=this,ye=!0;try{return this.fn()}finally{$n(this),Y=t,ye=o,this.flags&=-3}}stop(){if(this.flags&1){for(let t=this.deps;t;t=t.nextDep)Ko(t);this.deps=this.depsTail=void 0,rn(this),this.onStop&&this.onStop(),this.flags&=-2}}trigger(){this.flags&64?fo.add(this):this.scheduler?this.scheduler():this.runIfDirty()}runIfDirty(){ko(this)&&this.run()}get dirty(){return ko(this)}}let jn=0,bt,wt;function Nn(e,t=!1){if(e.flags|=8,t){e.next=wt,wt=e;return}e.next=bt,bt=e}function qo(){jn++}function _o(){if(--jn>0)return;if(wt){let t=wt;for(wt=void 0;t;){const o=t.next;t.next=void 0,t.flags&=-9,t=o}}let e;for(;bt;){let t=bt;for(bt=void 0;t;){const o=t.next;if(t.next=void 0,t.flags&=-9,t.flags&1)try{t.trigger()}catch(n){e||(e=n)}t=o}}if(e)throw e}function Gn(e){for(let t=e.deps;t;t=t.nextDep)t.version=-1,t.prevActiveLink=t.dep.activeLink,t.dep.activeLink=t}function $n(e){let t,o=e.depsTail,n=o;for(;n;){const s=n.prevDep;n.version===-1?(n===o&&(o=s),Ko(n),sa(n)):t=n,n.dep.activeLink=n.prevActiveLink,n.prevActiveLink=void 0,n=s}e.deps=t,e.depsTail=o}function ko(e){for(let t=e.deps;t;t=t.nextDep)if(t.dep.version!==t.version||t.dep.computed&&(Qn(t.dep.computed)||t.dep.version!==t.version))return!0;return!!e._dirty}function Qn(e){if(e.flags&4&&!(e.flags&16)||(e.flags&=-17,e.globalVersion===Ct)||(e.globalVersion=Ct,!e.isSSR&&e.flags&128&&(!e.deps&&!e._dirty||!ko(e))))return;e.flags|=2;const t=e.dep,o=Y,n=ye;Y=e,ye=!0;try{Gn(e);const s=e.fn(e._value);(t.version===0||_e(s,e._value))&&(e.flags|=128,e._value=s,t.version++)}catch(s){throw t.version++,s}finally{Y=o,ye=n,$n(e),e.flags&=-3}}function Ko(e,t=!1){const{dep:o,prevSub:n,nextSub:s}=e;if(n&&(n.nextSub=s,e.prevSub=void 0),s&&(s.prevSub=n,e.nextSub=void 0),o.subs===e&&(o.subs=n,!n&&o.computed)){o.computed.flags&=-5;for(let a=o.computed.deps;a;a=a.nextDep)Ko(a,!0)}!t&&!--o.sc&&o.map&&o.map.delete(o.key)}function sa(e){const{prevDep:t,nextDep:o}=e;t&&(t.nextDep=o,e.prevDep=void 0),o&&(o.prevDep=t,e.nextDep=void 0)}let ye=!0;const Jn=[];function Le(){Jn.push(ye),ye=!1}function Fe(){const e=Jn.pop();ye=e===void 0?!0:e}function rn(e){const{cleanup:t}=e;if(e.cleanup=void 0,t){const o=Y;Y=void 0;try{t()}finally{Y=o}}}let Ct=0;class aa{constructor(t,o){this.sub=t,this.dep=o,this.version=o.version,this.nextDep=this.prevDep=this.nextSub=this.prevSub=this.prevActiveLink=void 0}}class Vo{constructor(t){this.computed=t,this.version=0,this.activeLink=void 0,this.subs=void 0,this.map=void 0,this.key=void 0,this.sc=0}track(t){if(!Y||!ye||Y===this.computed)return;let o=this.activeLink;if(o===void 0||o.sub!==Y)o=this.activeLink=new aa(Y,this),Y.deps?(o.prevDep=Y.depsTail,Y.depsTail.nextDep=o,Y.depsTail=o):Y.deps=Y.depsTail=o,Zn(o);else if(o.version===-1&&(o.version=this.version,o.nextDep)){const n=o.nextDep;n.prevDep=o.prevDep,o.prevDep&&(o.prevDep.nextDep=n),o.prevDep=Y.depsTail,o.nextDep=void 0,Y.depsTail.nextDep=o,Y.depsTail=o,Y.deps===o&&(Y.deps=n)}return o}trigger(t){this.version++,Ct++,this.notify(t)}notify(t){qo();try{for(let o=this.subs;o;o=o.prevSub)o.sub.notify()&&o.sub.dep.notify()}finally{_o()}}}function Zn(e){if(e.dep.sc++,e.sub.flags&4){const t=e.dep.computed;if(t&&!e.dep.subs){t.flags|=20;for(let n=t.deps;n;n=n.nextDep)Zn(n)}const o=e.dep.subs;o!==e&&(e.prevSub=o,o&&(o.nextSub=e)),e.dep.subs=e}}const Wo=new WeakMap,Ze=Symbol(""),Io=Symbol(""),kt=Symbol("");function X(e,t,o){if(ye&&Y){let n=Wo.get(e);n||Wo.set(e,n=new Map);let s=n.get(o);s||(n.set(o,s=new Vo),s.map=n,s.key=o),s.track()}}function Be(e,t,o,n,s,a){const i=Wo.get(e);if(!i){Ct++;return}const r=l=>{l&&l.trigger()};if(qo(),t==="clear")i.forEach(r);else{const l=T(e),p=l&&Fo(o);if(l&&o==="length"){const h=Number(n);i.forEach((f,C)=>{(C==="length"||C===kt||!De(C)&&C>=h)&&r(f)})}else switch((o!==void 0||i.has(void 0))&&r(i.get(o)),p&&r(i.get(kt)),t){case"add":l?p&&r(i.get("length")):(r(i.get(Ze)),st(e)&&r(i.get(Io)));break;case"delete":l||(r(i.get(Ze)),st(e)&&r(i.get(Io)));break;case"set":st(e)&&r(i.get(Ze));break}}_o()}function tt(e){const t=L(e);return t===e?t:(X(t,"iterate",kt),me(e)?t:t.map(Z))}function no(e){return X(e=L(e),"iterate",kt),e}const ia={__proto__:null,[Symbol.iterator](){return mo(this,Symbol.iterator,Z)},concat(...e){return tt(this).concat(...e.map(t=>T(t)?tt(t):t))},entries(){return mo(this,"entries",e=>(e[1]=Z(e[1]),e))},every(e,t){return Ee(this,"every",e,t,void 0,arguments)},filter(e,t){return Ee(this,"filter",e,t,o=>o.map(Z),arguments)},find(e,t){return Ee(this,"find",e,t,Z,arguments)},findIndex(e,t){return Ee(this,"findIndex",e,t,void 0,arguments)},findLast(e,t){return Ee(this,"findLast",e,t,Z,arguments)},findLastIndex(e,t){return Ee(this,"findLastIndex",e,t,void 0,arguments)},forEach(e,t){return Ee(this,"forEach",e,t,void 0,arguments)},includes(...e){return go(this,"includes",e)},indexOf(...e){return go(this,"indexOf",e)},join(e){return tt(this).join(e)},lastIndexOf(...e){return go(this,"lastIndexOf",e)},map(e,t){return Ee(this,"map",e,t,void 0,arguments)},pop(){return ft(this,"pop")},push(...e){return ft(this,"push",e)},reduce(e,...t){return cn(this,"reduce",e,t)},reduceRight(e,...t){return cn(this,"reduceRight",e,t)},shift(){return ft(this,"shift")},some(e,t){return Ee(this,"some",e,t,void 0,arguments)},splice(...e){return ft(this,"splice",e)},toReversed(){return tt(this).toReversed()},toSorted(e){return tt(this).toSorted(e)},toSpliced(...e){return tt(this).toSpliced(...e)},unshift(...e){return ft(this,"unshift",e)},values(){return mo(this,"values",Z)}};function mo(e,t,o){const n=no(e),s=n[t]();return n!==e&&!me(e)&&(s._next=s.next,s.next=()=>{const a=s._next();return a.value&&(a.value=o(a.value)),a}),s}const ra=Array.prototype;function Ee(e,t,o,n,s,a){const i=no(e),r=i!==e&&!me(e),l=i[t];if(l!==ra[t]){const f=l.apply(e,a);return r?Z(f):f}let p=o;i!==e&&(r?p=function(f,C){return o.call(this,Z(f),C,e)}:o.length>2&&(p=function(f,C){return o.call(this,f,C,e)}));const h=l.call(i,p,n);return r&&s?s(h):h}function cn(e,t,o,n){const s=no(e);let a=o;return s!==e&&(me(e)?o.length>3&&(a=function(i,r,l){return o.call(this,i,r,l,e)}):a=function(i,r,l){return o.call(this,i,Z(r),l,e)}),s[t](a,...n)}function go(e,t,o){const n=L(e);X(n,"iterate",kt);const s=n[t](...o);return(s===-1||s===!1)&&No(o[0])?(o[0]=L(o[0]),n[t](...o)):s}function ft(e,t,o=[]){Le(),qo();const n=L(e)[t].apply(e,o);return _o(),Fe(),n}const ca=Oo("__proto__,__v_isRef,__isVue"),Xn=new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(De));function la(e){De(e)||(e=String(e));const t=L(this);return X(t,"has",e),t.hasOwnProperty(e)}class es{constructor(t=!1,o=!1){this._isReadonly=t,this._isShallow=o}get(t,o,n){if(o==="__v_skip")return t.__v_skip;const s=this._isReadonly,a=this._isShallow;if(o==="__v_isReactive")return!s;if(o==="__v_isReadonly")return s;if(o==="__v_isShallow")return a;if(o==="__v_raw")return n===(s?a?wa:ss:a?ns:os).get(t)||Object.getPrototypeOf(t)===Object.getPrototypeOf(n)?t:void 0;const i=T(t);if(!s){let l;if(i&&(l=ia[o]))return l;if(o==="hasOwnProperty")return la}const r=Reflect.get(t,o,ee(t)?t:n);return(De(o)?Xn.has(o):ca(o))||(s||X(t,"get",o),a)?r:ee(r)?i&&Fo(o)?r:r.value:H(r)?s?as(r):Ho(r):r}}class ts extends es{constructor(t=!1){super(!1,t)}set(t,o,n,s){let a=t[o];if(!this._isShallow){const l=Ve(a);if(!me(n)&&!Ve(n)&&(a=L(a),n=L(n)),!T(t)&&ee(a)&&!ee(n))return l?!1:(a.value=n,!0)}const i=T(t)&&Fo(o)?Number(o)<t.length:F(t,o),r=Reflect.set(t,o,n,ee(t)?t:s);return t===L(s)&&(i?_e(n,a)&&Be(t,"set",o,n):Be(t,"add",o,n)),r}deleteProperty(t,o){const n=F(t,o);t[o];const s=Reflect.deleteProperty(t,o);return s&&n&&Be(t,"delete",o,void 0),s}has(t,o){const n=Reflect.has(t,o);return(!De(o)||!Xn.has(o))&&X(t,"has",o),n}ownKeys(t){return X(t,"iterate",T(t)?"length":Ze),Reflect.ownKeys(t)}}class ua extends es{constructor(t=!1){super(!0,t)}set(t,o){return!0}deleteProperty(t,o){return!0}}const ha=new ts,da=new ua,pa=new ts(!0);const To=e=>e,Ut=e=>Reflect.getPrototypeOf(e);function fa(e,t,o){return function(...n){const s=this.__v_raw,a=L(s),i=st(a),r=e==="entries"||e===Symbol.iterator&&i,l=e==="keys"&&i,p=s[e](...n),h=o?To:t?Yt:Z;return!t&&X(a,"iterate",l?Io:Ze),{next(){const{value:f,done:C}=p.next();return C?{value:f,done:C}:{value:r?[h(f[0]),h(f[1])]:h(f),done:C}},[Symbol.iterator](){return this}}}}function Lt(e){return function(...t){return e==="delete"?!1:e==="clear"?void 0:this}}function ma(e,t){const o={get(s){const a=this.__v_raw,i=L(a),r=L(s);e||(_e(s,r)&&X(i,"get",s),X(i,"get",r));const{has:l}=Ut(i),p=t?To:e?Yt:Z;if(l.call(i,s))return p(a.get(s));if(l.call(i,r))return p(a.get(r));a!==i&&a.get(s)},get size(){const s=this.__v_raw;return!e&&X(L(s),"iterate",Ze),Reflect.get(s,"size",s)},has(s){const a=this.__v_raw,i=L(a),r=L(s);return e||(_e(s,r)&&X(i,"has",s),X(i,"has",r)),s===r?a.has(s):a.has(s)||a.has(r)},forEach(s,a){const i=this,r=i.__v_raw,l=L(r),p=t?To:e?Yt:Z;return!e&&X(l,"iterate",Ze),r.forEach((h,f)=>s.call(a,p(h),p(f),i))}};return ae(o,e?{add:Lt("add"),set:Lt("set"),delete:Lt("delete"),clear:Lt("clear")}:{add(s){!t&&!me(s)&&!Ve(s)&&(s=L(s));const a=L(this);return Ut(a).has.call(a,s)||(a.add(s),Be(a,"add",s,s)),this},set(s,a){!t&&!me(a)&&!Ve(a)&&(a=L(a));const i=L(this),{has:r,get:l}=Ut(i);let p=r.call(i,s);p||(s=L(s),p=r.call(i,s));const h=l.call(i,s);return i.set(s,a),p?_e(a,h)&&Be(i,"set",s,a):Be(i,"add",s,a),this},delete(s){const a=L(this),{has:i,get:r}=Ut(a);let l=i.call(a,s);l||(s=L(s),l=i.call(a,s)),r&&r.call(a,s);const p=a.delete(s);return l&&Be(a,"delete",s,void 0),p},clear(){const s=L(this),a=s.size!==0,i=s.clear();return a&&Be(s,"clear",void 0,void 0),i}}),["keys","values","entries",Symbol.iterator].forEach(s=>{o[s]=fa(s,e,t)}),o}function Yo(e,t){const o=ma(e,t);return(n,s,a)=>s==="__v_isReactive"?!e:s==="__v_isReadonly"?e:s==="__v_raw"?n:Reflect.get(F(o,s)&&s in n?o:n,s,a)}const ga={get:Yo(!1,!1)},ya={get:Yo(!1,!0)},ba={get:Yo(!0,!1)};const os=new WeakMap,ns=new WeakMap,ss=new WeakMap,wa=new WeakMap;function Aa(e){switch(e){case"Object":case"Array":return 1;case"Map":case"Set":case"WeakMap":case"WeakSet":return 2;default:return 0}}function va(e){return e.__v_skip||!Object.isExtensible(e)?0:Aa(js(e))}function Ho(e){return Ve(e)?e:jo(e,!1,ha,ga,os)}function Sa(e){return jo(e,!1,pa,ya,ns)}function as(e){return jo(e,!0,da,ba,ss)}function jo(e,t,o,n,s){if(!H(e)||e.__v_raw&&!(t&&e.__v_isReactive))return e;const a=va(e);if(a===0)return e;const i=s.get(e);if(i)return i;const r=new Proxy(e,a===2?n:o);return s.set(e,r),r}function at(e){return Ve(e)?at(e.__v_raw):!!(e&&e.__v_isReactive)}function Ve(e){return!!(e&&e.__v_isReadonly)}function me(e){return!!(e&&e.__v_isShallow)}function No(e){return e?!!e.__v_raw:!1}function L(e){const t=e&&e.__v_raw;return t?L(t):e}function Ca(e){return!F(e,"__v_skip")&&Object.isExtensible(e)&&qn(e,"__v_skip",!0),e}const Z=e=>H(e)?Ho(e):e,Yt=e=>H(e)?as(e):e;function ee(e){return e?e.__v_isRef===!0:!1}function xe(e){return ka(e,!1)}function ka(e,t){return ee(e)?e:new Wa(e,t)}class Wa{constructor(t,o){this.dep=new Vo,this.__v_isRef=!0,this.__v_isShallow=!1,this._rawValue=o?t:L(t),this._value=o?t:Z(t),this.__v_isShallow=o}get value(){return this.dep.track(),this._value}set value(t){const o=this._rawValue,n=this.__v_isShallow||me(t)||Ve(t);t=n?t:L(t),_e(t,o)&&(this._rawValue=t,this._value=n?t:Z(t),this.dep.trigger())}}function Ia(e){return ee(e)?e.value:e}const Ta={get:(e,t,o)=>t==="__v_raw"?e:Ia(Reflect.get(e,t,o)),set:(e,t,o,n)=>{const s=e[t];return ee(s)&&!ee(o)?(s.value=o,!0):Reflect.set(e,t,o,n)}};function is(e){return at(e)?e:new Proxy(e,Ta)}class Da{constructor(t,o,n){this.fn=t,this.setter=o,this._value=void 0,this.dep=new Vo(this),this.__v_isRef=!0,this.deps=void 0,this.depsTail=void 0,this.flags=16,this.globalVersion=Ct-1,this.next=void 0,this.effect=this,this.__v_isReadonly=!o,this.isSSR=n}notify(){if(this.flags|=16,!(this.flags&8)&&Y!==this)return Nn(this,!0),!0}get value(){const t=this.dep.track();return Qn(this),t&&(t.version=this.dep.version),this._value}set value(t){this.setter&&this.setter(t)}}function Ma(e,t,o=!1){let n,s;return P(e)?n=e:(n=e.get,s=e.set),new Da(n,s,o)}const Ft={},Ht=new WeakMap;let Qe;function Ea(e,t=!1,o=Qe){if(o){let n=Ht.get(o);n||Ht.set(o,n=[]),n.push(e)}}function xa(e,t,o=q){const{immediate:n,deep:s,once:a,scheduler:i,augmentJob:r,call:l}=o,p=D=>s?D:me(D)||s===!1||s===0?Oe(D,1):Oe(D);let h,f,C,k,x=!1,O=!1;if(ee(e)?(f=()=>e.value,x=me(e)):at(e)?(f=()=>p(e),x=!0):T(e)?(O=!0,x=e.some(D=>at(D)||me(D)),f=()=>e.map(D=>{if(ee(D))return D.value;if(at(D))return p(D);if(P(D))return l?l(D,2):D()})):P(e)?t?f=l?()=>l(e,2):e:f=()=>{if(C){Le();try{C()}finally{Fe()}}const D=Qe;Qe=h;try{return l?l(e,3,[k]):e(k)}finally{Qe=D}}:f=Te,t&&s){const D=f,B=s===!0?1/0:s;f=()=>Oe(D(),B)}const J=na(),z=()=>{h.stop(),J&&J.active&&Lo(J.effects,h)};if(a&&t){const D=t;t=(...B)=>{D(...B),z()}}let j=O?new Array(e.length).fill(Ft):Ft;const N=D=>{if(!(!(h.flags&1)||!h.dirty&&!D))if(t){const B=h.run();if(s||x||(O?B.some((E,$)=>_e(E,j[$])):_e(B,j))){C&&C();const E=Qe;Qe=h;try{const $=[B,j===Ft?void 0:O&&j[0]===Ft?[]:j,k];j=B,l?l(t,3,$):t(...$)}finally{Qe=E}}}else h.run()};return r&&r(N),h=new Hn(f),h.scheduler=i?()=>i(N,!1):N,k=D=>Ea(D,!1,h),C=h.onStop=()=>{const D=Ht.get(h);if(D){if(l)l(D,4);else for(const B of D)B();Ht.delete(h)}},t?n?N(!0):j=h.run():i?i(N.bind(null,!0),!0):h.run(),z.pause=h.pause.bind(h),z.resume=h.resume.bind(h),z.stop=z,z}function Oe(e,t=1/0,o){if(t<=0||!H(e)||e.__v_skip||(o=o||new Set,o.has(e)))return e;if(o.add(e),t--,ee(e))Oe(e.value,t,o);else if(T(e))for(let n=0;n<e.length;n++)Oe(e[n],t,o);else if(Xt(e)||st(e))e.forEach(n=>{Oe(n,t,o)});else if(zn(e)){for(const n in e)Oe(e[n],t,o);for(const n of Object.getOwnPropertySymbols(e))Object.prototype.propertyIsEnumerable.call(e,n)&&Oe(e[n],t,o)}return e}/**
* @vue/runtime-core v3.5.16
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/function Mt(e,t,o,n){try{return n?e(...n):e()}catch(s){so(s,t,o)}}function Me(e,t,o,n){if(P(e)){const s=Mt(e,t,o,n);return s&&Ln(s)&&s.catch(a=>{so(a,t,o)}),s}if(T(e)){const s=[];for(let a=0;a<e.length;a++)s.push(Me(e[a],t,o,n));return s}}function so(e,t,o,n=!0){const s=t?t.vnode:null,{errorHandler:a,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||q;if(t){let r=t.parent;const l=t.proxy,p=`https://vuejs.org/error-reference/#runtime-${o}`;for(;r;){const h=r.ec;if(h){for(let f=0;f<h.length;f++)if(h[f](e,l,p)===!1)return}r=r.parent}if(a){Le(),Mt(a,null,10,[e,l,p]),Fe();return}}Pa(e,o,s,n,i)}function Pa(e,t,o,n=!0,s=!1){if(s)throw e;console.error(e)}const ne=[];let ke=-1;const it=[];let Re=null,ot=0;const rs=Promise.resolve();let jt=null;function Ba(e){const t=jt||rs;return e?t.then(this?e.bind(this):e):t}function Oa(e){let t=ke+1,o=ne.length;for(;t<o;){const n=t+o>>>1,s=ne[n],a=Wt(s);a<e||a===e&&s.flags&2?t=n+1:o=n}return t}function Go(e){if(!(e.flags&1)){const t=Wt(e),o=ne[ne.length-1];!o||!(e.flags&2)&&t>=Wt(o)?ne.push(e):ne.splice(Oa(t),0,e),e.flags|=1,cs()}}function cs(){jt||(jt=rs.then(us))}function Ua(e){T(e)?it.push(...e):Re&&e.id===-1?Re.splice(ot+1,0,e):e.flags&1||(it.push(e),e.flags|=1),cs()}function ln(e,t,o=ke+1){for(;o<ne.length;o++){const n=ne[o];if(n&&n.flags&2){if(e&&n.id!==e.uid)continue;ne.splice(o,1),o--,n.flags&4&&(n.flags&=-2),n(),n.flags&4||(n.flags&=-2)}}}function ls(e){if(it.length){const t=[...new Set(it)].sort((o,n)=>Wt(o)-Wt(n));if(it.length=0,Re){Re.push(...t);return}for(Re=t,ot=0;ot<Re.length;ot++){const o=Re[ot];o.flags&4&&(o.flags&=-2),o.flags&8||o(),o.flags&=-2}Re=null,ot=0}}const Wt=e=>e.id==null?e.flags&2?-1:1/0:e.id;function us(e){try{for(ke=0;ke<ne.length;ke++){const t=ne[ke];t&&!(t.flags&8)&&(t.flags&4&&(t.flags&=-2),Mt(t,t.i,t.i?15:14),t.flags&4||(t.flags&=-2))}}finally{for(;ke<ne.length;ke++){const t=ne[ke];t&&(t.flags&=-2)}ke=-1,ne.length=0,ls(),jt=null,(ne.length||it.length)&&us()}}let fe=null,hs=null;function Nt(e){const t=fe;return fe=e,hs=e&&e.type.__scopeId||null,t}function La(e,t=fe,o){if(!t||e._n)return e;const n=(...s)=>{n._d&&wn(-1);const a=Nt(t);let i;try{i=e(...s)}finally{Nt(a),n._d&&wn(1)}return i};return n._n=!0,n._c=!0,n._d=!0,n}function un(e,t){if(fe===null)return e;const o=co(fe),n=e.dirs||(e.dirs=[]);for(let s=0;s<t.length;s++){let[a,i,r,l=q]=t[s];a&&(P(a)&&(a={mounted:a,updated:a}),a.deep&&Oe(i),n.push({dir:a,instance:o,value:i,oldValue:void 0,arg:r,modifiers:l}))}return e}function Ge(e,t,o,n){const s=e.dirs,a=t&&t.dirs;for(let i=0;i<s.length;i++){const r=s[i];a&&(r.oldValue=a[i].value);let l=r.dir[n];l&&(Le(),Me(l,o,8,[e.el,r,e,t]),Fe())}}const Fa=Symbol("_vte"),za=e=>e.__isTeleport;function $o(e,t){e.shapeFlag&6&&e.component?(e.transition=t,$o(e.component.subTree,t)):e.shapeFlag&128?(e.ssContent.transition=t.clone(e.ssContent),e.ssFallback.transition=t.clone(e.ssFallback)):e.transition=t}function ds(e){e.ids=[e.ids[0]+e.ids[2]+++"-",0,0]}function Gt(e,t,o,n,s=!1){if(T(e)){e.forEach((x,O)=>Gt(x,t&&(T(t)?t[O]:t),o,n,s));return}if(At(n)&&!s){n.shapeFlag&512&&n.type.__asyncResolved&&n.component.subTree.component&&Gt(e,t,o,n.component.subTree);return}const a=n.shapeFlag&4?co(n.component):n.el,i=s?null:a,{i:r,r:l}=e,p=t&&t.r,h=r.refs===q?r.refs={}:r.refs,f=r.setupState,C=L(f),k=f===q?()=>!1:x=>F(C,x);if(p!=null&&p!==l&&(Q(p)?(h[p]=null,k(p)&&(f[p]=null)):ee(p)&&(p.value=null)),P(l))Mt(l,r,12,[i,h]);else{const x=Q(l),O=ee(l);if(x||O){const J=()=>{if(e.f){const z=x?k(l)?f[l]:h[l]:l.value;s?T(z)&&Lo(z,a):T(z)?z.includes(a)||z.push(a):x?(h[l]=[a],k(l)&&(f[l]=h[l])):(l.value=[a],e.k&&(h[e.k]=l.value))}else x?(h[l]=i,k(l)&&(f[l]=i)):O&&(l.value=i,e.k&&(h[e.k]=i))};i?(J.id=-1,de(J,o)):J()}}}to().requestIdleCallback;to().cancelIdleCallback;const At=e=>!!e.type.__asyncLoader,ps=e=>e.type.__isKeepAlive;function Ra(e,t){fs(e,"a",t)}function qa(e,t){fs(e,"da",t)}function fs(e,t,o=se){const n=e.__wdc||(e.__wdc=()=>{let s=o;for(;s;){if(s.isDeactivated)return;s=s.parent}return e()});if(ao(t,n,o),o){let s=o.parent;for(;s&&s.parent;)ps(s.parent.vnode)&&_a(n,t,o,s),s=s.parent}}function _a(e,t,o,n){const s=ao(t,e,n,!0);gs(()=>{Lo(n[t],s)},o)}function ao(e,t,o=se,n=!1){if(o){const s=o[e]||(o[e]=[]),a=t.__weh||(t.__weh=(...i)=>{Le();const r=Et(o),l=Me(t,o,e,i);return r(),Fe(),l});return n?s.unshift(a):s.push(a),a}}const ze=e=>(t,o=se)=>{(!Tt||e==="sp")&&ao(e,(...n)=>t(...n),o)},Ka=ze("bm"),Va=ze("m"),Ya=ze("bu"),Ha=ze("u"),ms=ze("bum"),gs=ze("um"),ja=ze("sp"),Na=ze("rtg"),Ga=ze("rtc");function $a(e,t=se){ao("ec",e,t)}const Qa=Symbol.for("v-ndc");function Ja(e,t,o,n){let s;const a=o,i=T(e);if(i||Q(e)){const r=i&&at(e);let l=!1,p=!1;r&&(l=!me(e),p=Ve(e),e=no(e)),s=new Array(e.length);for(let h=0,f=e.length;h<f;h++)s[h]=t(l?p?Yt(Z(e[h])):Z(e[h]):e[h],h,void 0,a)}else if(typeof e=="number"){s=new Array(e);for(let r=0;r<e;r++)s[r]=t(r+1,r,void 0,a)}else if(H(e))if(e[Symbol.iterator])s=Array.from(e,(r,l)=>t(r,l,void 0,a));else{const r=Object.keys(e);s=new Array(r.length);for(let l=0,p=r.length;l<p;l++){const h=r[l];s[l]=t(e[h],h,l,a)}}else s=[];return s}const Do=e=>e?Fs(e)?co(e):Do(e.parent):null,vt=ae(Object.create(null),{$:e=>e,$el:e=>e.vnode.el,$data:e=>e.data,$props:e=>e.props,$attrs:e=>e.attrs,$slots:e=>e.slots,$refs:e=>e.refs,$parent:e=>Do(e.parent),$root:e=>Do(e.root),$host:e=>e.ce,$emit:e=>e.emit,$options:e=>bs(e),$forceUpdate:e=>e.f||(e.f=()=>{Go(e.update)}),$nextTick:e=>e.n||(e.n=Ba.bind(e.proxy)),$watch:e=>wi.bind(e)}),yo=(e,t)=>e!==q&&!e.__isScriptSetup&&F(e,t),Za={get({_:e},t){if(t==="__v_skip")return!0;const{ctx:o,setupState:n,data:s,props:a,accessCache:i,type:r,appContext:l}=e;let p;if(t[0]!=="$"){const k=i[t];if(k!==void 0)switch(k){case 1:return n[t];case 2:return s[t];case 4:return o[t];case 3:return a[t]}else{if(yo(n,t))return i[t]=1,n[t];if(s!==q&&F(s,t))return i[t]=2,s[t];if((p=e.propsOptions[0])&&F(p,t))return i[t]=3,a[t];if(o!==q&&F(o,t))return i[t]=4,o[t];Mo&&(i[t]=0)}}const h=vt[t];let f,C;if(h)return t==="$attrs"&&X(e.attrs,"get",""),h(e);if((f=r.__cssModules)&&(f=f[t]))return f;if(o!==q&&F(o,t))return i[t]=4,o[t];if(C=l.config.globalProperties,F(C,t))return C[t]},set({_:e},t,o){const{data:n,setupState:s,ctx:a}=e;return yo(s,t)?(s[t]=o,!0):n!==q&&F(n,t)?(n[t]=o,!0):F(e.props,t)||t[0]==="$"&&t.slice(1)in e?!1:(a[t]=o,!0)},has({_:{data:e,setupState:t,accessCache:o,ctx:n,appContext:s,propsOptions:a}},i){let r;return!!o[i]||e!==q&&F(e,i)||yo(t,i)||(r=a[0])&&F(r,i)||F(n,i)||F(vt,i)||F(s.config.globalProperties,i)},defineProperty(e,t,o){return o.get!=null?e._.accessCache[t]=0:F(o,"value")&&this.set(e,t,o.value,null),Reflect.defineProperty(e,t,o)}};function hn(e){return T(e)?e.reduce((t,o)=>(t[o]=null,t),{}):e}let Mo=!0;function Xa(e){const t=bs(e),o=e.proxy,n=e.ctx;Mo=!1,t.beforeCreate&&dn(t.beforeCreate,e,"bc");const{data:s,computed:a,methods:i,watch:r,provide:l,inject:p,created:h,beforeMount:f,mounted:C,beforeUpdate:k,updated:x,activated:O,deactivated:J,beforeDestroy:z,beforeUnmount:j,destroyed:N,unmounted:D,render:B,renderTracked:E,renderTriggered:$,errorCaptured:ie,serverPrefetch:et,expose:He,inheritAttrs:ut,components:xt,directives:Pt,filters:lo}=t;if(p&&ei(p,n,null),i)for(const G in i){const _=i[G];P(_)&&(n[G]=_.bind(o))}if(s){const G=s.call(o,o);H(G)&&(e.data=Ho(G))}if(Mo=!0,a)for(const G in a){const _=a[G],je=P(_)?_.bind(o,o):P(_.get)?_.get.bind(o,o):Te,Bt=!P(_)&&P(_.set)?_.set.bind(o):Te,Ne=Rs({get:je,set:Bt});Object.defineProperty(n,G,{enumerable:!0,configurable:!0,get:()=>Ne.value,set:be=>Ne.value=be})}if(r)for(const G in r)ys(r[G],n,o,G);if(l){const G=P(l)?l.call(o):l;Reflect.ownKeys(G).forEach(_=>{ii(_,G[_])})}h&&dn(h,e,"c");function te(G,_){T(_)?_.forEach(je=>G(je.bind(o))):_&&G(_.bind(o))}if(te(Ka,f),te(Va,C),te(Ya,k),te(Ha,x),te(Ra,O),te(qa,J),te($a,ie),te(Ga,E),te(Na,$),te(ms,j),te(gs,D),te(ja,et),T(He))if(He.length){const G=e.exposed||(e.exposed={});He.forEach(_=>{Object.defineProperty(G,_,{get:()=>o[_],set:je=>o[_]=je})})}else e.exposed||(e.exposed={});B&&e.render===Te&&(e.render=B),ut!=null&&(e.inheritAttrs=ut),xt&&(e.components=xt),Pt&&(e.directives=Pt),et&&ds(e)}function ei(e,t,o=Te){T(e)&&(e=Eo(e));for(const n in e){const s=e[n];let a;H(s)?"default"in s?a=qt(s.from||n,s.default,!0):a=qt(s.from||n):a=qt(s),ee(a)?Object.defineProperty(t,n,{enumerable:!0,configurable:!0,get:()=>a.value,set:i=>a.value=i}):t[n]=a}}function dn(e,t,o){Me(T(e)?e.map(n=>n.bind(t.proxy)):e.bind(t.proxy),t,o)}function ys(e,t,o,n){let s=n.includes(".")?xs(o,n):()=>o[n];if(Q(e)){const a=t[e];P(a)&&wo(s,a)}else if(P(e))wo(s,e.bind(o));else if(H(e))if(T(e))e.forEach(a=>ys(a,t,o,n));else{const a=P(e.handler)?e.handler.bind(o):t[e.handler];P(a)&&wo(s,a,e)}}function bs(e){const t=e.type,{mixins:o,extends:n}=t,{mixins:s,optionsCache:a,config:{optionMergeStrategies:i}}=e.appContext,r=a.get(t);let l;return r?l=r:!s.length&&!o&&!n?l=t:(l={},s.length&&s.forEach(p=>$t(l,p,i,!0)),$t(l,t,i)),H(t)&&a.set(t,l),l}function $t(e,t,o,n=!1){const{mixins:s,extends:a}=t;a&&$t(e,a,o,!0),s&&s.forEach(i=>$t(e,i,o,!0));for(const i in t)if(!(n&&i==="expose")){const r=ti[i]||o&&o[i];e[i]=r?r(e[i],t[i]):t[i]}return e}const ti={data:pn,props:fn,emits:fn,methods:gt,computed:gt,beforeCreate:oe,created:oe,beforeMount:oe,mounted:oe,beforeUpdate:oe,updated:oe,beforeDestroy:oe,beforeUnmount:oe,destroyed:oe,unmounted:oe,activated:oe,deactivated:oe,errorCaptured:oe,serverPrefetch:oe,components:gt,directives:gt,watch:ni,provide:pn,inject:oi};function pn(e,t){return t?e?function(){return ae(P(e)?e.call(this,this):e,P(t)?t.call(this,this):t)}:t:e}function oi(e,t){return gt(Eo(e),Eo(t))}function Eo(e){if(T(e)){const t={};for(let o=0;o<e.length;o++)t[e[o]]=e[o];return t}return e}function oe(e,t){return e?[...new Set([].concat(e,t))]:t}function gt(e,t){return e?ae(Object.create(null),e,t):t}function fn(e,t){return e?T(e)&&T(t)?[...new Set([...e,...t])]:ae(Object.create(null),hn(e),hn(t??{})):t}function ni(e,t){if(!e)return t;if(!t)return e;const o=ae(Object.create(null),e);for(const n in t)o[n]=oe(e[n],t[n]);return o}function ws(){return{app:null,config:{isNativeTag:Ys,performance:!1,globalProperties:{},optionMergeStrategies:{},errorHandler:void 0,warnHandler:void 0,compilerOptions:{}},mixins:[],components:{},directives:{},provides:Object.create(null),optionsCache:new WeakMap,propsCache:new WeakMap,emitsCache:new WeakMap}}let si=0;function ai(e,t){return function(n,s=null){P(n)||(n=ae({},n)),s!=null&&!H(s)&&(s=null);const a=ws(),i=new WeakSet,r=[];let l=!1;const p=a.app={_uid:si++,_component:n,_props:s,_container:null,_context:a,_instance:null,version:qi,get config(){return a.config},set config(h){},use(h,...f){return i.has(h)||(h&&P(h.install)?(i.add(h),h.install(p,...f)):P(h)&&(i.add(h),h(p,...f))),p},mixin(h){return a.mixins.includes(h)||a.mixins.push(h),p},component(h,f){return f?(a.components[h]=f,p):a.components[h]},directive(h,f){return f?(a.directives[h]=f,p):a.directives[h]},mount(h,f,C){if(!l){const k=p._ceVNode||Ue(n,s);return k.appContext=a,C===!0?C="svg":C===!1&&(C=void 0),e(k,h,C),l=!0,p._container=h,h.__vue_app__=p,co(k.component)}},onUnmount(h){r.push(h)},unmount(){l&&(Me(r,p._instance,16),e(null,p._container),delete p._container.__vue_app__)},provide(h,f){return a.provides[h]=f,p},runWithContext(h){const f=rt;rt=p;try{return h()}finally{rt=f}}};return p}}let rt=null;function ii(e,t){if(se){let o=se.provides;const n=se.parent&&se.parent.provides;n===o&&(o=se.provides=Object.create(n)),o[e]=t}}function qt(e,t,o=!1){const n=se||fe;if(n||rt){let s=rt?rt._context.provides:n?n.parent==null||n.ce?n.vnode.appContext&&n.vnode.appContext.provides:n.parent.provides:void 0;if(s&&e in s)return s[e];if(arguments.length>1)return o&&P(t)?t.call(n&&n.proxy):t}}const As={},vs=()=>Object.create(As),Ss=e=>Object.getPrototypeOf(e)===As;function ri(e,t,o,n=!1){const s={},a=vs();e.propsDefaults=Object.create(null),Cs(e,t,s,a);for(const i in e.propsOptions[0])i in s||(s[i]=void 0);o?e.props=n?s:Sa(s):e.type.props?e.props=s:e.props=a,e.attrs=a}function ci(e,t,o,n){const{props:s,attrs:a,vnode:{patchFlag:i}}=e,r=L(s),[l]=e.propsOptions;let p=!1;if((n||i>0)&&!(i&16)){if(i&8){const h=e.vnode.dynamicProps;for(let f=0;f<h.length;f++){let C=h[f];if(io(e.emitsOptions,C))continue;const k=t[C];if(l)if(F(a,C))k!==a[C]&&(a[C]=k,p=!0);else{const x=Ke(C);s[x]=xo(l,r,x,k,e,!1)}else k!==a[C]&&(a[C]=k,p=!0)}}}else{Cs(e,t,s,a)&&(p=!0);let h;for(const f in r)(!t||!F(t,f)&&((h=Xe(f))===f||!F(t,h)))&&(l?o&&(o[f]!==void 0||o[h]!==void 0)&&(s[f]=xo(l,r,f,void 0,e,!0)):delete s[f]);if(a!==r)for(const f in a)(!t||!F(t,f))&&(delete a[f],p=!0)}p&&Be(e.attrs,"set","")}function Cs(e,t,o,n){const[s,a]=e.propsOptions;let i=!1,r;if(t)for(let l in t){if(yt(l))continue;const p=t[l];let h;s&&F(s,h=Ke(l))?!a||!a.includes(h)?o[h]=p:(r||(r={}))[h]=p:io(e.emitsOptions,l)||(!(l in n)||p!==n[l])&&(n[l]=p,i=!0)}if(a){const l=L(o),p=r||q;for(let h=0;h<a.length;h++){const f=a[h];o[f]=xo(s,l,f,p[f],e,!F(p,f))}}return i}function xo(e,t,o,n,s,a){const i=e[o];if(i!=null){const r=F(i,"default");if(r&&n===void 0){const l=i.default;if(i.type!==Function&&!i.skipFactory&&P(l)){const{propsDefaults:p}=s;if(o in p)n=p[o];else{const h=Et(s);n=p[o]=l.call(null,t),h()}}else n=l;s.ce&&s.ce._setProp(o,n)}i[0]&&(a&&!r?n=!1:i[1]&&(n===""||n===Xe(o))&&(n=!0))}return n}const li=new WeakMap;function ks(e,t,o=!1){const n=o?li:t.propsCache,s=n.get(e);if(s)return s;const a=e.props,i={},r=[];let l=!1;if(!P(e)){const h=f=>{l=!0;const[C,k]=ks(f,t,!0);ae(i,C),k&&r.push(...k)};!o&&t.mixins.length&&t.mixins.forEach(h),e.extends&&h(e.extends),e.mixins&&e.mixins.forEach(h)}if(!a&&!l)return H(e)&&n.set(e,nt),nt;if(T(a))for(let h=0;h<a.length;h++){const f=Ke(a[h]);mn(f)&&(i[f]=q)}else if(a)for(const h in a){const f=Ke(h);if(mn(f)){const C=a[h],k=i[f]=T(C)||P(C)?{type:C}:ae({},C),x=k.type;let O=!1,J=!0;if(T(x))for(let z=0;z<x.length;++z){const j=x[z],N=P(j)&&j.name;if(N==="Boolean"){O=!0;break}else N==="String"&&(J=!1)}else O=P(x)&&x.name==="Boolean";k[0]=O,k[1]=J,(O||F(k,"default"))&&r.push(f)}}const p=[i,r];return H(e)&&n.set(e,p),p}function mn(e){return e[0]!=="$"&&!yt(e)}const Qo=e=>e[0]==="_"||e==="$stable",Jo=e=>T(e)?e.map(Ie):[Ie(e)],ui=(e,t,o)=>{if(t._n)return t;const n=La((...s)=>Jo(t(...s)),o);return n._c=!1,n},Ws=(e,t,o)=>{const n=e._ctx;for(const s in e){if(Qo(s))continue;const a=e[s];if(P(a))t[s]=ui(s,a,n);else if(a!=null){const i=Jo(a);t[s]=()=>i}}},Is=(e,t)=>{const o=Jo(t);e.slots.default=()=>o},Ts=(e,t,o)=>{for(const n in t)(o||!Qo(n))&&(e[n]=t[n])},hi=(e,t,o)=>{const n=e.slots=vs();if(e.vnode.shapeFlag&32){const s=t._;s?(Ts(n,t,o),o&&qn(n,"_",s,!0)):Ws(t,n)}else t&&Is(e,t)},di=(e,t,o)=>{const{vnode:n,slots:s}=e;let a=!0,i=q;if(n.shapeFlag&32){const r=t._;r?o&&r===1?a=!1:Ts(s,t,o):(a=!t.$stable,Ws(t,s)),i=t}else t&&(Is(e,t),i={default:1});if(a)for(const r in s)!Qo(r)&&i[r]==null&&delete s[r]},de=Ii;function pi(e){return fi(e)}function fi(e,t){const o=to();o.__VUE__=!0;const{insert:n,remove:s,patchProp:a,createElement:i,createText:r,createComment:l,setText:p,setElementText:h,parentNode:f,nextSibling:C,setScopeId:k=Te,insertStaticContent:x}=e,O=(c,u,d,y=null,m=null,g=null,v=void 0,A=null,w=!!u.dynamicChildren)=>{if(c===u)return;c&&!mt(c,u)&&(y=Ot(c),be(c,m,g,!0),c=null),u.patchFlag===-2&&(w=!1,u.dynamicChildren=null);const{type:b,ref:I,shapeFlag:S}=u;switch(b){case ro:J(c,u,d,y);break;case Ye:z(c,u,d,y);break;case Ao:c==null&&j(u,d,y,v);break;case We:xt(c,u,d,y,m,g,v,A,w);break;default:S&1?B(c,u,d,y,m,g,v,A,w):S&6?Pt(c,u,d,y,m,g,v,A,w):(S&64||S&128)&&b.process(c,u,d,y,m,g,v,A,w,dt)}I!=null&&m&&Gt(I,c&&c.ref,g,u||c,!u)},J=(c,u,d,y)=>{if(c==null)n(u.el=r(u.children),d,y);else{const m=u.el=c.el;u.children!==c.children&&p(m,u.children)}},z=(c,u,d,y)=>{c==null?n(u.el=l(u.children||""),d,y):u.el=c.el},j=(c,u,d,y)=>{[c.el,c.anchor]=x(c.children,u,d,y,c.el,c.anchor)},N=({el:c,anchor:u},d,y)=>{let m;for(;c&&c!==u;)m=C(c),n(c,d,y),c=m;n(u,d,y)},D=({el:c,anchor:u})=>{let d;for(;c&&c!==u;)d=C(c),s(c),c=d;s(u)},B=(c,u,d,y,m,g,v,A,w)=>{u.type==="svg"?v="svg":u.type==="math"&&(v="mathml"),c==null?E(u,d,y,m,g,v,A,w):et(c,u,m,g,v,A,w)},E=(c,u,d,y,m,g,v,A)=>{let w,b;const{props:I,shapeFlag:S,transition:W,dirs:M}=c;if(w=c.el=i(c.type,g,I&&I.is,I),S&8?h(w,c.children):S&16&&ie(c.children,w,null,y,m,bo(c,g),v,A),M&&Ge(c,null,y,"created"),$(w,c,c.scopeId,v,y),I){for(const K in I)K!=="value"&&!yt(K)&&a(w,K,null,I[K],g,y);"value"in I&&a(w,"value",null,I.value,g),(b=I.onVnodeBeforeMount)&&Ce(b,y,c)}M&&Ge(c,null,y,"beforeMount");const U=mi(m,W);U&&W.beforeEnter(w),n(w,u,d),((b=I&&I.onVnodeMounted)||U||M)&&de(()=>{b&&Ce(b,y,c),U&&W.enter(w),M&&Ge(c,null,y,"mounted")},m)},$=(c,u,d,y,m)=>{if(d&&k(c,d),y)for(let g=0;g<y.length;g++)k(c,y[g]);if(m){let g=m.subTree;if(u===g||Bs(g.type)&&(g.ssContent===u||g.ssFallback===u)){const v=m.vnode;$(c,v,v.scopeId,v.slotScopeIds,m.parent)}}},ie=(c,u,d,y,m,g,v,A,w=0)=>{for(let b=w;b<c.length;b++){const I=c[b]=A?qe(c[b]):Ie(c[b]);O(null,I,u,d,y,m,g,v,A)}},et=(c,u,d,y,m,g,v)=>{const A=u.el=c.el;let{patchFlag:w,dynamicChildren:b,dirs:I}=u;w|=c.patchFlag&16;const S=c.props||q,W=u.props||q;let M;if(d&&$e(d,!1),(M=W.onVnodeBeforeUpdate)&&Ce(M,d,u,c),I&&Ge(u,c,d,"beforeUpdate"),d&&$e(d,!0),(S.innerHTML&&W.innerHTML==null||S.textContent&&W.textContent==null)&&h(A,""),b?He(c.dynamicChildren,b,A,d,y,bo(u,m),g):v||_(c,u,A,null,d,y,bo(u,m),g,!1),w>0){if(w&16)ut(A,S,W,d,m);else if(w&2&&S.class!==W.class&&a(A,"class",null,W.class,m),w&4&&a(A,"style",S.style,W.style,m),w&8){const U=u.dynamicProps;for(let K=0;K<U.length;K++){const R=U[K],le=S[R],re=W[R];(re!==le||R==="value")&&a(A,R,le,re,m,d)}}w&1&&c.children!==u.children&&h(A,u.children)}else!v&&b==null&&ut(A,S,W,d,m);((M=W.onVnodeUpdated)||I)&&de(()=>{M&&Ce(M,d,u,c),I&&Ge(u,c,d,"updated")},y)},He=(c,u,d,y,m,g,v)=>{for(let A=0;A<u.length;A++){const w=c[A],b=u[A],I=w.el&&(w.type===We||!mt(w,b)||w.shapeFlag&198)?f(w.el):d;O(w,b,I,null,y,m,g,v,!0)}},ut=(c,u,d,y,m)=>{if(u!==d){if(u!==q)for(const g in u)!yt(g)&&!(g in d)&&a(c,g,u[g],null,m,y);for(const g in d){if(yt(g))continue;const v=d[g],A=u[g];v!==A&&g!=="value"&&a(c,g,A,v,m,y)}"value"in d&&a(c,"value",u.value,d.value,m)}},xt=(c,u,d,y,m,g,v,A,w)=>{const b=u.el=c?c.el:r(""),I=u.anchor=c?c.anchor:r("");let{patchFlag:S,dynamicChildren:W,slotScopeIds:M}=u;M&&(A=A?A.concat(M):M),c==null?(n(b,d,y),n(I,d,y),ie(u.children||[],d,I,m,g,v,A,w)):S>0&&S&64&&W&&c.dynamicChildren?(He(c.dynamicChildren,W,d,m,g,v,A),(u.key!=null||m&&u===m.subTree)&&Ds(c,u,!0)):_(c,u,d,I,m,g,v,A,w)},Pt=(c,u,d,y,m,g,v,A,w)=>{u.slotScopeIds=A,c==null?u.shapeFlag&512?m.ctx.activate(u,d,y,v,w):lo(u,d,y,m,g,v,w):Xo(c,u,w)},lo=(c,u,d,y,m,g,v)=>{const A=c.component=Oi(c,y,m);if(ps(c)&&(A.ctx.renderer=dt),Ui(A,!1,v),A.asyncDep){if(m&&m.registerDep(A,te,v),!c.el){const w=A.subTree=Ue(Ye);z(null,w,u,d)}}else te(A,c,u,d,m,g,v)},Xo=(c,u,d)=>{const y=u.component=c.component;if(ki(c,u,d))if(y.asyncDep&&!y.asyncResolved){G(y,u,d);return}else y.next=u,y.update();else u.el=c.el,y.vnode=u},te=(c,u,d,y,m,g,v)=>{const A=()=>{if(c.isMounted){let{next:S,bu:W,u:M,parent:U,vnode:K}=c;{const Ae=Ms(c);if(Ae){S&&(S.el=K.el,G(c,S,v)),Ae.asyncDep.then(()=>{c.isUnmounted||A()});return}}let R=S,le;$e(c,!1),S?(S.el=K.el,G(c,S,v)):S=K,W&&Rt(W),(le=S.props&&S.props.onVnodeBeforeUpdate)&&Ce(le,U,S,K),$e(c,!0);const re=yn(c),we=c.subTree;c.subTree=re,O(we,re,f(we.el),Ot(we),c,m,g),S.el=re.el,R===null&&Wi(c,re.el),M&&de(M,m),(le=S.props&&S.props.onVnodeUpdated)&&de(()=>Ce(le,U,S,K),m)}else{let S;const{el:W,props:M}=u,{bm:U,m:K,parent:R,root:le,type:re}=c,we=At(u);$e(c,!1),U&&Rt(U),!we&&(S=M&&M.onVnodeBeforeMount)&&Ce(S,R,u),$e(c,!0);{le.ce&&le.ce._injectChildStyle(re);const Ae=c.subTree=yn(c);O(null,Ae,d,y,c,m,g),u.el=Ae.el}if(K&&de(K,m),!we&&(S=M&&M.onVnodeMounted)){const Ae=u;de(()=>Ce(S,R,Ae),m)}(u.shapeFlag&256||R&&At(R.vnode)&&R.vnode.shapeFlag&256)&&c.a&&de(c.a,m),c.isMounted=!0,u=d=y=null}};c.scope.on();const w=c.effect=new Hn(A);c.scope.off();const b=c.update=w.run.bind(w),I=c.job=w.runIfDirty.bind(w);I.i=c,I.id=c.uid,w.scheduler=()=>Go(I),$e(c,!0),b()},G=(c,u,d)=>{u.component=c;const y=c.vnode.props;c.vnode=u,c.next=null,ci(c,u.props,y,d),di(c,u.children,d),Le(),ln(c),Fe()},_=(c,u,d,y,m,g,v,A,w=!1)=>{const b=c&&c.children,I=c?c.shapeFlag:0,S=u.children,{patchFlag:W,shapeFlag:M}=u;if(W>0){if(W&128){Bt(b,S,d,y,m,g,v,A,w);return}else if(W&256){je(b,S,d,y,m,g,v,A,w);return}}M&8?(I&16&&ht(b,m,g),S!==b&&h(d,S)):I&16?M&16?Bt(b,S,d,y,m,g,v,A,w):ht(b,m,g,!0):(I&8&&h(d,""),M&16&&ie(S,d,y,m,g,v,A,w))},je=(c,u,d,y,m,g,v,A,w)=>{c=c||nt,u=u||nt;const b=c.length,I=u.length,S=Math.min(b,I);let W;for(W=0;W<S;W++){const M=u[W]=w?qe(u[W]):Ie(u[W]);O(c[W],M,d,null,m,g,v,A,w)}b>I?ht(c,m,g,!0,!1,S):ie(u,d,y,m,g,v,A,w,S)},Bt=(c,u,d,y,m,g,v,A,w)=>{let b=0;const I=u.length;let S=c.length-1,W=I-1;for(;b<=S&&b<=W;){const M=c[b],U=u[b]=w?qe(u[b]):Ie(u[b]);if(mt(M,U))O(M,U,d,null,m,g,v,A,w);else break;b++}for(;b<=S&&b<=W;){const M=c[S],U=u[W]=w?qe(u[W]):Ie(u[W]);if(mt(M,U))O(M,U,d,null,m,g,v,A,w);else break;S--,W--}if(b>S){if(b<=W){const M=W+1,U=M<I?u[M].el:y;for(;b<=W;)O(null,u[b]=w?qe(u[b]):Ie(u[b]),d,U,m,g,v,A,w),b++}}else if(b>W)for(;b<=S;)be(c[b],m,g,!0),b++;else{const M=b,U=b,K=new Map;for(b=U;b<=W;b++){const ue=u[b]=w?qe(u[b]):Ie(u[b]);ue.key!=null&&K.set(ue.key,b)}let R,le=0;const re=W-U+1;let we=!1,Ae=0;const pt=new Array(re);for(b=0;b<re;b++)pt[b]=0;for(b=M;b<=S;b++){const ue=c[b];if(le>=re){be(ue,m,g,!0);continue}let ve;if(ue.key!=null)ve=K.get(ue.key);else for(R=U;R<=W;R++)if(pt[R-U]===0&&mt(ue,u[R])){ve=R;break}ve===void 0?be(ue,m,g,!0):(pt[ve-U]=b+1,ve>=Ae?Ae=ve:we=!0,O(ue,u[ve],d,null,m,g,v,A,w),le++)}const on=we?gi(pt):nt;for(R=on.length-1,b=re-1;b>=0;b--){const ue=U+b,ve=u[ue],nn=ue+1<I?u[ue+1].el:y;pt[b]===0?O(null,ve,d,nn,m,g,v,A,w):we&&(R<0||b!==on[R]?Ne(ve,d,nn,2):R--)}}},Ne=(c,u,d,y,m=null)=>{const{el:g,type:v,transition:A,children:w,shapeFlag:b}=c;if(b&6){Ne(c.component.subTree,u,d,y);return}if(b&128){c.suspense.move(u,d,y);return}if(b&64){v.move(c,u,d,dt);return}if(v===We){n(g,u,d);for(let S=0;S<w.length;S++)Ne(w[S],u,d,y);n(c.anchor,u,d);return}if(v===Ao){N(c,u,d);return}if(y!==2&&b&1&&A)if(y===0)A.beforeEnter(g),n(g,u,d),de(()=>A.enter(g),m);else{const{leave:S,delayLeave:W,afterLeave:M}=A,U=()=>{c.ctx.isUnmounted?s(g):n(g,u,d)},K=()=>{S(g,()=>{U(),M&&M()})};W?W(g,U,K):K()}else n(g,u,d)},be=(c,u,d,y=!1,m=!1)=>{const{type:g,props:v,ref:A,children:w,dynamicChildren:b,shapeFlag:I,patchFlag:S,dirs:W,cacheIndex:M}=c;if(S===-2&&(m=!1),A!=null&&(Le(),Gt(A,null,d,c,!0),Fe()),M!=null&&(u.renderCache[M]=void 0),I&256){u.ctx.deactivate(c);return}const U=I&1&&W,K=!At(c);let R;if(K&&(R=v&&v.onVnodeBeforeUnmount)&&Ce(R,u,c),I&6)Vs(c.component,d,y);else{if(I&128){c.suspense.unmount(d,y);return}U&&Ge(c,null,u,"beforeUnmount"),I&64?c.type.remove(c,u,d,dt,y):b&&!b.hasOnce&&(g!==We||S>0&&S&64)?ht(b,u,d,!1,!0):(g===We&&S&384||!m&&I&16)&&ht(w,u,d),y&&en(c)}(K&&(R=v&&v.onVnodeUnmounted)||U)&&de(()=>{R&&Ce(R,u,c),U&&Ge(c,null,u,"unmounted")},d)},en=c=>{const{type:u,el:d,anchor:y,transition:m}=c;if(u===We){Ks(d,y);return}if(u===Ao){D(c);return}const g=()=>{s(d),m&&!m.persisted&&m.afterLeave&&m.afterLeave()};if(c.shapeFlag&1&&m&&!m.persisted){const{leave:v,delayLeave:A}=m,w=()=>v(d,g);A?A(c.el,g,w):w()}else g()},Ks=(c,u)=>{let d;for(;c!==u;)d=C(c),s(c),c=d;s(u)},Vs=(c,u,d)=>{const{bum:y,scope:m,job:g,subTree:v,um:A,m:w,a:b,parent:I,slots:{__:S}}=c;gn(w),gn(b),y&&Rt(y),I&&T(S)&&S.forEach(W=>{I.renderCache[W]=void 0}),m.stop(),g&&(g.flags|=8,be(v,c,u,d)),A&&de(A,u),de(()=>{c.isUnmounted=!0},u),u&&u.pendingBranch&&!u.isUnmounted&&c.asyncDep&&!c.asyncResolved&&c.suspenseId===u.pendingId&&(u.deps--,u.deps===0&&u.resolve())},ht=(c,u,d,y=!1,m=!1,g=0)=>{for(let v=g;v<c.length;v++)be(c[v],u,d,y,m)},Ot=c=>{if(c.shapeFlag&6)return Ot(c.component.subTree);if(c.shapeFlag&128)return c.suspense.next();const u=C(c.anchor||c.el),d=u&&u[Fa];return d?C(d):u};let uo=!1;const tn=(c,u,d)=>{c==null?u._vnode&&be(u._vnode,null,null,!0):O(u._vnode||null,c,u,null,null,null,d),u._vnode=c,uo||(uo=!0,ln(),ls(),uo=!1)},dt={p:O,um:be,m:Ne,r:en,mt:lo,mc:ie,pc:_,pbc:He,n:Ot,o:e};return{render:tn,hydrate:void 0,createApp:ai(tn)}}function bo({type:e,props:t},o){return o==="svg"&&e==="foreignObject"||o==="mathml"&&e==="annotation-xml"&&t&&t.encoding&&t.encoding.includes("html")?void 0:o}function $e({effect:e,job:t},o){o?(e.flags|=32,t.flags|=4):(e.flags&=-33,t.flags&=-5)}function mi(e,t){return(!e||e&&!e.pendingBranch)&&t&&!t.persisted}function Ds(e,t,o=!1){const n=e.children,s=t.children;if(T(n)&&T(s))for(let a=0;a<n.length;a++){const i=n[a];let r=s[a];r.shapeFlag&1&&!r.dynamicChildren&&((r.patchFlag<=0||r.patchFlag===32)&&(r=s[a]=qe(s[a]),r.el=i.el),!o&&r.patchFlag!==-2&&Ds(i,r)),r.type===ro&&(r.el=i.el),r.type===Ye&&!r.el&&(r.el=i.el)}}function gi(e){const t=e.slice(),o=[0];let n,s,a,i,r;const l=e.length;for(n=0;n<l;n++){const p=e[n];if(p!==0){if(s=o[o.length-1],e[s]<p){t[n]=s,o.push(n);continue}for(a=0,i=o.length-1;a<i;)r=a+i>>1,e[o[r]]<p?a=r+1:i=r;p<e[o[a]]&&(a>0&&(t[n]=o[a-1]),o[a]=n)}}for(a=o.length,i=o[a-1];a-- >0;)o[a]=i,i=t[i];return o}function Ms(e){const t=e.subTree.component;if(t)return t.asyncDep&&!t.asyncResolved?t:Ms(t)}function gn(e){if(e)for(let t=0;t<e.length;t++)e[t].flags|=8}const yi=Symbol.for("v-scx"),bi=()=>qt(yi);function wo(e,t,o){return Es(e,t,o)}function Es(e,t,o=q){const{immediate:n,deep:s,flush:a,once:i}=o,r=ae({},o),l=t&&n||!t&&a!=="post";let p;if(Tt){if(a==="sync"){const k=bi();p=k.__watcherHandles||(k.__watcherHandles=[])}else if(!l){const k=()=>{};return k.stop=Te,k.resume=Te,k.pause=Te,k}}const h=se;r.call=(k,x,O)=>Me(k,h,x,O);let f=!1;a==="post"?r.scheduler=k=>{de(k,h&&h.suspense)}:a!=="sync"&&(f=!0,r.scheduler=(k,x)=>{x?k():Go(k)}),r.augmentJob=k=>{t&&(k.flags|=4),f&&(k.flags|=2,h&&(k.id=h.uid,k.i=h))};const C=xa(e,t,r);return Tt&&(p?p.push(C):l&&C()),C}function wi(e,t,o){const n=this.proxy,s=Q(e)?e.includes(".")?xs(n,e):()=>n[e]:e.bind(n,n);let a;P(t)?a=t:(a=t.handler,o=t);const i=Et(this),r=Es(s,a.bind(n),o);return i(),r}function xs(e,t){const o=t.split(".");return()=>{let n=e;for(let s=0;s<o.length&&n;s++)n=n[o[s]];return n}}const Ai=(e,t)=>t==="modelValue"||t==="model-value"?e.modelModifiers:e[`${t}Modifiers`]||e[`${Ke(t)}Modifiers`]||e[`${Xe(t)}Modifiers`];function vi(e,t,...o){if(e.isUnmounted)return;const n=e.vnode.props||q;let s=o;const a=t.startsWith("update:"),i=a&&Ai(n,t.slice(7));i&&(i.trim&&(s=o.map(h=>Q(h)?h.trim():h)),i.number&&(s=o.map(Co)));let r,l=n[r=ho(t)]||n[r=ho(Ke(t))];!l&&a&&(l=n[r=ho(Xe(t))]),l&&Me(l,e,6,s);const p=n[r+"Once"];if(p){if(!e.emitted)e.emitted={};else if(e.emitted[r])return;e.emitted[r]=!0,Me(p,e,6,s)}}function Ps(e,t,o=!1){const n=t.emitsCache,s=n.get(e);if(s!==void 0)return s;const a=e.emits;let i={},r=!1;if(!P(e)){const l=p=>{const h=Ps(p,t,!0);h&&(r=!0,ae(i,h))};!o&&t.mixins.length&&t.mixins.forEach(l),e.extends&&l(e.extends),e.mixins&&e.mixins.forEach(l)}return!a&&!r?(H(e)&&n.set(e,null),null):(T(a)?a.forEach(l=>i[l]=null):ae(i,a),H(e)&&n.set(e,i),i)}function io(e,t){return!e||!Zt(t)?!1:(t=t.slice(2).replace(/Once$/,""),F(e,t[0].toLowerCase()+t.slice(1))||F(e,Xe(t))||F(e,t))}function yn(e){const{type:t,vnode:o,proxy:n,withProxy:s,propsOptions:[a],slots:i,attrs:r,emit:l,render:p,renderCache:h,props:f,data:C,setupState:k,ctx:x,inheritAttrs:O}=e,J=Nt(e);let z,j;try{if(o.shapeFlag&4){const D=s||n,B=D;z=Ie(p.call(B,D,h,f,k,C,x)),j=r}else{const D=t;z=Ie(D.length>1?D(f,{attrs:r,slots:i,emit:l}):D(f,null)),j=t.props?r:Si(r)}}catch(D){St.length=0,so(D,e,1),z=Ue(Ye)}let N=z;if(j&&O!==!1){const D=Object.keys(j),{shapeFlag:B}=N;D.length&&B&7&&(a&&D.some(Uo)&&(j=Ci(j,a)),N=lt(N,j,!1,!0))}return o.dirs&&(N=lt(N,null,!1,!0),N.dirs=N.dirs?N.dirs.concat(o.dirs):o.dirs),o.transition&&$o(N,o.transition),z=N,Nt(J),z}const Si=e=>{let t;for(const o in e)(o==="class"||o==="style"||Zt(o))&&((t||(t={}))[o]=e[o]);return t},Ci=(e,t)=>{const o={};for(const n in e)(!Uo(n)||!(n.slice(9)in t))&&(o[n]=e[n]);return o};function ki(e,t,o){const{props:n,children:s,component:a}=e,{props:i,children:r,patchFlag:l}=t,p=a.emitsOptions;if(t.dirs||t.transition)return!0;if(o&&l>=0){if(l&1024)return!0;if(l&16)return n?bn(n,i,p):!!i;if(l&8){const h=t.dynamicProps;for(let f=0;f<h.length;f++){const C=h[f];if(i[C]!==n[C]&&!io(p,C))return!0}}}else return(s||r)&&(!r||!r.$stable)?!0:n===i?!1:n?i?bn(n,i,p):!0:!!i;return!1}function bn(e,t,o){const n=Object.keys(t);if(n.length!==Object.keys(e).length)return!0;for(let s=0;s<n.length;s++){const a=n[s];if(t[a]!==e[a]&&!io(o,a))return!0}return!1}function Wi({vnode:e,parent:t},o){for(;t;){const n=t.subTree;if(n.suspense&&n.suspense.activeBranch===e&&(n.el=e.el),n===e)(e=t.vnode).el=o,t=t.parent;else break}}const Bs=e=>e.__isSuspense;function Ii(e,t){t&&t.pendingBranch?T(e)?t.effects.push(...e):t.effects.push(e):Ua(e)}const We=Symbol.for("v-fgt"),ro=Symbol.for("v-txt"),Ye=Symbol.for("v-cmt"),Ao=Symbol.for("v-stc"),St=[];let pe=null;function ge(e=!1){St.push(pe=e?null:[])}function Ti(){St.pop(),pe=St[St.length-1]||null}let It=1;function wn(e,t=!1){It+=e,e<0&&pe&&t&&(pe.hasOnce=!0)}function Os(e){return e.dynamicChildren=It>0?pe||nt:null,Ti(),It>0&&pe&&pe.push(e),e}function Se(e,t,o,n,s,a){return Os(V(e,t,o,n,s,a,!0))}function Di(e,t,o,n,s){return Os(Ue(e,t,o,n,s,!0))}function Us(e){return e?e.__v_isVNode===!0:!1}function mt(e,t){return e.type===t.type&&e.key===t.key}const Ls=({key:e})=>e??null,_t=({ref:e,ref_key:t,ref_for:o})=>(typeof e=="number"&&(e=""+e),e!=null?Q(e)||ee(e)||P(e)?{i:fe,r:e,k:t,f:!!o}:e:null);function V(e,t=null,o=null,n=0,s=null,a=e===We?0:1,i=!1,r=!1){const l={__v_isVNode:!0,__v_skip:!0,type:e,props:t,key:t&&Ls(t),ref:t&&_t(t),scopeId:hs,slotScopeIds:null,children:o,component:null,suspense:null,ssContent:null,ssFallback:null,dirs:null,transition:null,el:null,anchor:null,target:null,targetStart:null,targetAnchor:null,staticCount:0,shapeFlag:a,patchFlag:n,dynamicProps:s,dynamicChildren:null,appContext:null,ctx:fe};return r?(Zo(l,o),a&128&&e.normalize(l)):o&&(l.shapeFlag|=Q(o)?8:16),It>0&&!i&&pe&&(l.patchFlag>0||a&6)&&l.patchFlag!==32&&pe.push(l),l}const Ue=Mi;function Mi(e,t=null,o=null,n=0,s=null,a=!1){if((!e||e===Qa)&&(e=Ye),Us(e)){const r=lt(e,t,!0);return o&&Zo(r,o),It>0&&!a&&pe&&(r.shapeFlag&6?pe[pe.indexOf(e)]=r:pe.push(r)),r.patchFlag=-2,r}if(Ri(e)&&(e=e.__vccOpts),t){t=Ei(t);let{class:r,style:l}=t;r&&!Q(r)&&(t.class=Ro(r)),H(l)&&(No(l)&&!T(l)&&(l=ae({},l)),t.style=zo(l))}const i=Q(e)?1:Bs(e)?128:za(e)?64:H(e)?4:P(e)?2:0;return V(e,t,o,n,s,i,a,!0)}function Ei(e){return e?No(e)||Ss(e)?ae({},e):e:null}function lt(e,t,o=!1,n=!1){const{props:s,ref:a,patchFlag:i,children:r,transition:l}=e,p=t?xi(s||{},t):s,h={__v_isVNode:!0,__v_skip:!0,type:e.type,props:p,key:p&&Ls(p),ref:t&&t.ref?o&&a?T(a)?a.concat(_t(t)):[a,_t(t)]:_t(t):a,scopeId:e.scopeId,slotScopeIds:e.slotScopeIds,children:r,target:e.target,targetStart:e.targetStart,targetAnchor:e.targetAnchor,staticCount:e.staticCount,shapeFlag:e.shapeFlag,patchFlag:t&&e.type!==We?i===-1?16:i|16:i,dynamicProps:e.dynamicProps,dynamicChildren:e.dynamicChildren,appContext:e.appContext,dirs:e.dirs,transition:l,component:e.component,suspense:e.suspense,ssContent:e.ssContent&&lt(e.ssContent),ssFallback:e.ssFallback&&lt(e.ssFallback),el:e.el,anchor:e.anchor,ctx:e.ctx,ce:e.ce};return l&&n&&$o(h,l.clone(h)),h}function Kt(e=" ",t=0){return Ue(ro,null,e,t)}function zt(e="",t=!1){return t?(ge(),Di(Ye,null,e)):Ue(Ye,null,e)}function Ie(e){return e==null||typeof e=="boolean"?Ue(Ye):T(e)?Ue(We,null,e.slice()):Us(e)?qe(e):Ue(ro,null,String(e))}function qe(e){return e.el===null&&e.patchFlag!==-1||e.memo?e:lt(e)}function Zo(e,t){let o=0;const{shapeFlag:n}=e;if(t==null)t=null;else if(T(t))o=16;else if(typeof t=="object")if(n&65){const s=t.default;s&&(s._c&&(s._d=!1),Zo(e,s()),s._c&&(s._d=!0));return}else{o=32;const s=t._;!s&&!Ss(t)?t._ctx=fe:s===3&&fe&&(fe.slots._===1?t._=1:(t._=2,e.patchFlag|=1024))}else P(t)?(t={default:t,_ctx:fe},o=32):(t=String(t),n&64?(o=16,t=[Kt(t)]):o=8);e.children=t,e.shapeFlag|=o}function xi(...e){const t={};for(let o=0;o<e.length;o++){const n=e[o];for(const s in n)if(s==="class")t.class!==n.class&&(t.class=Ro([t.class,n.class]));else if(s==="style")t.style=zo([t.style,n.style]);else if(Zt(s)){const a=t[s],i=n[s];i&&a!==i&&!(T(a)&&a.includes(i))&&(t[s]=a?[].concat(a,i):i)}else s!==""&&(t[s]=n[s])}return t}function Ce(e,t,o,n=null){Me(e,t,7,[o,n])}const Pi=ws();let Bi=0;function Oi(e,t,o){const n=e.type,s=(t?t.appContext:e.appContext)||Pi,a={uid:Bi++,vnode:e,type:n,parent:t,appContext:s,root:null,next:null,subTree:null,effect:null,update:null,job:null,scope:new oa(!0),render:null,proxy:null,exposed:null,exposeProxy:null,withProxy:null,provides:t?t.provides:Object.create(s.provides),ids:t?t.ids:["",0,0],accessCache:null,renderCache:[],components:null,directives:null,propsOptions:ks(n,s),emitsOptions:Ps(n,s),emit:null,emitted:null,propsDefaults:q,inheritAttrs:n.inheritAttrs,ctx:q,data:q,props:q,attrs:q,slots:q,refs:q,setupState:q,setupContext:null,suspense:o,suspenseId:o?o.pendingId:0,asyncDep:null,asyncResolved:!1,isMounted:!1,isUnmounted:!1,isDeactivated:!1,bc:null,c:null,bm:null,m:null,bu:null,u:null,um:null,bum:null,da:null,a:null,rtg:null,rtc:null,ec:null,sp:null};return a.ctx={_:a},a.root=t?t.root:a,a.emit=vi.bind(null,a),e.ce&&e.ce(a),a}let se=null,Qt,Po;{const e=to(),t=(o,n)=>{let s;return(s=e[o])||(s=e[o]=[]),s.push(n),a=>{s.length>1?s.forEach(i=>i(a)):s[0](a)}};Qt=t("__VUE_INSTANCE_SETTERS__",o=>se=o),Po=t("__VUE_SSR_SETTERS__",o=>Tt=o)}const Et=e=>{const t=se;return Qt(e),e.scope.on(),()=>{e.scope.off(),Qt(t)}},An=()=>{se&&se.scope.off(),Qt(null)};function Fs(e){return e.vnode.shapeFlag&4}let Tt=!1;function Ui(e,t=!1,o=!1){t&&Po(t);const{props:n,children:s}=e.vnode,a=Fs(e);ri(e,n,a,t),hi(e,s,o||t);const i=a?Li(e,t):void 0;return t&&Po(!1),i}function Li(e,t){const o=e.type;e.accessCache=Object.create(null),e.proxy=new Proxy(e.ctx,Za);const{setup:n}=o;if(n){Le();const s=e.setupContext=n.length>1?zi(e):null,a=Et(e),i=Mt(n,e,0,[e.props,s]),r=Ln(i);if(Fe(),a(),(r||e.sp)&&!At(e)&&ds(e),r){if(i.then(An,An),t)return i.then(l=>{vn(e,l)}).catch(l=>{so(l,e,0)});e.asyncDep=i}else vn(e,i)}else zs(e)}function vn(e,t,o){P(t)?e.type.__ssrInlineRender?e.ssrRender=t:e.render=t:H(t)&&(e.setupState=is(t)),zs(e)}function zs(e,t,o){const n=e.type;e.render||(e.render=n.render||Te);{const s=Et(e);Le();try{Xa(e)}finally{Fe(),s()}}}const Fi={get(e,t){return X(e,"get",""),e[t]}};function zi(e){const t=o=>{e.exposed=o||{}};return{attrs:new Proxy(e.attrs,Fi),slots:e.slots,emit:e.emit,expose:t}}function co(e){return e.exposed?e.exposeProxy||(e.exposeProxy=new Proxy(is(Ca(e.exposed)),{get(t,o){if(o in t)return t[o];if(o in vt)return vt[o](e)},has(t,o){return o in t||o in vt}})):e.proxy}function Ri(e){return P(e)&&"__vccOpts"in e}const Rs=(e,t)=>Ma(e,t,Tt),qi="3.5.16";/**
* @vue/runtime-dom v3.5.16
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/let Bo;const Sn=typeof window<"u"&&window.trustedTypes;if(Sn)try{Bo=Sn.createPolicy("vue",{createHTML:e=>e})}catch{}const qs=Bo?e=>Bo.createHTML(e):e=>e,_i="http://www.w3.org/2000/svg",Ki="http://www.w3.org/1998/Math/MathML",Pe=typeof document<"u"?document:null,Cn=Pe&&Pe.createElement("template"),Vi={insert:(e,t,o)=>{t.insertBefore(e,o||null)},remove:e=>{const t=e.parentNode;t&&t.removeChild(e)},createElement:(e,t,o,n)=>{const s=t==="svg"?Pe.createElementNS(_i,e):t==="mathml"?Pe.createElementNS(Ki,e):o?Pe.createElement(e,{is:o}):Pe.createElement(e);return e==="select"&&n&&n.multiple!=null&&s.setAttribute("multiple",n.multiple),s},createText:e=>Pe.createTextNode(e),createComment:e=>Pe.createComment(e),setText:(e,t)=>{e.nodeValue=t},setElementText:(e,t)=>{e.textContent=t},parentNode:e=>e.parentNode,nextSibling:e=>e.nextSibling,querySelector:e=>Pe.querySelector(e),setScopeId(e,t){e.setAttribute(t,"")},insertStaticContent(e,t,o,n,s,a){const i=o?o.previousSibling:t.lastChild;if(s&&(s===a||s.nextSibling))for(;t.insertBefore(s.cloneNode(!0),o),!(s===a||!(s=s.nextSibling)););else{Cn.innerHTML=qs(n==="svg"?`<svg>${e}</svg>`:n==="mathml"?`<math>${e}</math>`:e);const r=Cn.content;if(n==="svg"||n==="mathml"){const l=r.firstChild;for(;l.firstChild;)r.appendChild(l.firstChild);r.removeChild(l)}t.insertBefore(r,o)}return[i?i.nextSibling:t.firstChild,o?o.previousSibling:t.lastChild]}},Yi=Symbol("_vtc");function Hi(e,t,o){const n=e[Yi];n&&(t=(t?[t,...n]:[...n]).join(" ")),t==null?e.removeAttribute("class"):o?e.setAttribute("class",t):e.className=t}const kn=Symbol("_vod"),ji=Symbol("_vsh"),Ni=Symbol(""),Gi=/(^|;)\s*display\s*:/;function $i(e,t,o){const n=e.style,s=Q(o);let a=!1;if(o&&!s){if(t)if(Q(t))for(const i of t.split(";")){const r=i.slice(0,i.indexOf(":")).trim();o[r]==null&&Vt(n,r,"")}else for(const i in t)o[i]==null&&Vt(n,i,"");for(const i in o)i==="display"&&(a=!0),Vt(n,i,o[i])}else if(s){if(t!==o){const i=n[Ni];i&&(o+=";"+i),n.cssText=o,a=Gi.test(o)}}else t&&e.removeAttribute("style");kn in e&&(e[kn]=a?n.display:"",e[ji]&&(n.display="none"))}const Wn=/\s*!important$/;function Vt(e,t,o){if(T(o))o.forEach(n=>Vt(e,t,n));else if(o==null&&(o=""),t.startsWith("--"))e.setProperty(t,o);else{const n=Qi(e,t);Wn.test(o)?e.setProperty(Xe(n),o.replace(Wn,""),"important"):e[n]=o}}const In=["Webkit","Moz","ms"],vo={};function Qi(e,t){const o=vo[t];if(o)return o;let n=Ke(t);if(n!=="filter"&&n in e)return vo[t]=n;n=Rn(n);for(let s=0;s<In.length;s++){const a=In[s]+n;if(a in e)return vo[t]=a}return t}const Tn="http://www.w3.org/1999/xlink";function Dn(e,t,o,n,s,a=ea(t)){n&&t.startsWith("xlink:")?o==null?e.removeAttributeNS(Tn,t.slice(6,t.length)):e.setAttributeNS(Tn,t,o):o==null||a&&!_n(o)?e.removeAttribute(t):e.setAttribute(t,a?"":De(o)?String(o):o)}function Mn(e,t,o,n,s){if(t==="innerHTML"||t==="textContent"){o!=null&&(e[t]=t==="innerHTML"?qs(o):o);return}const a=e.tagName;if(t==="value"&&a!=="PROGRESS"&&!a.includes("-")){const r=a==="OPTION"?e.getAttribute("value")||"":e.value,l=o==null?e.type==="checkbox"?"on":"":String(o);(r!==l||!("_value"in e))&&(e.value=l),o==null&&e.removeAttribute(t),e._value=o;return}let i=!1;if(o===""||o==null){const r=typeof e[t];r==="boolean"?o=_n(o):o==null&&r==="string"?(o="",i=!0):r==="number"&&(o=0,i=!0)}try{e[t]=o}catch{}i&&e.removeAttribute(s||t)}function Je(e,t,o,n){e.addEventListener(t,o,n)}function Ji(e,t,o,n){e.removeEventListener(t,o,n)}const En=Symbol("_vei");function Zi(e,t,o,n,s=null){const a=e[En]||(e[En]={}),i=a[t];if(n&&i)i.value=n;else{const[r,l]=Xi(t);if(n){const p=a[t]=or(n,s);Je(e,r,p,l)}else i&&(Ji(e,r,i,l),a[t]=void 0)}}const xn=/(?:Once|Passive|Capture)$/;function Xi(e){let t;if(xn.test(e)){t={};let n;for(;n=e.match(xn);)e=e.slice(0,e.length-n[0].length),t[n[0].toLowerCase()]=!0}return[e[2]===":"?e.slice(3):Xe(e.slice(2)),t]}let So=0;const er=Promise.resolve(),tr=()=>So||(er.then(()=>So=0),So=Date.now());function or(e,t){const o=n=>{if(!n._vts)n._vts=Date.now();else if(n._vts<=o.attached)return;Me(nr(n,o.value),t,5,[n])};return o.value=e,o.attached=tr(),o}function nr(e,t){if(T(t)){const o=e.stopImmediatePropagation;return e.stopImmediatePropagation=()=>{o.call(e),e._stopped=!0},t.map(n=>s=>!s._stopped&&n&&n(s))}else return t}const Pn=e=>e.charCodeAt(0)===111&&e.charCodeAt(1)===110&&e.charCodeAt(2)>96&&e.charCodeAt(2)<123,sr=(e,t,o,n,s,a)=>{const i=s==="svg";t==="class"?Hi(e,n,i):t==="style"?$i(e,o,n):Zt(t)?Uo(t)||Zi(e,t,o,n,a):(t[0]==="."?(t=t.slice(1),!0):t[0]==="^"?(t=t.slice(1),!1):ar(e,t,n,i))?(Mn(e,t,n),!e.tagName.includes("-")&&(t==="value"||t==="checked"||t==="selected")&&Dn(e,t,n,i,a,t!=="value")):e._isVueCE&&(/[A-Z]/.test(t)||!Q(n))?Mn(e,Ke(t),n,a,t):(t==="true-value"?e._trueValue=n:t==="false-value"&&(e._falseValue=n),Dn(e,t,n,i))};function ar(e,t,o,n){if(n)return!!(t==="innerHTML"||t==="textContent"||t in e&&Pn(t)&&P(o));if(t==="spellcheck"||t==="draggable"||t==="translate"||t==="autocorrect"||t==="form"||t==="list"&&e.tagName==="INPUT"||t==="type"&&e.tagName==="TEXTAREA")return!1;if(t==="width"||t==="height"){const s=e.tagName;if(s==="IMG"||s==="VIDEO"||s==="CANVAS"||s==="SOURCE")return!1}return Pn(t)&&Q(o)?!1:t in e}const Jt=e=>{const t=e.props["onUpdate:modelValue"]||!1;return T(t)?o=>Rt(t,o):t};function ir(e){e.target.composing=!0}function Bn(e){const t=e.target;t.composing&&(t.composing=!1,t.dispatchEvent(new Event("input")))}const ct=Symbol("_assign"),rr={created(e,{modifiers:{lazy:t,trim:o,number:n}},s){e[ct]=Jt(s);const a=n||s.props&&s.props.type==="number";Je(e,t?"change":"input",i=>{if(i.target.composing)return;let r=e.value;o&&(r=r.trim()),a&&(r=Co(r)),e[ct](r)}),o&&Je(e,"change",()=>{e.value=e.value.trim()}),t||(Je(e,"compositionstart",ir),Je(e,"compositionend",Bn),Je(e,"change",Bn))},mounted(e,{value:t}){e.value=t??""},beforeUpdate(e,{value:t,oldValue:o,modifiers:{lazy:n,trim:s,number:a}},i){if(e[ct]=Jt(i),e.composing)return;const r=(a||e.type==="number")&&!/^0\d/.test(e.value)?Co(e.value):e.value,l=t??"";r!==l&&(document.activeElement===e&&e.type!=="range"&&(n&&t===o||s&&e.value.trim()===l)||(e.value=l))}},cr={deep:!0,created(e,t,o){e[ct]=Jt(o),Je(e,"change",()=>{const n=e._modelValue,s=lr(e),a=e.checked,i=e[ct];if(T(n)){const r=Kn(n,s),l=r!==-1;if(a&&!l)i(n.concat(s));else if(!a&&l){const p=[...n];p.splice(r,1),i(p)}}else if(Xt(n)){const r=new Set(n);a?r.add(s):r.delete(s),i(r)}else i(_s(e,a))})},mounted:On,beforeUpdate(e,t,o){e[ct]=Jt(o),On(e,t,o)}};function On(e,{value:t,oldValue:o},n){e._modelValue=t;let s;if(T(t))s=Kn(t,n.props.value)>-1;else if(Xt(t))s=t.has(n.props.value);else{if(t===o)return;s=oo(t,_s(e,!0))}e.checked!==s&&(e.checked=s)}function lr(e){return"_value"in e?e._value:e.value}function _s(e,t){const o=t?"_trueValue":"_falseValue";return o in e?e[o]:t}const ur=ae({patchProp:sr},Vi);let Un;function hr(){return Un||(Un=pi(ur))}const dr=(...e)=>{const t=hr().createApp(...e),{mount:o}=t;return t.mount=n=>{const s=fr(n);if(!s)return;const a=t._component;!P(a)&&!a.render&&!a.template&&(a.template=s.innerHTML),s.nodeType===1&&(s.textContent="");const i=o(s,!1,pr(s));return s instanceof Element&&(s.removeAttribute("v-cloak"),s.setAttribute("data-v-app","")),i},t};function pr(e){if(e instanceof SVGElement)return"svg";if(typeof MathMLElement=="function"&&e instanceof MathMLElement)return"mathml"}function fr(e){return Q(e)?document.querySelector(e):e}const mr=JSON.parse(`[{"question":"A company wants to have a secure way of generating, storing and managing cryptographic exclusive access for the keys. Which of the following can be used for this purpose?","choices":{"A":"Use KMS and the normal KMS encryption keys","B":"Use KMS and use an external key material","C":"Use S3 Server Side encryption","D":"Use Cloud HSM"},"answer":"D","explanation":"The AWS Documentation mentions the following The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. AWS and AWS Marketplace partners offer a variety of solutions for protecting sensitive data within the AWS platform, but for some applications and data subject to contractual or regulatory mandates for managing cryptographic keys, additional protection may be necessary. CloudHSM complements existing data protection solutions and allows you to protect your encryption keys within HSMs that are desigr and validated to government standards for secure key management. CloudHSM allows you to securely generate, store and manage cryptographic keys used for data encryption in a way that keys are accessible only by you. Option A.B and Care invalid because in all of these cases, the management of the key will be with AWS. Here the question specifically mentions that you want to have exclusive access over the keys. This can be achieved with Cloud HSM For more information on CloudHSM, please visit the following URL: https://aws.amazon.com/cloudhsm/faq: The correct answer is: Use Cloud HSM"},{"question":"A company is hosting a website that must be accessible to users for HTTPS traffic. Also port 22 should be open for administrative purposes. The administrator's workstation has a static IP address of 203.0.113.1/32. Which of the following security group configurations are the MOST secure but still functional to support these requirements? Choose 2 answers from the options given below","choices":{"A":"Port 443 coming from 0.0.0.0/0","B":"Port 443 coming from 10.0.0.0/16","C":"Port 22 coming from 0.0.0.0/0","D":"Port 22 coming from 203.0.113.1/32"},"answer":"AD","explanation":"Since HTTPS traffic is required for all users on the Internet, Port 443 should be open on all IP addresses. For port 22, the traffic should be restricted to an internal subnet. Option B is invalid, because this only allow traffic from a particular CIDR block and not from the internet Option C is invalid because allowing port 22 from the internet is a security risk For more information on AWS Security Groups, please visit the following UR https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usins-network-secunty.htmll The correct answers are: Port 443 coming from 0.0.0.0/0, Port 22 coming from 203.0.113.1 /32"},{"question":"Your company has defined privileged users for their AWS Account. These users are administrators for key resources defined in the company. There is now a mandate to enhance the security authentication for these users. How can this be accomplished?","choices":{"A":"Enable MFA for these user accounts","B":"Enable versioning for these user accounts","C":"Enable accidental deletion for these user accounts","D":"Disable root access for the users"},"answer":"A","explanation":"The AWS Documentation mentions the following as a best practices for IAM users. For extra security, enable multi-factor authentication (MFA) for privileged IAM users (users who are allowed access to sensitive resources or APIs). With MFA, users have a device that generates unique authentication code (a one-time password, or OTP). Users must provide both their normal credentials (like their user name and password) and the OTP. The MFA device can either be a special piece of hardware, or it can be a virtual device (for example, it can run in an app on a smartphone). Option B,C and D are invalid because no such security options are available in AWS For more information on IAM best practices, please visit the below URL https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html The correct answer is: Enable MFA for these user accounts"},{"question":"An application running on EC2 instances must use a username and password to access a database. The developer has stored those secrets in the SSM Parameter Store with type SecureString using the default KMS CMK. Which combination of configuration steps will allow the application to access the secrets via the API? Select 2 answers from the options below","choices":{"A":"Add the EC2 instance role as a trusted service to the SSM service role.","B":"Add permission to use the KMS key to decrypt to the SSM service role.","C":"Add permission to read the SSM parameter to the EC2 instance role..","D":"Add permission to use the KMS key to decrypt to the EC2 instance role","E":"Add the SSM service role as a trusted service to the EC2 instance rol"},"answer":"CD","explanation":"The below example policy from the AWS Documentation is required to be given to the EC2 Instance in order to read a secure string from AWS KMS. Permissions need to be given to the Get Parameter API and the KMS API call to decrypt the secret. Option A is invalid because roles can be attached to EC2 and not EC2 roles to SSM Option B is invalid because the KMS key does not need to decrypt the SSM service role. Option E is invalid because this configuration is valid For more information on the parameter store, please visit the below URL: https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.htmll The correct answers are: Add permission to read the SSM parameter to the EC2 instance role., Add permission to use the KMS key to decrypt to the EC2 instance role"},{"question":"When you enable automatic key rotation for an existing CMK key where the backing key is managed by AWS, after how long is the key rotated?","choices":{"A":"After 30 days","B":"After 128 days","C":"After 365 days","D":"After 3 years"},"answer":"D","explanation":"The AWS Documentation states the following  AWS managed CM Ks: You cannot manage key rotation for AWS managed CMKs. AWS KMS automatically rotates AWS managed keys every three years (1095 days). Note: AWS-managed CMKs are rotated every 3yrs, Customer-Managed CMKs are rotated every 365- days from when rotation is enabled. Option A, B, C are invalid because the dettings for automatic key rotation is not changeable. For more information on key rotation please visit the below URL https://docs.aws.amazon.com/kms/latest/developereuide/rotate-keys.html AWS managed CMKs are CMKs in your account that are created, managed, and used on your behalf by an AWS service that is integrated with AWS KMS. This CMK is unique to your AWS account and region. Only the service that created the AWS managed CMK can use it You can login to you IAM dashbaord . Click on \\"Encryption Keys\\" You will find the list based on the services you are using as follows:  aws/elasticfilesystem 1 aws/lightsail  aws/s3  aws/rds and many more Detailed Guide: KMS You can recognize AWS managed CMKs because their aliases have the format aws/service-name, such as aws/redshift. Typically, a service creates its AWS managed CMK in your account when you set up the service or the first time you use the CMfC The AWS services that integrate with AWS KMS can use it in many different ways. Some services create AWS managed CMKs in your account. Other services require that you specify a customer managed CMK that you have created. And, others support both types of CMKs to allow you the ease of an AWS managed CMK or the control of a customer-managed CMK Rotation period for CMKs is as follows:  AWS managed CMKs: 1095 days  Customer managed CMKs: 365 days Since question mentions about \\"CMK where backing keys is managed by AWS\\", its Amazon(AWS) managed and its rotation period turns out to be 1095 days{every 3 years) For more details, please check below AWS Docs: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html The correct answer is: After 3 years"},{"question":"You are deivising a policy to allow users to have the ability to access objects in a bucket called appbucket. You define the below custom bucket policy But when you try to apply the policy you get the error \\"Action does not apply to any resource(s) in statement.\\" What should be done to rectify the error","choices":{"A":"Change the IAM permissions by applying PutBucketPolicy permissions.","B":"Verify that the policy has the same name as the bucket nam","C":"If no","D":"make it the same.","E":"Change the Resource section to \\"arn:aws:s3:::appbucket/*'.","F":"Create the bucket \\"appbucket\\" and then apply the polic"},"answer":"C","explanation":"When you define access to objects in a bucket you need to ensure that you specify to which objects in the bucket access needs to be given to. In this case, the * can be used to assign the permission to all objects in the bucket Option A is invalid because the right permissions are already provided as per the question requirement Option B is invalid because it is not necessary that the policy has the same name as the bucket Option D is invalid because this should be the default flow for applying the policy For more information on bucket policies please visit the below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.htmll The correct answer is: Change the Resource section to \\"arn:aws:s3:::appbucket/\\""},{"question":"A company wants to have an Intrusion detection system available for their VPC in AWS. They want to have complete control over the system. Which of the following would be ideal to implement?","choices":{"A":"Use AWS WAF to catch all intrusions occurring on the systems in the VPC","B":"Use a custom solution available in the AWS Marketplace","C":"Use VPC Flow logs to detect the issues and flag them accordingly.","D":"Use AWS Cloudwatch to monitor all traffic"},"answer":"B","explanation":"Sometimes companies want to have custom solutions in place for monitoring Intrusions to their systems. In such a case, you can use the AWS Marketplace for looking at custom solutions. Option A.C and D are all invalid because they cannot be used to conduct intrusion detection or prevention. For more information on using custom security solutions please visit the below URL https://d1.awsstatic.com/Marketplace/security/AWSMP_Security_Solution%200verview.pdf For more information on using custom security solutions please visit the below URL: https://d1 .awsstatic.com/Marketplace/security/AWSMP Security Solution%20Overview.pd1 The correct answer is: Use a custom solution available in the AWS Marketplace"},{"question":"Your IT Security department has mandated that all data on EBS volumes created for underlying EC2 Instances need to be encrypted. Which of the following can help achieve this?","choices":{"A":"AWS KMS API","B":"AWS Certificate Manager","C":"API Gateway with STS","D":"IAM Access Key"},"answer":"A","explanation":"The AWS Documentation mentions the following on AWS KMS AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your dat A. AWS KMS is integrated with other AWS services including Amazon Elastic Block Store (Amazon EBS), Amazon Simple Storage Service (Amazon S3), Amazon Redshift Amazon Elastic Transcoder, Amazon WorkMail, Amazon Relational Database Service (Amazon RDS), and others to make it simple to encrypt your data with encryption keys that you manage Option B is incorrect - The AWS Certificate manager can be used to generate SSL certificates that can be used to encrypt traffic transit, but not at rest Option C is incorrect is again used for issuing tokens when using API gateway for traffic in transit. Option D is used for secure access to EC2 Instances For more information on AWS KMS, please visit the following URL: https://docs.aws.amazon.com/kms/latest/developereuide/overview.htmll The correct answer is: AWS KMS API"},{"question":"Your company has mandated that all calls to the AWS KMS service be recorded. How can this be achieved?","choices":{"A":"Enable logging on the KMS service","B":"Enable a trail in Cloudtrail","C":"Enable Cloudwatch logs","D":"Use Cloudwatch metrics"},"answer":"B","explanation":"The AWS Documentation states the following AWS KMS is integrated with CloudTrail, a service that captures API calls made by or on behalf of AWS KMS in your AWS account and delivers the log files to an Amazon S3 bucket that you specify. CloudTrail captures API calls from the AWS KMS console or from the AWS KMS API. Using the information collected by CloudTrail, you can determine what request was made, the source IP address from which the request was made, who made the request when it was made, and so on. Option A is invalid because logging is not possible in the KMS service Option C and D are invalid because Cloudwatch cannot be used to monitor API calls For more information on logging using Cloudtrail please visit the below URL https://docs.aws.amazon.com/kms/latest/developerguide/loeeing-usine-cloudtrail.html The correct answer is: Enable a trail in Cloudtrail Jubmit your Feedback/Queries to our Experts"},{"question":"A company is using CloudTrail to log all AWS API activity for all regions in all of its accounts. The CISO has asked that additional steps be taken to protect the integrity of the log files. What combination of steps will protect the log files from intentional or unintentional alteration? Choose 2 answers from the options given below","choices":{"A":"Create an S3 bucket in a dedicated log account and grant the other accounts write only acces","B":"Deliver all log files from every account to this S3 bucket.","C":"Write a Lambda function that queries the Trusted Advisor Cloud Trail check","D":"Run the function every 10 minutes.","E":"Enable CloudTrail log file integrity validation","F":"Use Systems Manager Configuration Compliance to continually monitor the access policies of S3 buckets containing Cloud Trail logs.","G":"Create a Security Group that blocks all traffic except calls from the CloudTrail servic","H":"Associate the security group with) all the Cloud Trail destination S3 buckets."},"answer":"AC","explanation":"The AWS Documentation mentions the following To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it you can use CloudTrail log fill integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. Option B is invalid because there is no such thing as Trusted Advisor Cloud Trail checks Option D is invalid because Systems Manager cannot be used for this purpose. Option E is invalid because Security Groups cannot be used to block calls from other services For more information on Cloudtrail log file validation, please visit the below URL: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-loe-file-validationintro. htmll For more information on delivering Cloudtrail logs from multiple accounts, please visit the below URL: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multipleaccounts. html The correct answers are: Create an S3 bucket in a dedicated log account and grant the other accounts write only access. Deliver all log files from every account to this S3 bucket, Enable Cloud Trail log file integrity validation"},{"question":"You have just received an email from AWS Support stating that your AWS account might have been compromised. Which of the following steps would you look to carry out immediately. Choose 3 answers from the options below.","choices":{"A":"Change the root account password.","B":"Rotate all IAM access keys","C":"Keep all resources running to avoid disruption","D":"Change the password for all IAM user"},"answer":"ABD","explanation":"One of the articles from AWS mentions what should be done in such a scenario If you suspect that your account has been compromised, or if you have received a notification from AWS that the account has been compromised, perform the following tasks: Change your AWS root account password and the passwords of any IAM users. Delete or rotate all root and AWS Identity and Access Management (IAM) access keys. Delete any resources on your account you didn't create, especially running EC2 instances, EC2 spot bids, or IAM users. Respond to any notifications you received from AWS Support through the AWS Support Center. Option C is invalid because there could be compromised instances or resources running on your environment. They should be shutdown or stopped immediately. For more information on the article, please visit the below URL: https://aws.amazon.com/premiumsupport/knowledee-center/potential-account-compromise> The correct answers are: Change the root account password. Rotate all IAM access keys. Change the password for all IAM users."},{"question":"Your company is planning on hosting an internal network in AWS. They want machines in the VPC to authenticate using private certificates. They want to minimize the work and maintenance in working with certificates. What is the ideal way to fulfil this requirement.","choices":{"A":"Consider using Windows Server 2016 Certificate Manager","B":"Consider using AWS Certificate Manager","C":"Consider using AWS Access keys to generate the certificates","D":"Consider using AWS Trusted Advisor for managing the certificates"},"answer":"B","explanation":"The AWS Documentation mentions the following ACM is tightly linked with AWS Certificate Manager Private Certificate Authority. You can use ACM PCA to create a private certificate authority (CA) and then use ACM to issue private certificates. These are SSL/TLS X.509 certificates that identify users, computers, applications, services, servers, and other devices internally. Private certificates cannot be publicly trusted Option A is partially invalid. Windows Server 2016 Certificate Manager can be used but since there is a requirement to \\"minimize the work and maintenance\\", AWS Certificate Manager should be used Option C and D are invalid because these cannot be used for managing certificates. For more information on ACM, please visit the below URL: https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html The correct answer is: Consider using AWS Certificate Manager"},{"question":"You have enabled Cloudtrail logs for your company's AWS account. In addition, the IT Security department has mentioned that the logs need to be encrypted. How can this be achieved?","choices":{"A":"Enable SSL certificates for the Cloudtrail logs","B":"There is no need to do anything since the logs will already be encrypted","C":"Enable Server side encryption for the trail","D":"Enable Server side encryption for the destination S3 bucket"},"answer":"B","explanation":"The AWS Documentation mentions the following. By default CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encryption your log files with an AWS Key Management Service (AWS KMS) key. You can store your log files in your bucket for as long as you want. You can also define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about lo file delivery and validation, you can set up Amazon SNS notifications. Option A.C and D are not valid since logs will already be encrypted For more information on how Cloudtrail works, please visit the following URL: https://docs.aws.amazon.com/awscloudtrail/latest/usereuide/how-cloudtrail- works.htmll The correct answer is: There is no need to do anything since the logs will already be encrypted"},{"question":"A security team is creating a response plan in the event an employee executes unauthorized actions on AWS infrastructure. They want to include steps to determine if the employee's IAM permissions changed as part of the incident. What steps should the team document in the plan?","choices":{"A":"Use AWS Config to examine the employee's IAM permissions prior to the incident and compare them to the employee's current IAM permissions.","B":"Use Made to examine the employee's IAM permissions prior to the incident and compare them to the employee's A current IAM permissions.","C":"Use CloudTrail to examine the employee's IAM permissions prior to the incident and compare them to the employee's current IAM permissions.","D":"Use Trusted Advisor to examine the employee's IAM permissions prior to the incident and compare them to the employee's current IAM permissions."},"answer":"A","explanation":"You can use the AWSConfig history to see the history of a particular item. The below snapshot shows an example configuration for a user in AWS Config Option B,C and D are all invalid because these services cannot be used to see the history of a particular configuration item. This can only be accomplished by AWS Config. For more information on tracking changes in AWS Config, please visit the below URL: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/TrackineChanees.htmll The correct answer is: Use AWS Config to examine the employee's IAM permissions prior to the incident and compare them the employee's current IAM permissions."},{"question":"A security team must present a daily briefing to the CISO that includes a report of which of the company's thousands of EC2 instances and on-premises servers are missing the latest security patches. All instances/servers must be brought into compliance within 24 hours so they do not show up on the next day's report. How can the security team fulfill these requirements?","choices":{"A":"Use Amazon QuickSight and Cloud Trail to generate the report of out of compliance instances/server","B":"Redeploy all out of compliance instances/servers using an AMI with the latest patches.","C":"Use Systems Manger Patch Manger to generate the report of out of compliance instances/ server","D":"Use Systems Manager Patch Manger to install the missing patches.","E":"Use Systems Manger Patch Manger to generate the report of out of compliance instances/ server","F":"Redeploy all out of1 compliance instances/servers using an AMI with the latest patches.","G":"Use Trusted Advisor to generate the report of out of compliance instances/server","H":"Use Systems Manger Patch Manger to install the missing patches."},"answer":"B","explanation":"Use the Systems Manger Patch Manger to generate the report and also install the missing patches The AWS Documentation mentions the following AWS Systems Manager Patch Manager automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fileets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. Option A is invalid because Amazon QuickSight and Cloud Trail cannot be used to generate the list of servers that don't meet compliance needs. Option C is wrong because deploying instances via new AMI'S would impact the applications hosted on these servers Option D is invalid because Amazon Trusted Advisor cannot be used to generate the list of servers that don't meet compliance needs. For more information on the AWS Patch Manager, please visit the below URL: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager- patch.html ( The correct answer is: Use Systems Manger Patch Manger to generate the report of out of compliance instances/ servers. Use Systems Manager Patch Manger to install the missing patches."},{"question":"You have setup a set of applications across 2 VPC's. You have also setup VPC Peering. The applications are still not able to communicate across the Peering connection. Which network troubleshooting steps should be taken to resolve the issue?","choices":{"A":"Ensure the applications are hosted in a public subnet","B":"Check to see if the VPC has an Internet gateway attached.","C":"Check to see if the VPC has a NAT gateway attached.","D":"Check the Route tables for the VPC's"},"answer":"D","explanation":"After the VPC peering connection is established, you need to ensure that the route tables are modified to ensure traffic can between the VPCs Option A ,B and C are invalid because allowing access the Internet gateway and usage of public subnets can help for Inter, access, but not for VPC Peering. For more information on VPC peering routing, please visit the below URL: .com/AmazonVPC/latest/Peeri The correct answer is: Check the Route tables for the VPCs"},{"question":"A company requires that data stored in AWS be encrypted at rest. Which of the following approaches achieve this requirement? Select 2 answers from the options given below.","choices":{"A":"When storing data in Amazon EBS, use only EBS-optimized Amazon EC2 instances.","B":"When storing data in EBS, encrypt the volume by using AWS KMS.","C":"When storing data in Amazon S3, use object versioning and MFA Delete.","D":"When storing data in Amazon EC2 Instance Store, encrypt the volume by using KMS.","E":"When storing data in S3, enable server-side encryptio"},"answer":"BE","explanation":"The AWS Documentation mentions the following To create an encrypted Amazon EBS volume, select the appropriate box in the Amazon EBS section of the Amazon EC2 console. You can use a custom customer master key (CMK) by choosing one from the list that appears below the encryption box. If you do not specify a custom CMK, Amazon EBS uses the AWS- managed CMK for Amazon EBS in your account. If there is no AWS-managed CMK for Amazon EBS in your account, Amazon EBS creates one. Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options of protecting data at rest in Amazon S3.  Use Server-Side Encryption - You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.  Use Client-Side Encryption - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. Option A is invalid because using EBS-optimized Amazon EC2 instances alone will not guarantee protection of instances at rest. Option C is invalid because this will not encrypt data at rest for S3 objects. Option D is invalid because you don't store data in Instance store. For more information on EBS encryption, please visit the below URL: https://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html For more information on S3 encryption, please visit the below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsinEEncryption.html The correct answers are: When storing data in EBS, encrypt the volume by using AWS KMS. When storing data in S3, enable server-side encryption."},{"question":"You want to ensure that you keep a check on the Active EBS Volumes, Active snapshots and Elastic IP addresses you use so that you don't go beyond the service limit. Which of the below services can help in this regard?","choices":{"A":"AWS Cloudwatch","B":"AWS EC2","C":"AWS Trusted Advisor","D":"AWS SNS"},"answer":"C","explanation":"Below is a snapshot of the service limits that the Trusted Advisor can monitor Option A is invalid because even though you can monitor resources, it cannot be checked against the service limit. Option B is invalid because this is the Elastic Compute cloud service Option D is invalid because it can be send notification but not check on service limit For more information on the Trusted Advisor monitoring, please visit the below URL: https://aws.amazon.com/premiumsupport/ta-faqs> The correct answer is: AWS Trusted Advisor"},{"question":"A company has a legacy application that outputs all logs to a local text file. Logs from all applications running on AWS must be continually monitored for security related messages. What can be done to allow the company to deploy the legacy application on Amazon EC2 and still meet the monitoring requirement?","choices":{"A":"Create a Lambda function that mounts the EBS volume with the logs and scans the logs for security incident","B":"Trigger the function every 5 minutes with a scheduled Cloudwatch event.","C":"Send the local text log files to CloudWatch Logs and configure a CloudWatch metric filte","D":"Trigger cloudwatch alarms based on the metrics.","E":"Install the Amazon inspector agent on any EC2 instance running the legacy applicatio","F":"Generate CloudWatch alerts a based on any Amazon inspector findings.","G":"Export the local text log files to CloudTrai","H":"Create a Lambda function that queries the CloudTrail logs for security ' incidents using Athena."},"answer":"B","explanation":"One can send the log files to Cloudwatch Logs. Log files can also be sent from On-premise servers. You can then specify metrii to search the logs for any specific values. And then create alarms based on these metrics. Option A is invalid because this will be just a long over drawn process to achieve this requirement Option C is invalid because AWS Inspector cannot be used to monitor for security related messages. Option D is invalid because files cannot be exported to AWS Cloudtrail For more information on Cloudwatch logs agent please visit the below URL: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2lnstance.hti The correct answer is: Send the local text log files to Cloudwatch Logs and configure a Cloudwatch metric filter. Trigger cloudwatch alarms based on the metrics."},{"question":"An application running on EC2 instances in a VPC must call an external web service via TLS (port 443). The instances run in public subnets. Which configurations below allow the application to function and minimize the exposure of the instances? Select 2 answers from the options given below","choices":{"A":"A network ACL with a rule that allows outgoing traffic on port 443.","B":"A network ACL with rules that allow outgoing traffic on port 443 and incoming traffic on ephemeral ports","C":"A network ACL with rules that allow outgoing traffic on port 443 and incoming traffic on port 443.","D":"A security group with a rule that allows outgoing traffic on port 443","E":"A security group with rules that allow outgoing traffic on port 443 and incoming traffic on ephemeral ports.","F":"A security group with rules that allow outgoing traffic on port 443 and incoming traffic on port 443."},"answer":"BD","explanation":"Since here the traffic needs to flow outbound from the Instance to a web service on Port 443, the outbound rules on both the Network and Security Groups need to allow outbound traffic. The Incoming traffic should be allowed on ephermal ports for the Operating System on the Instance to allow a connection to be established on any desired or available port. Option A is invalid because this rule alone is not enough. You also need to ensure incoming traffic on ephemeral ports Option C is invalid because need to ensure incoming traffic on ephemeral ports and not only port 443 Option E and F are invalid since here you are allowing additional ports on Security groups which are not required For more information on VPC Security Groups, please visit the below URL: https://docs.aws.amazon.com/AmazonVPC/latest/UserGuideA/PC_SecurityGroups.htmll The correct answers are: A network ACL with rules that allow outgoing traffic on port 443 and incoming traffic on ephemeral ports, A security group with a rule that allows outgoing traffic on port 443"},{"question":"A company is deploying a new web application on AWS. Based on their other web applications, they anticipate being the target of frequent DDoS attacks. Which steps can the company use to protect their application? Select 2 answers from the options given below.","choices":{"A":"Associate the EC2 instances with a security group that blocks traffic from blacklisted IP addresses.","B":"Use an ELB Application Load Balancer and Auto Scaling group to scale to absorb application layer traffic.","C":"Use Amazon Inspector on the EC2 instances to examine incoming traffic and discard malicious traffic.","D":"Use CloudFront and AWS WAF to prevent malicious traffic from reaching the application","E":"Enable GuardDuty to block malicious traffic from reaching the application"},"answer":"BD","explanation":"The below diagram from AWS shows the best case scenario for avoiding DDos attacks using services such as AWS Cloudfro WAF, ELB and Autoscaling Option A is invalid because by default security groups don't allow access Option C is invalid because AWS Inspector cannot be used to examine traffic Option E is invalid because this can be used for attacks on EC2 Instances but not against DDos attacks on the entire application For more information on DDos mitigation from AWS, please visit the below URL: https://aws.amazon.com/answers/networking/aws-ddos-attack-mitieationi The correct answers are: Use an ELB Application Load Balancer and Auto Scaling group to scale to absorb application layer traffic., Use CloudFront and AWS WAF to prevent malicious traffic from reaching the application"},{"question":"You are working in the media industry and you have created a web application where users will be able to upload photos they create to your website. This web application must be able to call the S3 API in order to be able to function. Where should you store your API credentials whilst maintaining the maximum level of security?","choices":{"A":"Save the API credentials to your PHP files.","B":"Don't save your API credentials, instead create a role in IAM and assign this role to an EC2 instance when you first create it.","C":"Save your API credentials in a public Github repository.","D":"Pass API credentials to the instance using instance userdat"},"answer":"B","explanation":"Applications must sign their API requests with AWS credentials. Therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances. For example, you can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests, whil protecting your credentials from other users. However, it's challenging to securely distribute credentials to each instance. especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS credentials. IAM roles are designed so that your applications can securely make API requests from your instances, without requiring yo manage the security credentials that the applications use. Option A.C and D are invalid because using AWS Credentials in an application in production is a direct no recommendation 1 secure access For more information on IAM Roles, please visit the below URL: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html The correct answer is: Don't save your API credentials. Instead create a role in IAM and assign this role to an EC2 instance when you first create it"},{"question":"You have a set of Keys defined using the AWS KMS service. You want to stop using a couple of keys , but are not sure of which services are currently using the keys. Which of the following would be a safe option to stop using the keys from further usage.","choices":{"A":"Delete the keys since anyway there is a 7 day waiting period before deletion","B":"Disable the keys","C":"Set an alias for the key","D":"Change the key material for the key"},"answer":"B","explanation":"Option A is invalid because once you schedule the deletion and waiting period ends, you cannot come back from the deletion process. Option C and D are invalid because these will not check to see if the keys are being used or not The AWS Documentation mentions the following Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. It deletes the key material and all metadata associated with the CMK, and is irreversible. After a CMK is deleted you can no longer decrypt the data that was encrypted under that CMK, which means that data becomes unrecoverable. You should delete a CMK only when you are sure that you don't need to use it anymore. If you are not sure, consider disabling the CMK instead of deleting it. You can re-enable a disabled CMK if you need to use it again later, but you cannot recover a deleted CMK. For more information on deleting keys from KMS, please visit the below URL: https://docs.aws.amazon.com/kms/latest/developereuide/deleting-keys.html The correct answer is: Disable the keys"},{"question":"A security engineer must ensure that all infrastructure launched in the company AWS account be monitored for deviation from compliance rules, specifically that all EC2 instances are launched from one of a specified list of AM Is and that all attached EBS volumes are encrypted. Infrastructure not in compliance should be terminated. What combination of steps should the Engineer implement? Select 2 answers from the options given below.","choices":{"A":"Set up a CloudWatch event based on Trusted Advisor metrics","B":"Trigger a Lambda function from a scheduled CloudWatch event that terminates non-compliant infrastructure.","C":"Set up a CloudWatch event based on Amazon inspector findings","D":"Monitor compliance with AWS Config Rules triggered by configuration changes","E":"Trigger a CLI command from a CloudWatch event that terminates the infrastructure"},"answer":"BD","explanation":"You can use AWS Config to monitor for such Event Option A is invalid because you cannot set Cloudwatch events based on Trusted Advisor checks. Option C is invalid Amazon inspector cannot be used to check whether instances are launched from a specific A Option E is invalid because triggering a CLI command is not the preferred option, instead you should use Lambda functions for all automation purposes. For more information on Config Rules please see the below Link: https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html These events can then trigger a lambda function to terminate instances For more information on Cloudwatch events please see the below Link: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatlsCloudWatchEvents. ( The correct answers are: Trigger a Lambda function from a scheduled Cloudwatch event that terminates non-compliant infrastructure., Monitor compliance with AWS Config Rules triggered by configuration changes"},{"question":"An application running on EC2 instances in a VPC must access sensitive data in the data center. The access must be encrypted in transit and have consistent low latency. Which hybrid architecture will meet these requirements?","choices":{"A":"Expose the data with a public HTTPS endpoint.","B":"A VPN between the VPC and the data center over a Direct Connect connection","C":"A VPN between the VPC and the data center.","D":"A Direct Connect connection between the VPC and data center"},"answer":"B","explanation":"Since this is required over a consistency low latency connection, you should use Direct Connect. For encryption, you can make use of a VPN Option A is invalid because exposing an HTTPS endpoint will not help all traffic to flow between a VPC and the data center. Option C is invalid because low latency is a key requirement Option D is invalid because only Direct Connect will not suffice For more information on the connection options please see the below Link: https://aws.amazon.com/answers/networking/aws-multiple-vpc-vpn-connection-sharint The correct answer is: A VPN between the VPC and the data center over a Direct Connect connection"},{"question":"A company hosts data in S3. There is a requirement to control access to the S3 buckets. Which are the 2 ways in which this can be achieved?","choices":{"A":"Use Bucket policies","B":"Use the Secure Token service","C":"Use IAM user policies","D":"Use AWS Access Keys"},"answer":"AC","explanation":"The AWS Documentation mentions the following Amazon S3 offers access policy options broadly categorized as resource-based policies and user policies. Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies. For example, bucket policies and access control lists (ACLs) are resourcebased policies. You can also attach access policies to users in your account. These are called user policies. You may choose to use resource-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources. Option B and D are invalid because these cannot be used to control access to S3 buckets For more information on S3 access control, please refer to the below Link: https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.htmll The correct answers are: Use Bucket policies. Use IAM user policies"},{"question":"You are responsible to deploying a critical application onto AWS. Part of the requirements for this application is to ensure that the controls set for this application met PCI compliance. Also there is a need to monitor web application logs to identify any malicious activity. Which of the following services can be used to fulfil this requirement. Choose 2 answers from the options given below","choices":{"A":"Amazon Cloudwatch Logs","B":"Amazon VPC Flow Logs","C":"Amazon AWS Config","D":"Amazon Cloudtrail"},"answer":"AD","explanation":"The AWS Documentation mentions the following about these services AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Option B is incorrect because VPC flow logs can only check for flow to instances in a VPC Option C is incorrect because this can check for configuration changes only For more information on Cloudtrail, please refer to below URL: https://aws.amazon.com/cloudtrail; You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, and other sources. You can then retrieve the associated log data from CloudWatch Logs. For more information on Cloudwatch logs, please refer to below URL: http://docs.aws.amazon.com/AmazonCloudWatch/latest/loes/WhatisCloudWatchLoES.htmll The correct answers are: Amazon Cloudwatch Logs, Amazon Cloudtrail"},{"question":"A company wishes to enable Single Sign On (SSO) so its employees can login to the management console using their corporate directory identity. Which steps below are required as part of the process? Select 2 answers from the options given below.","choices":{"A":"Create a Direct Connect connection between on-premise network and AW","B":"Use an AD connector for connecting AWS with on-premise active directory.","C":"Create IAM policies that can be mapped to group memberships in the corporate directory.","D":"Create a Lambda function to assign IAM roles to the temporary security tokens provided to the users.","E":"Create IAM users that can be mapped to the employees' corporate identities","F":"Create an IAM role that establishes a trust relationship between IAM and the corporate directory identity provider (IdP)"},"answer":"AE","explanation":"Create a Direct Connect connection so that corporate users can access the AWS account Option B is incorrect because IAM policies are not directly mapped to group memberships in the corporate directory. It is IAM roles which are mapped. Option C is incorrect because Lambda functions is an incorrect option to assign roles. Option D is incorrect because IAM users are not directly mapped to employees' corporate identities. For more information on Direct Connect, please refer to below URL: ' https://aws.amazon.com/directconnect/ From the AWS Documentation, for federated access, you also need to ensure the right policy permissions are in place Configure permissions in AWS for your federated users The next step is to create an IAM role that establishes a trust relationship between IAM and your organization's IdP that identifies your IdP as a principal (trusted entity) for purposes of federation. The role also defines what users authenticated your organization's IdP are allowed to do in AWS. You can use the IAM console to create this role. When you create the trust policy that indicates who can assume the role, you specify the SAML provider that you created earlier in IAM along with one or more SAML attributes that a user must match to be allowed to assume the role. For example, you can specify that only users whose SAML eduPersonOrgDN value is ExampleOrg are allowed to sign in. The role wizard automatically adds a condition to test the saml:aud attribute to make sure that the role is assumed only for sign-in to the AWS Management Console. The trust policy for the role might look like this: For more information on SAML federation, please refer to below URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enabli Note: What directories can I use with AWS SSO? You can connect AWS SSO to Microsoft Active Directory, running either on-premises or in the AWS Cloud. AWS SSO supports AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, and AD Connector. AWS SSO does not support Simple AD. See AWS Directory Service Getting Started to learn more. To connect to your on-premises directory with AD Connector, you need the following: VPC Set up a VPC with the following:  At least two subnets. Each of the subnets must be in a different Availability Zone.  The VPC must be connected to your on-premises network through a virtual private network (VPN) connection or AWS Direct Connect.  The VPC must have default hardware tenancy.  https://aws.amazon.com/single-sign-on/  https://aws.amazon.com/single-sign-on/faqs/  https://aws.amazon.com/bloj using-corporate-credentials/  https://docs.aws.amazon.com/directoryservice/latest/admin- The correct answers are: Create a Direct Connect connection between on-premise network and AWS. Use an AD connector connecting AWS with on-premise active directory.. Create an IAM role that establishes a trust relationship between IAM and corporate directory identity provider (IdP)"},{"question":"A company continually generates sensitive records that it stores in an S3 bucket. All objects in the bucket are encrypted using SSE-KMS using one of the company's CMKs. Company compliance policies require that no more than one month of data be encrypted using the same encryption key. What solution below will meet the company's requirements?","choices":{"A":"Trigger a Lambda function with a monthly CloudWatch event that creates a new CMK and updates the S3 bucket to use the new CMK.","B":"Configure the CMK to rotate the key material every month.","C":"Trigger a Lambda function with a monthly CloudWatch event that creates a new CMK, updates the S3 bucket to use thfl new CMK, and deletes the old CMK.","D":"Trigger a Lambda function with a monthly CloudWatch event that rotates the key material in the CMK."},"answer":"A","explanation":"You can use a Lambda function to create a new key and then update the S3 bucket to use the new key. Remember not to delete the old key, else you will not be able to decrypt the documents stored in the S3 bucket using the older key. Option B is incorrect because AWS KMS cannot rotate keys on a monthly basis Option C is incorrect because deleting the old key means that you cannot access the older objects Option D is incorrect because rotating key material is not possible. For more information on AWS KMS keys, please refer to below URL: https://docs.aws.amazon.com/kms/latest/developereuide/concepts.htmll The correct answer is: Trigger a Lambda function with a monthly CloudWatch event that creates a new CMK and updates the S3 bucket to use the new CMK."},{"question":"Company policy requires that all insecure server protocols, such as FTP, Telnet, HTTP, etc be disabled on all servers. The security team would like to regularly check all servers to ensure compliance with this requirement by using a scheduled CloudWatch event to trigger a review of the current infrastructure. What process will check compliance of the company's EC2 instances?","choices":{"A":"Trigger an AWS Config Rules evaluation of the restricted-common-ports rule against every EC2 instance.","B":"Query the Trusted Advisor API for all best practice security checks and check for \\"action recommened\\" status.","C":"Enable a GuardDuty threat detection analysis targeting the port configuration on every EC2 instance.","D":"Run an Amazon inspector assessment using the Runtime Behavior Analysis rules package against every EC2 instance."},"answer":"D","explanation":"Option B is incorrect because querying Trusted Advisor API's are not possible Option C is incorrect because GuardDuty should be used to detect threats and not check the compliance of security protocols. Option D states that Run Amazon Inspector using runtime behavior analysis rules which will analyze the behavior of your instances during an assessment run, and provide guidance about how to make your EC2 instances more secure. Insecure Server Protocols This rule helps determine whether your EC2 instances allow support for insecure and unencrypted ports/services such as FTP, Telnet HTTP, IMAP, POP version 3, SMTP, SNMP versions 1 and 2, rsh, and rlogin. For more information, please refer to below URL: https://docs.aws.amazon.eom/mspector/latest/userguide/inspector_runtime-behavioranalysis. html#insecure- protocols ( The correct answer is: Run an Amazon Inspector assessment using the Runtime Behavior Analysis rules package against every EC2 instance."},{"question":"A web application runs in a VPC on EC2 instances behind an ELB Application Load Balancer. The application stores data in an RDS MySQL DB instance. A Linux bastion host is used to apply schema updates to the database - administrators connect to the host via SSH from a corporate workstation. The following security groups are applied to the infrastructure- * sgLB - associated with the ELB * sgWeb - associated with the EC2 instances. * sgDB - associated with the database * sgBastion - associated with the bastion host Which security group configuration will allow the application to be secure and functional?","choices":{"A":"sgLB: allow port 80 and 443 traffic from 0.0.0.0/0\\nsgWeb: allow port 80 and 443 traffic from 0.0.0.0/0\\nsgDB: allow port 3306 traffic from sgWeb and sgBastion\\nsgBastion: allow port 22 traffic from the corporate IP address range","B":"sgLB: aIlow port 80 and 443 traffic from 0.0.0.0/0\\nsgWeb: allow port 80 and 443 traffic from sgLB\\nsgDB: allow port 3306 traffic from sgWeb and sgLB\\nsgBastion: allow port 22 traffic from the VPC IP address range","C":"sgLB: allow port 80 and 443 traffic from 0.0.0.0/0\\nsgWeb: allow port 80 and 443 traffic from sgLB\\nsgDB: allow port 3306 traffic from sgWeb and sgBastion\\nsgBastion: allow port 22 traffic from the VPC IP address range","D":"sgLB: allow port 80 and 443 traffic from 0.0.0.0/0\\nsgWeb: allow port 80 and 443 traffic from sgLB\\nsgDB: allow port 3306 traffic from sgWeb and sgBastion\\nsgBastion: allow port 22 traffic from the corporate IP address range"},"answer":"D","explanation":"The Load Balancer should accept traffic on ow port 80 and 443 traffic from 0.0.0.0/0 The backend EC2 Instances should accept traffic from the Load Balancer The database should allow traffic from the Web server And the Bastion host should only allow traffic from a specific corporate IP address range Option A is incorrect because the Web group should only allow traffic from the Load balancer For more information on AWS Security Groups, please refer to below URL: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usins- network-security.htmll The correct answer is: sgLB :allow port 80 and 443 traffic from 0.0.0.0/0 sgWeb :allow port 80 and 443 traffic from sgLB sgDB :allow port 3306 traffic from sgWeb and sgBastion sgBastion: allow port 22 traffic from the corporate IP address range"},{"question":"How can you ensure that instance in an VPC does not use AWS DNS for routing DNS requests. You want to use your own managed DNS instance. How can this be achieved?","choices":{"A":"Change the existing DHCP options set","B":"Create a new DHCP options set and replace the existing one.","C":"Change the route table for the VPC","D":"Change the subnet configuration to allow DNS requests from the new DNS Server"},"answer":"B","explanation":"In order to use your own DNS server, you need to ensure that you create a new custom DHCP options set with the IP of th custom DNS server. You cannot modify the existing set, so you need to create a new one. Option A is invalid because you cannot make changes to an existing DHCP options Set. Option C is invalid because this can only be used to work with Routes and not with a custom DNS solution. Option D is invalid because this needs to be done at the VPC level and not at the Subnet level For more information on DHCP options set, please visit the following url https://docs.aws.amazon.com/AmazonVPC/latest/UserGuideA/PC DHCP Options.html The correct answer is: Create a new DHCP options set and replace the existing one."},{"question":"A windows machine in one VPC needs to join the AD domain in another VPC. VPC Peering has been established. But the domain join is not working. What is the other step that needs to be followed to ensure that the AD domain join can work as intended","choices":{"A":"Change the VPC peering connection to a VPN connection","B":"Change the VPC peering connection to a Direct Connect connection","C":"Ensure the security groups for the AD hosted subnet has the right rule for relevant subnets","D":"Ensure that the AD is placed in a public subnet"},"answer":"C","explanation":"In addition to VPC peering and setting the right route tables, the security groups for the AD EC2 instance needs to ensure the right rules are put in place for allowing incoming traffic. Option A and B is invalid because changing the connection type will not help. This is a problem with the Security Groups. Option D is invalid since the AD should not be placed in a public subnet For more information on allowing ingress traffic for AD, please visit the following url |https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/ingress.html| The correct answer is: Ensure the security groups for the AD hosted subnet has the right rule for relevant subnets"},{"question":"You need to have a requirement to store objects in an S3 bucket with a key that is automatically managed and rotated. Which of the following can be used for this purpose?","choices":{"A":"AWS KMS","B":"AWS S3 Server side encryption","C":"AWS Customer Keys","D":"AWS Cloud HSM"},"answer":"B","explanation":"The AWS Documentation mentions the following Server-side encryption protects data at rest. Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) uses strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. All other options are invalid since here you need to ensure the keys are manually rotated since you manage the entire key set Using AWS S3 Server side encryption, AWS will manage the rotation of keys automatically. For more information on Server side encryption, please visit the following URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsineServerSideEncryption.htmll The correct answer is: AWS S3 Server side encryption"},{"question":"You are trying to use the AWS Systems Manager run command on a set of Instances. The run command on a set of Instances. What can you do to diagnose the issue? Choose 2 answers from the options given","choices":{"A":"Ensure that the SSM agent is running on the target machine","B":"Check the /var/log/amazon/ssm/errors.log file","C":"Ensure the right AMI is used for the Instance","D":"Ensure the security groups allow outbound communication for the instance"},"answer":"AB","explanation":"The AWS Documentation mentions the following If you experience problems executing commands using Run Command, there might be a problem with the SSM Agent. Use the following information to help you troubleshoot the agent View Agent Logs The SSM Agent logs information in the following files. The information in these files can help you troubleshoot problems. On Windows %PROGRAMDATA%\\\\Amazon\\\\SSM\\\\Logs\\\\amazon-ssm-agent.log %PROGRAMDATA%\\\\Amazon\\\\SSM\\\\Logs\\\\error.log The default filename of the seelog is seelog-xml.template. If you modify a seelog, you must rename the file to seelog.xml. On Linux /var/log/amazon/ssm/amazon-ssm-agentlog /var/log/amazon/ssm/errors.log Option C is invalid because the right AMI has nothing to do with the issues. The agent which is used to execute run commands can run on a variety of AMI'S Option D is invalid because security groups does not come into the picture with the communication between the agent and the SSM service For more information on troubleshooting AWS SSM, please visit the following URL: https://docs.aws.amazon.com/systems- manaeer/latest/userguide/troubleshootine-remotecommands. htmll The correct answers are: Ensure that the SSM agent is running on the target machine. Check the /var/log/amazon/ssm/errors.log file"},{"question":"You have an EBS volume attached to an EC2 Instance which uses KMS for Encryption. Someone has now gone ahead and deleted the Customer Key which was used for the EBS encryption. What should be done to ensure the data can be decrypted.","choices":{"A":"Create a new Customer Key using KMS and attach it to the existing volume","B":"You cannot decrypt the data that was encrypted under the CMK, and the data is not recoverable.","C":"Request AWS Support to recover the key","D":"Use AWS Config to recover the key"},"answer":"B","explanation":"Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. It deletes the key material and all metadata associated with the CMK, and is irreversible. After a CMK is deleted you can no longer decrypt the data that was encrypted under that CMK, which means that data becomes unrecoverable. You should delete a CMK only when you are sure that you don't need to use it anymore. If you are not sure, consider disabling the CMK instead of deleting it. You can re-enable a disabled CMK if you need to use it again later, but you cannot recover a deleted CMK. https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html A is incorrect because Creating a new CMK and attaching it to the exiting volume will not allow the data to be decrypted, you cannot attach customer master keys after the volume is encrypted Option C and D are invalid because once the key has been deleted, you cannot recover it For more information on EBS Encryption with KMS, please visit the following URL: https://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html The correct answer is: You cannot decrypt the data that was encrypted under the CMK, and the data is not recoverable."},{"question":"A company had developed an incident response plan 18 months ago. Regular implementations of the response plan are carried out. No changes have been made to the response plan have been made since its creation. Which of the following is a right statement with regards to the plan?","choices":{"A":"It places too much emphasis on already implemented security controls.","B":"The response plan is not implemented on a regular basis","C":"The response plan does not cater to new services","D":"The response plan is complete in its entirety"},"answer":"C","explanation":"So definitely the case here is that the incident response plan is not catering to newly created services. AWS keeps on changing and adding new services and hence the response plan must cater to these new services. Option A and B are invalid because we don't know this for a fact. Option D is invalid because we know that the response plan is not complete, because it does not cater to new features of AWS For more information on incident response plan please visit the following URL: https://aws.amazon.com/blogs/publicsector/buildins-a-cloud-specific-incident- response-plan; The correct answer is: The response plan does not cater to new services"},{"question":"Your application currently uses customer keys which are generated via AWS KMS in the US east region. You now want to use the same set of keys from the EU- Central region. How can this be accomplished?","choices":{"A":"Export the key from the US east region and import them into the EU-Central region","B":"Use key rotation and rotate the existing keys to the EU-Central region","C":"Use the backing key from the US east region and use it in the EU-Central region","D":"This is not possible since keys from KMS are region specific"},"answer":"D","explanation":"Option A is invalid because keys cannot be exported and imported across regions. Option B is invalid because key rotation cannot be used to export keys Option C is invalid because the backing key cannot be used to export keys This is mentioned in the AWS documentation What geographic region are my keys stored in? Keys are only stored and used in the region in which they are created. They cannot be transferred to another region. For example; keys created in the EU-Central (Frankfurt) region are only stored and used within the EU-Central (Frankfurt) region For more information on KMS please visit the following URL: https://aws.amazon.com/kms/faqs/ The correct answer is: This is not possible since keys from KMS are region specific"},{"question":"Your company has created a set of keys using the AWS KMS service. They need to ensure that each key is only used for certain services. For example , they want one key to be used only for the S3 service. How can this be achieved?","choices":{"A":"Create an IAM policy that allows the key to be accessed by only the S3 service.","B":"Create a bucket policy that allows the key to be accessed by only the S3 service.","C":"Use the kms:ViaService condition in the Key policy","D":"Define an IAM user, allocate the key and then assign the permissions to the required service"},"answer":"C","explanation":"Option A and B are invalid because mapping keys to services cannot be done via either the IAM or bucket policy Option D is invalid because keys for IAM users cannot be assigned to services This is mentioned in the AWS Documentation The kms:ViaService condition key limits use of a customer-managed CMK to requests from particular AWS services. (AWS managed CMKs in your account, such as aws/s3, are always restricted to the AWS service that created them.) For example, you can use kms:V1aService to allow a user to use a customer managed CMK only for requests that Amazon S3 makes on their behalf. Or you can use it to deny the user permission to a CMK when a request on their behalf comes from AWS Lambda. For more information on key policy's for KMS please visit the following URL: https://docs.aws.amazon.com/kms/latest/developereuide/policy-conditions.html The correct answer is: Use the kms:ViaServtce condition in the Key policy"},{"question":"An EC2 Instance hosts a Java based application that access a DynamoDB table. This EC2 Instance is currently serving production based users. Which of the following is a secure way of ensuring that the EC2 Instance access the Dynamo table","choices":{"A":"Use IAM Roles with permissions to interact with DynamoDB and assign it to the EC2 Instance","B":"Use KMS keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance","C":"Use IAM Access Keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance","D":"Use IAM Access Groups with the right permissions to interact with DynamoDB and assign it to the EC2 Instance"},"answer":"A","explanation":"To always ensure secure access to AWS resources from EC2 Instances, always ensure to assign a Role to the EC2 Instance Option B is invalid because KMS keys are not used as a mechanism for providing EC2 Instances access to AWS services. Option C is invalid Access keys is not a safe mechanism for providing EC2 Instances access to AWS services. Option D is invalid because there is no way access groups can be assigned to EC2 Instances. For more information on IAM Roles, please refer to the below URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/id roles.html The correct answer is: Use IAM Roles with permissions to interact with DynamoDB and assign it to the EC2 Instance"},{"question":"An application running on EC2 instances processes sensitive information stored on Amazon S3. The information is accessed over the Internet. The security team is concerned that the Internet connectivity to Amazon S3 is a security risk. Which solution will resolve the security concern?","choices":{"A":"Access the data through an Internet Gateway.","B":"Access the data through a VPN connection.","C":"Access the data through a NAT Gateway.","D":"Access the data through a VPC endpoint for Amazon S3"},"answer":"D","explanation":"The AWS Documentation mentions the followii A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. Option A.B and C are all invalid because the question specifically mentions that access should not be provided via the Internet For more information on VPC endpoints, please refer to the below URL: The correct answer is: Access the data through a VPC endpoint for Amazon S3"},{"question":"Development teams in your organization use S3 buckets to store the log files for various applications hosted ir development environments in AWS. The developers want to keep the logs for one month for troubleshooting purposes, and then purge the logs. What feature will enable this requirement?","choices":{"A":"Adding a bucket policy on the S3 bucket.","B":"Configuring lifecycle configuration rules on the S3 bucket.","C":"Creating an IAM policy for the S3 bucket.","D":"Enabling CORS on the S3 bucke"},"answer":"B","explanation":"The AWS Documentation mentions the following on lifecycle policies Lifecycle configuration enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. These actions can be classified a follows: Transition actions - In which you define when objects transition to another . For example, you may choose to transition objects to the STANDARDJA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation. Expiration actions - In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf. Option A and C are invalid because neither bucket policies neither IAM policy's can control the purging of logs Option D is invalid CORS is used for accessing objects across domains and not for purging of logs For more information on AWS S3 Lifecycle policies, please visit the following URL: .com/AmazonS3/latest/d< The correct answer is: Configuring lifecycle configuration rules on the S3 bucket."},{"question":"A company has resources hosted in their AWS Account. There is a requirement to monitor all API activity for all regions. The audit needs to be applied for future regions as well. Which of the following can be used to fulfil this requirement.","choices":{"A":"Ensure Cloudtrail for each regio","B":"Then enable for each future region.","C":"Ensure one Cloudtrail trail is enabled for all regions.","D":"Create a Cloudtrail for each regio","E":"Use Cloudformation to enable the trail for all future regions.","F":"Create a Cloudtrail for each regio","G":"Use AWS Config to enable the trail for all future region"},"answer":"B","explanation":"The AWS Documentation mentions the following You can now turn on a trail across all regions for your AWS account. CloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified. Additionally, when AWS launches a new region, CloudTrail will create the same trail in the new region. As a result you will receive log files containing API activity for the new region without taking any action. Option A and C is invalid because this would be a maintenance overhead to enable cloudtrail for every region Option D is invalid because this AWS Config cannot be used to enable trails For more information on this feature, please visit the following URL: https://aws.ama2on.com/about-aws/whats-new/20l5/l2/turn-on-cloudtrail-across-all-reeions-andsupport- for-multiple-trails The correct answer is: Ensure one Cloudtrail trail is enabled for all regions."},{"question":"Your company has a requirement to work with a DynamoDB table. There is a security mandate that all data should be encrypted at rest. What is the easiest way to accomplish this for DynamoDB.","choices":{"A":"Use the AWS SDK to encrypt the data before sending it to the DynamoDB table","B":"Encrypt the DynamoDB table using KMS during its creation","C":"Encrypt the table using AWS KMS after it is created","D":"Use S3 buckets to encrypt the data before sending it to DynamoDB"},"answer":"B","explanation":"The most easiest option is to enable encryption when the DynamoDB table is created. The AWS Documentation mentions the following Amazon DynamoDB offers fully managed encryption at rest. DynamoDB encryption at rest provides enhanced security by encrypting your data at rest using an AWS Key Management Service (AWS KMS) managed encryption key for DynamoDB. This functionality eliminates the operational burden and complexity involved in protecting sensitive data. Option A is partially correct, you can use the AWS SDK to encrypt the data, but the easier option would be to encrypt the table before hand. Option C is invalid because you cannot encrypt the table after it is created Option D is invalid because encryption for S3 buckets is for the objects in S3 only. For more information on securing data at rest for DynamoDB please refer to below URL: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.htmll The correct answer is: Encrypt the DynamoDB table using KMS during its creation"},{"question":"Your company hosts critical data in an S3 bucket. There is a requirement to ensure that all data is encrypted. There is also metadata about the information stored in the bucket that needs to be encrypted as well. Which of the below measures would you take to ensure that the metadata is encrypted?","choices":{"A":"Put the metadata as metadata for each object in the S3 bucket and then enable S3 Server side encryption.","B":"Put the metadata as metadata for each object in the S3 bucket and then enable S3 Server KMS encryption.","C":"Put the metadata in a DynamoDB table and ensure the table is encrypted during creation time.","D":"Put thp metadata in thp S3 hurkpf itsel"},"answer":"C","explanation":"Option A ,B and D are all invalid because the metadata will not be encrypted in any case and this is a key requirement from the question. One key thing to note is that when the S3 bucket objects are encrypted, the meta data is not encrypted. So the best option is to use an encrypted DynamoDB table Important All GET and PUT requests for an object protected by AWS KMS will fail if they are not made via SSL or by using SigV4. SSE-KMS encrypts only the object dat A. Any object metadata is not encrypted. For more information on using KMS encryption for S3, please refer to below URL: 1 https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html The correct answer is: Put the metadata in a DynamoDB table and ensure the table is encrypted during creation time."},{"question":"Your company has a set of EC2 Instances that are placed behind an ELB. Some of the applications hosted on these instances communicate via a legacy protocol. There is a security mandate that all traffic between the client and the EC2 Instances need to be secure. How would you accomplish this?","choices":{"A":"Use an Application Load balancer and terminate the SSL connection at the ELB","B":"Use a Classic Load balancer and terminate the SSL connection at the ELB","C":"Use an Application Load balancer and terminate the SSL connection at the EC2 Instances","D":"Use a Classic Load balancer and terminate the SSL connection at the EC2 Instances"},"answer":"D","explanation":"Since there are applications which work on legacy protocols, you need to ensure that the ELB can be used at the network layer as well and hence you should choose the Classic ELB. Since the traffic needs to be secure till the EC2 Instances, the SSL termination should occur on the Ec2 Instances. Option A and C are invalid because you need to use a Classic Load balancer since this is a legacy application. Option B is incorrect since encryption is required until the EC2 Instance For more information on HTTPS listeners for classic load balancers, please refer to below URL https://docs.aws.ama20n.com/elasticloadbalancing/latest/classic/elb-https-load-balancers.htmll The correct answer is: Use a Classic Load balancer and terminate the SSL connection at the EC2 Instances"},{"question":"Your company has a set of EC2 Instances defined in AWS. They need to ensure that all traffic packets are monitored and inspected for any security threats. How can this be achieved? Choose 2 answers from the options given below","choices":{"A":"Use a host based intrusion detection system","B":"Use a third party firewall installed on a central EC2 instance","C":"Use VPC Flow logs","D":"Use Network Access control lists logging"},"answer":"AB","explanation":"If you want to inspect the packets themselves, then you need to use custom based software A diagram representation of this is given in the AWS Security best practices Option C is invalid because VPC Flow logs cannot conduct packet inspection. For more information on AWS Security best practices, please refer to below URL: The correct answers are: Use a host based intrusion detection system. Use a third party firewall installed on a central EC2"},{"question":"Your company hosts a large section of EC2 instances in AWS. There are strict security rules governing the EC2 Instances. During a potential security breach , you need to ensure quick investigation of the underlying EC2 Instance. Which of the following service can help you quickly provision a test environment to look into the breached instance.","choices":{"A":"AWS Cloudwatch","B":"AWS Cloudformation","C":"AWS Cloudtrail","D":"AWS Config"},"answer":"B","explanation":"The AWS Security best practises mentions the following Unique to AWS, security practitioners can use CloudFormation to quickly create a new, trusted environment in which to conduct deeper investigation. The CloudFormation template can preconfigure instances in an isolated environment that contains all the necessary tools forensic teams need to determine the cause of the incident This cuts down on the time it takes to gather necessary tools, isolates systems under examination, and ensures that the team is operating in a clean room. Option A is incorrect since this is a logging service and cannot be used to provision a test environment Option C is incorrect since this is an API logging service and cannot be used to provision a test environment Option D is incorrect since this is a configuration service and cannot be used to provision a test environment For more information on AWS Security best practises, please refer to below URL: https://d1.awsstatic.com/whitepapers/architecture/AWS-Security-Pillar.pd1 The correct answer is: AWS Cloudformation"},{"question":"Your company use AWS KMS for management of its customer keys. From time to time, there is a requirement to delete existing keys as part of housekeeping activities. What can be done during the deletion process to verify that the key is no longer being used.","choices":{"A":"Use CloudTrail to see if any KMS API request has been issued against existing keys","B":"Use Key policies to see the access level for the keys","C":"Rotate the keys once before deletion to see if other services are using the keys","D":"Change the IAM policy for the keys to see if other services are using the keys"},"answer":"A","explanation":"The AWS lentation mentions the following You can use a combination of AWS CloudTrail, Amazon CloudWatch Logs, and Amazon Simple Notification Service (Amazon SNS) to create an alarm that notifies you of AWS KMS API requests that attempt to use a customer master key (CMK) that is pending deletion. If you receive a notification from such an alarm, you might want to cancel deletion of the CMK to give yourself more time to determine whether you want to delete it Options B and D are incorrect because Key policies nor IAM policies can be used to check if the keys are being used. Option C is incorrect since rotation will not help you check if the keys are being used. For more information on deleting keys, please refer to below URL: https://docs.aws.amazon.com/kms/latest/developereuide/deletine-keys-creatine-cloudwatchalarm. html The correct answer is: Use CloudTrail to see if any KMS API request has been issued against existing keys"},{"question":"You have a bucket and a VPC defined in AWS. You need to ensure that the bucket can only be accessed by the VPC endpoint. How can you accomplish this?","choices":{"A":"Modify the security groups for the VPC to allow access to the 53 bucket","B":"Modify the route tables to allow access for the VPC endpoint","C":"Modify the IAM Policy for the bucket to allow access for the VPC endpoint","D":"Modify the bucket Policy for the bucket to allow access for the VPC endpoint"},"answer":"D","explanation":"This is mentioned in the AWS Documentation Restricting Access to a Specific VPC Endpoint The following is an example of an S3 bucket policy that restricts access to a specific bucket, examplebucket only from the VPC endpoint with the ID vpce-la2b3c4d. The policy denies all access to the bucket if the specified endpoint is not being used. The aws:sourceVpce condition is used to the specify the endpoint. The aws:sourceVpce condition does not require an ARN for the VPC endpoint resource, only the VPC endpoint ID. For more information about using conditions in a policy, see Specifying Conditions in a Policy. Options A and B are incorrect because using Security Groups nor route tables will help to allow access specifically for that bucke via the VPC endpoint Here you specifically need to ensure the bucket policy is changed. Option C is incorrect because it is the bucket policy that needs to be changed and not the IAM policy. For more information on example bucket policies for VPC endpoints, please refer to below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html The correct answer is: Modify the bucket Policy for the bucket to allow access for the VPC endpoint"},{"question":"A Devops team is currently looking at the security aspect of their CI/CD pipeline. They are making use of AWS resource? for their infrastructure. They want to ensure that the EC2 Instances don't have any high security vulnerabilities. They want to ensure a complete DevSecOps process. How can this be achieved?","choices":{"A":"Use AWS Config to check the state of the EC2 instance for any sort of security issues.","B":"Use AWS Inspector API's in the pipeline for the EC2 Instances","C":"Use AWS Trusted Advisor API's in the pipeline for the EC2 Instances","D":"Use AWS Security Groups to ensure no vulnerabilities are present"},"answer":"B","explanation":"Amazon Inspector offers a programmatic way to find security defects or misconfigurations in your operating systems and applications. Because you can use API calls to access both the processing of assessments and the results of your assessments, integration of the findings into workflow and notification systems is simple. DevOps teams can integrate Amazon Inspector into their CI/CD pipelines and use it to identify any pre-existing issues or when new issues are introduced. Option A.C and D are all incorrect since these services cannot check for Security Vulnerabilities. These can only be checked by the AWS Inspector service. For more information on AWS Security best practices, please refer to below URL: https://d1.awsstatic.com/whitepapers/Security/AWS Security Best Practices.pdl The correct answer is: Use AWS Inspector API's in the pipeline for the EC2 Instances"},{"question":"You want to track access requests for a particular S3 bucket. How can you achieve this in the easiest possible way?","choices":{"A":"Enable server access logging for the bucket","B":"Enable Cloudwatch metrics for the bucket","C":"Enable Cloudwatch logs for the bucket","D":"Enable AWS Config for the S3 bucket"},"answer":"A","explanation":"The AWS Documentation mentions the foil To track requests for access to your bucket you can enable access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any. Options B and C are incorrect Cloudwatch is used for metrics and logging and cannot be used to track access requests. Option D is incorrect since this can be used for Configuration management but for not for tracking S3 bucket requests. For more information on S3 server logs, please refer to below UF https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLoes.html The correct answer is: Enable server access logging for the bucket"},{"question":"Your application currently use AWS Cognito for authenticating users. Your application consists of different types of users. Some users are only allowed read access to the application and others are given contributor access. How wou you manage the access effectively?","choices":{"A":"Create different cognito endpoints, one for the readers and the other for the contributors.","B":"Create different cognito groups, one for the readers and the other for the contributors.","C":"You need to manage this within the application itself","D":"This needs to be managed via Web security tokens"},"answer":"B","explanation":"The AWS Documentation mentions the following You can use groups to create a collection of users in a user pool, which is often done to set the permissions for those users. For example, you can create separate groups for users who are readers, contributors, and editors of your website and app. Option A is incorrect since you need to create cognito groups and not endpoints Options C and D are incorrect since these would be overheads when you can use AWS Cognito For more information on AWS Cognito user groups please refer to the below Link: https://docs.aws.amazon.com/coenito/latest/developersuide/cognito-user-pools-user-groups.htmll The correct answer is: Create different cognito groups, one for the readers and the other for the contributors."},{"question":"DDoS attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these types of attacks, you should probably want to include a WAF (Web Application Firewall) as part of your infrastructure. To inspect all HTTP requests, WAFs sit in-line with your application traffic. Unfortunately, this creates a scenario where WAFs can become a point of failure or bottleneck. To mitigate this problem, you need the ability to run multiple WAFs on demand during traffic spikes. This type of scaling for WAF is done via a \\"WAF sandwich.\\" Which of the following statements best describes what a \\"WAF sandwich\\" is? Choose the correct answer from the options below","choices":{"A":"The EC2 instance running your WAF software is placed between your private subnets and any NATed connections to the internet.","B":"The EC2 instance running your WAF software is placed between your public subnets and your Internet Gateway.","C":"The EC2 instance running your WAF software is placed between your public subnets and your private subnets.","D":"he EC2 instance running your WAF software is included in an Auto Scaling group and placed in between two Elastic load balancers."},"answer":"D","explanation":"The below diagram shows how a WAF sandwich is created. Its the concept of placing the Ec2 instance which hosts the WAF software in between 2 elastic load balancers. Option A.B and C are incorrect since the EC2 Instance with the WAF software needs to be placed in an Autoscaling Group For more information on a WAF sandwich please refer to the below Link: https://www.cloudaxis.eom/2016/11/2l/waf-sandwich/l The correct answer is: The EC2 instance running your WAF software is included in an Auto Scaling group and placed in between two Elastic load balancers."},{"question":"A company has hired a third-party security auditor, and the auditor needs read-only access to all AWS resources and logs of all VPC records and events that have occurred on AWS. How can the company meet the auditor's requirements without comprising security in the AWS environment? Choose the correct answer from the options below","choices":{"A":"Create a role that has the required permissions for the auditor.","B":"Create an SNS notification that sends the CloudTrail log files to the auditor's email when CIoudTrail delivers the logs to S3, but do not allow the auditor access to the AWS environment","C":"The company should contact AWS as part of the shared responsibility model, and AWS will grant required access to th^ third-party auditor.","D":"Enable CloudTrail logging and create an IAM user who has read-only permissions to the required AWS resources, including the bucket containing the CloudTrail logs"},"answer":"D","explanation":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting. Option A and C are incorrect since Cloudtrail needs to be used as part of the solution Option B is incorrect since the auditor needs to have access to Cloudtrail For more information on cloudtrail, please visit the below URL: https://aws.amazon.com/cloudtraiL The correct answer is: Enable CloudTrail logging and create an IAM user who has read-only permissions to the required AWS resources, including the bucket containing the CloudTrail logs."},{"question":"You have several S3 buckets defined in your AWS account. You need to give access to external AWS accounts to these S3 buckets. Which of the following can allow you to define the permissions for the external accounts? Choose 2 answers from the options given below","choices":{"A":"IAM policies","B":"Buckets ACL's","C":"IAM users","D":"Bucket policies"},"answer":"BD","explanation":"The AWS Security whitepaper gives the type of access control and to what level the control can be given Options A and C are incorrect since for external access to buckets, you need to use either Bucket policies or Bucket ACL's or more information on Security for storage services role please refer to the below URL: https://d1.awsstatic.com/whitepapers/Security/Security Storage Services Whitepaper.pdf The correct answers are: Buckets ACL's, Bucket policies"},{"question":"A large organization is planning on AWS to host their resources. They have a number of autonomous departments that wish to use AWS. What could be the strategy to adopt for managing the accounts.","choices":{"A":"Use multiple VPCs in the account each VPC for each department","B":"Use multiple IAM groups, each group for each department","C":"Use multiple IAM roles, each group for each department","D":"Use multiple AWS accounts, each account for each department"},"answer":"D","explanation":"A recommendation for this is given in the AWS Security best practices Option A is incorrect since this would be applicable for resources in a VPC Options B and C are incorrect since operationally it would be difficult to manage For more information on AWS Security best practices please refer to the below URL https://d1.awsstatic.com/whitepapers/Security/AWS Security Best Practices.pdl The correct answer is: Use multiple AWS accounts, each account for each department"},{"question":"You are planning on using the AWS KMS service for managing keys for your application. For which of the following can the KMS CMK keys be used for encrypting? Choose 2 answers from the options given below","choices":{"A":"Image Objects","B":"Large files","C":"Password","D":"RSA Keys"},"answer":"CD","explanation":"The CMK keys themselves can only be used for encrypting data that is maximum 4KB in size. Hence it can be used for encryptii information such as passwords and RSA keys. Option A and B are invalid because the actual CMK key can only be used to encrypt small amounts of data and not large amoui of dat A\\\\\\\\ You have to generate the data key from the CMK key in order to encrypt high amounts of data For more information on the concepts for KMS, please visit the following URL: https://docs.aws.amazon.com/kms/latest/developereuide/concepts.htmll The correct answers are: Password, RSA Keys"},{"question":"Your company has been using AWS for the past 2 years. They have separate S3 buckets for logging the various AWS services that have been used. They have hired an external vendor for analyzing their log files. They have their own AWS account. What is the best way to ensure that the partner account can access the log files in the company account for analysis. Choose 2 answers from the options given below","choices":{"A":"Create an IAM user in the company account","B":"Create an IAM Role in the company account","C":"Ensure the IAM user has access for read-only to the S3 buckets","D":"Ensure the IAM Role has access for read-only to the S3 buckets"},"answer":"BD","explanation":"The AWS Documentation mentions the following To share log files between multiple AWS accounts, you must perform the following general steps. These steps are explained in detail later in this section. Create an IAM role for each account that you want to share log files with. For each of these IAM roles, create an access policy that grants read-only access to the account you want to share the log files with. Have an IAM user in each account programmatically assume the appropriate role and retrieve the log files. Options A and C are invalid because creating an IAM user and then sharing the IAM user credentials with the vendor is a direct 'NO' practise from a security perspective. For more information on sharing cloudtrail logs files, please visit the following URL https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-sharine- loes.htmll The correct answers are: Create an IAM Role in the company account Ensure the IAM Role has access for read-only to the S3 buckets"},{"question":"Your company has been using AWS for hosting EC2 Instances for their web and database applications. They want to have a compliance check to see the following Whether any ports are left open other than admin ones like SSH and RDP Whether any ports to the database server other than ones from the web server security group are open Which of the following can help achieve this in the easiest way possible. You don't want to carry out an extra configuration changes?","choices":{"A":"AWS Config","B":"AWS Trusted Advisor","C":"AWS Inspector D.AWSGuardDuty"},"answer":"B","explanation":"Trusted Advisor checks for compliance with the following security recommendations: Limited access to common administrative ports to only a small subset of addresses. This includes ports 22 (SSH), 23 (Telnet) 3389 (RDP), and 5500 (VNQ. Limited access to common database ports. This includes ports 1433 (MSSQL Server), 1434 (MSSQL Monitor), 3306 (MySQL), Oracle (1521) and 5432 (PostgreSQL). Option A is partially correct but then you would need to write custom rules for this. The AWS trusted advisor can give you all o these checks on its dashboard Option C is incorrect. Amazon Inspector needs a software agent to be installed on all EC2 instances that are included in th. assessment target, the security of which you want to evaluate with Amazon Inspector. It monitors the behavior of the EC2 instance on which it is installed, including network, file system, and process activity, and collects a wide set of behavior and configuration data (telemetry), which it then passes to the Amazon Inspector service. Our question's requirement is to choose a choice that is easy to implement. Hence Trusted Advisor is more appropriate for this ) question. Options D is invalid because this service dont provide these details. For more information on the Trusted Advisor, please visit the following URL https://aws.amazon.com/premiumsupport/trustedadvisor> The correct answer is: AWS Trusted Advisor"},{"question":"A company is planning on using AWS for hosting their applications. They want complete separation and isolation of their production , testing and development environments. Which of the following is an ideal way to design such a setup?","choices":{"A":"Use separate VPCs for each of the environments","B":"Use separate IAM Roles for each of the environments","C":"Use separate IAM Policies for each of the environments","D":"Use separate AWS accounts for each of the environments"},"answer":"D","explanation":"A recommendation from the AWS Security Best practices highlights this as well option A is partially valid, you can segregate resources, but a best practise is to have multiple accounts for this setup. Options B and C are invalid because from a maintenance perspective this could become very difficult For more information on the Security Best practices, please visit the following URL: https://dl.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf The correct answer is: Use separate AWS accounts for each of the environments"},{"question":"An application is designed to run on an EC2 Instance. The applications needs to work with an S3 bucket. From a security perspective , what is the ideal way for the EC2 instance/ application to be configured?","choices":{"A":"Use the AWS access keys ensuring that they are frequently rotated.","B":"Assign an IAM user to the application that has specific access to only that S3 bucket","C":"Assign an IAM Role and assign it to the EC2 Instance","D":"Assign an IAM group and assign it to the EC2 Instance"},"answer":"C","explanation":"The below diagram from the AWS whitepaper shows the best security practicse of allocating a role that has access to the S3 bucket Options A,B and D are invalid because using users, groups or access keys is an invalid security practise when giving access to resources from other AWS resources. For more information on the Security Best practices, please visit the following URL: https://d1.awsstatic.com/whitepapers/Security/AWS Security Best Practices.pdl The correct answer is: Assign an IAM Role and assign it to the EC2 Instance"},{"question":"Your company has an EC2 Instance hosted in AWS. This EC2 Instance hosts an application. Currently this application is experiencing a number of issues. You need to inspect the network packets to see what the type of error that is occurring? Which one of the below steps can help address this issue?","choices":{"A":"Use the VPC Flow Logs.","B":"Use a network monitoring tool provided by an AWS partner.","C":"Use another instanc","D":"Setup a port to \\"promiscuous mode\\" and sniff the traffic to analyze the packet","E":"-","F":"Use Cloudwatch metric"},"answer":"B","explanation":"A. Use the VPC Flow Logs. B. Use a network monitoring tool provided by an AWS partner. C. Use another instanc D. Setup a port to \\"promiscuous mode\\" and sniff the traffic to analyze the packet E. - F. Use Cloudwatch metric Answer: B"},{"question":"Which of the below services can be integrated with the AWS Web application firewall service. Choose 2 answers from the options given below","choices":{"A":"AWS Cloudfront","B":"AWS Lambda","C":"AWS Application Load Balancer","D":"AWS Classic Load Balancer"},"answer":"AC","explanation":"The AWS documentation mentions the following on the Application Load Balancer AWS WAF can be deployed on Amazon CloudFront and the Application Load Balancer (ALB). As part of Amazon CloudFront it car be part of your Content Distribution Network (CDN) protecting your resources and content at the Edge locations and as part of the Application Load Balancer it can protect your origin web servers running behind the ALBs. Options B and D are invalid because only Cloudfront and the Application Load Balancer services are supported by AWS WAF. For more information on the web application firewall please refer to the below URL: https://aws.amazon.com/waf/faq; The correct answers are: AWS Cloudfront AWS Application Load Balancer"},{"question":"A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at Rest. If the user is supplying his own keys for encryption SSE-C, which of the below mentioned statements is true?","choices":{"A":"The user should use the same encryption key for all versions of the same object","B":"It is possible to have different encryption keys for different versions of the same object","C":"AWS S3 does not allow the user to upload his own keys for server side encryption","D":"The SSE-C does not work when versioning is enabled"},"answer":"B","explanation":"Managing your own encryption keys, y You can encrypt the object and send it across to S3 Option A is invalid because ideally you should use different encryption keys Option C is invalid because you can use you own encryption keys Option D is invalid because encryption works even if versioning is enabled For more information on client side encryption please visit the below Link: \\"\\"Keys.html https://docs.aws.ama2on.com/AmazonS3/latest/dev/UsingClientSideEncryption.html The correct answer is: It is possible to have different encryption keys for different versions of the same object"},{"question":"You are planning to use AWS Configto check the configuration of the resources in your AWS account. You are planning on using an existing IAM role and using it for the AWS Config resource. Which of the following is required to ensure the AWS config service can work as required?","choices":{"A":"Ensure that there is a trust policy in place for the AWS Config service within the role","B":"Ensure that there is a grant policy in place for the AWS Config service within the role","C":"Ensure that there is a user policy in place for the AWS Config service within the role","D":"Ensure that there is a group policy in place for the AWS Config service within the role"},"answer":"A","explanation":"Options B,C and D are invalid because you need to ensure a trust policy is in place and not a grant, user or group policy or more information on the IAM role permissions please visit the below Link: https://docs.aws.amazon.com/config/latest/developerguide/iamrole-permissions.htmll The correct answer is: Ensure that there is a trust policy in place for the AWS Config service within the role"},{"question":"A company has a requirement to create a DynamoDB table. The company's software architect has provided the following CLI command for the DynamoDB table Which of the following has been taken of from a security perspective from the above command?","choices":{"A":"Since the ID is hashed, it ensures security of the underlying table.","B":"The above command ensures data encryption at rest for the Customer table","C":"The above command ensures data encryption in transit for the Customer table","D":"The right throughput has been specified from a security perspective"},"answer":"B","explanation":"The above command with the \\"-sse-specification Enabled=true\\" parameter ensures that the data for the DynamoDB table is encrypted at rest. Options A,C and D are all invalid because this command is specifically used to ensure data encryption at rest For more information on DynamoDB encryption, please visit the URL: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/encryption.tutorial.html The correct answer is: The above command ensures data encryption at rest for the Customer table"},{"question":"Your company is planning on using AWS EC2 and ELB for deployment for their web applications. The security policy mandates that all traffic should be encrypted. Which of the following options will ensure that this requirement is met. Choose 2 answers from the options below.","choices":{"A":"Ensure the load balancer listens on port 80","B":"Ensure the load balancer listens on port 443","C":"Ensure the HTTPS listener sends requests to the instances on port 443","D":"Ensure the HTTPS listener sends requests to the instances on port 80"},"answer":"BC","explanation":"The AWS Documentation mentions the following You can create a load balancer that listens on both the HTTP (80) and HTTPS (443) ports. If you specify that the HTTPS listener sends requests to the instances on port 80, the load balancer terminates the requests and communication from the load balancer to the instances is not encrypted, if the HTTPS listener sends requests to the instances on port 443, communication from the load balancer to the instances is encrypted. Option A is invalid because there is a need for secure traffic, so port 80 should not be used Option D is invalid because for the HTTPS listener you need to use port 443 For more information on HTTPS with ELB, please refer to the below Link: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-create-https-ssl- loadbalancer. htmll The correct answers are: Ensure the load balancer listens on port 443, Ensure the HTTPS listener sends requests to the instances on port 443"},{"question":"Your organization is preparing for a security assessment of your use of AWS. In preparation for this assessment, which three IAM best practices should you consider implementing?","choices":{"A":"Create individual IAM users","B":"Configure MFA on the root account and for privileged IAM users","C":"Assign IAM users and groups configured with policies granting least privilege access","D":"Ensure all users have been assigned and dre frequently rotating a password, access ID/secret key, and X.509 certificate"},"answer":"ABC","explanation":"When you go to the security dashboard, the security status will show the best practices for initiating the first level of security. Option D is invalid because as per the dashboard, this is not part of the security recommendation For more information on best security practices please visit the URL: https://aws.amazon.com/whitepapers/aws-security-best-practices; The correct answers are: Create individual IAM users, Configure MFA on the root account and for privileged IAM users. Assign IAM users and groups configured with policies granting least privilege access"},{"question":"You have private video content in S3 that you want to serve to subscribed users on the Internet. User IDs, credentials, and subscriptions are stored in an Amazon RDS database. Which configuration will allow you to securely serve private content to your users?","choices":{"A":"Generate pre-signed URLs for each user as they request access to protected S3 content","B":"Create an IAM user for each subscribed user and assign the GetObject permission to each IAM user","C":"Create an S3 bucket policy that limits access to your private content to only your subscribed users'credentials","D":"Create a Cloud Front Client Identity user for your subscribed users and assign the GetObject permission to this user"},"answer":"A","explanation":"All objects and buckets by default are private. The pre-signed URLs are useful if you want your user/customer to be able upload a specific object to your bucket but you don't require them to have AWS security credentials or permissions. When you create a pre-signed URL, you must provide your security credentials, specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The pre-signed URLs are valid only for the specified duration. Option B is invalid because this would be too difficult to implement at a user level. Option C is invalid because this is not possible Option D is invalid because this is used to serve private content via Cloudfront For more information on pre-signed urls, please refer to the Link: http://docs.aws.amazon.com/AmazonS3/latest/dev/PresienedUrlUploadObiect.htmll The correct answer is: Generate pre-signed URLs for each user as they request access to protected S3 content"},{"question":"You currently operate a web application In the AWS US-East region. The application runs on an autoscaled layer of EC2 instances and an RDS Multi-AZ database. Your IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your EC2.IAM and RDS resources. The solution must ensure the integrity and confidentiality of your log data. Which of these solutions would you recommend?","choices":{"A":"Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option selected","B":"Use IAM roles S3 bucket policies and Mufti Factor Authentication (MFA) Delete on the S3 bucket that stores your logs.","C":"Create a new CloudTrail with one new S3 bucket to store the log","D":"Configure SNS to send log file delivery notifications to your management syste","E":"Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs.","F":"Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the global services option selected","G":"Use S3 ACLsand Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs.","H":"Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs and one for command","I":"Use IAM roles and S3 bucket policies on the S3 buckets that store your logs."},"answer":"A","explanation":"AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticated AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets. You need to ensure that all services are included. Hence option B is partially correct. Option B is invalid because you need to ensure that global services is select Option C is invalid because you should use bucket policies Option D is invalid because you should ideally just create one S3 bucket For more information on Cloudtrail, please visit the below URL: http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-inteeration.html The correct answer is: Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services o selected. Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs."},{"question":"You have an S3 bucket defined in AWS. You want to ensure that you encrypt the data before sending it across the wire. What is the best way to achieve this.","choices":{"A":"Enable server side encryption for the S3 bucke","B":"This request will ensure that the data is encrypted first.","C":"Use the AWS Encryption CLI to encrypt the data first","D":"Use a Lambda function to encrypt the data before sending it to the S3 bucket.","E":"Enable client encryption for the bucket"},"answer":"B","explanation":"One can use the AWS Encryption CLI to encrypt the data before sending it across to the S3 bucket. Options A and C are invalid because this would still mean that data is transferred in plain text Option D is invalid because you cannot just enable client side encryption for the S3 bucket For more information on Encrypting and Decrypting data, please visit the below URL: https://aws.amazonxom/blogs/securirv/how4o-encrvpt-and-decrypt-your-data-with-the-awsencryption- cl The correct answer is: Use the AWS Encryption CLI to encrypt the data first"},{"question":"Your company has a set of EC2 Instances defined in AWS. These Ec2 Instances have strict security groups attached to them. You need to ensure that changes to the Security groups are noted and acted on accordingly. How can you achieve this?","choices":{"A":"Use Cloudwatch logs to monitor the activity on the Security Group","B":"Use filters to search for the changes and use SNS for the notification.","C":"Use Cloudwatch metrics to monitor the activity on the Security Group","D":"Use filters to search for the changes and use SNS for the notification.","E":"Use AWS inspector to monitor the activity on the Security Group","F":"Use filters to search for the changes and use SNS f the notification.","G":"Use Cloudwatch events to be triggered for any changes to the Security Group","H":"Configure theLambda function for email notification as wel"},"answer":"D","explanation":"The below diagram from an AWS blog shows how security groups can be monitored Option A is invalid because you need to use Cloudwatch Events to check for chan, Option B is invalid because you need to use Cloudwatch Events to check for chang Option C is invalid because AWS inspector is not used to monitor the activity on Security Groups For more information on monitoring security groups, please visit the below URL: Ihttpsy/aws.amazon.com/blogs/security/how-to-automatically-revert-and-receive-notificationsabout- changes-to-your-amazonj 'pc-security-groups/ The correct answer is: Use Cloudwatch events to be triggered for any changes to the Security Groups. Configure the Lambda function for email notification as well."},{"question":"Your company has just set up a new central server in a VPC. There is a requirement for other teams who have their servers located in different VPC's in the same region to connect to the central server. Which of the below options is best suited to achieve this requirement.","choices":{"A":"Set up VPC peering between the central server VPC and each of the teams VPCs.","B":"Set up AWS DirectConnect between the central server VPC and each of the teams VPCs.","C":"Set up an IPSec Tunnel between the central server VPC and each of the teams VPCs.","D":"None of the above options will work."},"answer":"A","explanation":"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region. Options B and C are invalid because you need to use VPC Peering Option D is invalid because VPC Peering is available For more information on VPC Peering please see the below Link: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html The correct answer is: Set up VPC peering between the central server VPC and each of the teams VPCs."},{"question":"There is a requirement for a company to transfer large amounts of data between AWS and an onpremise location. There is an additional requirement for low latency and high consistency traffic to AWS. Given these requirements how would you design a hybrid architecture? Choose the correct answer from the options below","choices":{"A":"Provision a Direct Connect connection to an AWS region using a Direct Connect partner.","B":"Create a VPN tunnel for private connectivity, which increases network consistency and reduces latency.","C":"Create an iPSec tunnel for private connectivity, which increases network consistency and reduces latency.","D":"Create a VPC peering connection between AWS and the Customer gatewa"},"answer":"A","explanation":"AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect you can establish private connectivity between AWS and your datacenter, office, or colocation environment which in many cases can reduce your network costs, increase bandwidth throughput and provide a more consistent network experience than InternetQuestions & Answers PDF P-140 based connections. Options B and C are invalid because these options will not reduce network latency Options D is invalid because this is only used to connect 2 VPC's For more information on AWS direct connect, just browse to the below URL: https://aws.amazon.com/directconnect The correct answer is: Provision a Direct Connect connection to an AWS region using a Direct Connect partner. omit your Feedback/Queries to our Experts"},{"question":"A company's AWS account consists of approximately 300 IAM users. Now there is a mandate that an access change is required for 100 IAM users to have unlimited privileges to S3.As a system administrator, how can you implement this effectively so that there is no need to apply the policy at the individual user level?","choices":{"A":"Create a new role and add each user to the IAM role","B":"Use the IAM groups and add users, based upon their role, to different groups and apply the policy to group","C":"Create a policy and apply it to multiple users using a JSON script","D":"Create an S3 bucket policy with unlimited access which includes each user's AWS account ID"},"answer":"B","explanation":"Option A is incorrect since you don't add a user to the IAM Role Option C is incorrect since you don't assign multiple users to a policy Option D is incorrect since this is not an ideal approach An IAM group is used to collectively manage users who need the same set of permissions. By having groups, it becomes easier to manage permissions. So if you change the permissions on the group scale, it will affect all the users in that group For more information on IAM Groups, just browse to the below URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_eroups.html The correct answer is: Use the IAM groups and add users, based upon their role, to different groups and apply the policy to group"},{"question":"You need to create a policy and apply it for just an individual user. How could you accomplish this in the right way?","choices":{"A":"Add an AWS managed policy for the user","B":"Add a service policy for the user","C":"Add an IAM role for the user","D":"Add an inline policy for the user"},"answer":"D","explanation":"Options A and B are incorrect since you need to add an inline policy just for the user Option C is invalid because you don't assign an IAM role to a user The AWS Documentation mentions the following An inline policy is a policy that's embedded in a principal entity (a user, group, or role)that is, the policy is an inherent part of the principal entity. You can create a policy and embed it in a principal entity, either when you create the principal entity or later. For more information on IAM Access and Inline policies, just browse to the below URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/access The correct answer is: Add an inline policy for the user"},{"question":"There are currently multiple applications hosted in a VPC. During monitoring it has been noticed that multiple port scans are coming in from a specific IP Address block. The internal security team has requested that all offending IP Addresses be denied for the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP Address's.","choices":{"A":"Create an AD policy to modify the Windows Firewall settings on all hosts in the VPC to deny access from the IP Address block.","B":"Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block.","C":"Add a rule to all of the VPC Security Groups to deny access from the IP Address block.","D":"Modify the Windows Firewall settings on all AMI'S that your organization uses in that VPC to deny access from the IP address block."},"answer":"B","explanation":"NACL acts as a firewall at the subnet level of the VPC and we can deny the offending IP address block at the subnet level using NACL rules to block the incoming traffic to the VPC instances. Since NACL rules are applied as per the Rule numbers make sure that this rule number should take precedence over other rule numbers if there are any such rules that will allow traffic from these IP ranges. The lowest rule number has more precedence over a rule that has a higher number. The AWS Documentation mentions the following as a best practices for IAM users For extra security, enable multi-factor authentication (MFA) for privileged IAM users (users who are allowed access to sensitive resources or APIs). With MFA, users have a device that generates a unique authentication code (a one-time password, or OTP). Users must provide both their normal credentials (like their user name and password) and the OTP. The MFA device can either be a special piece of hardware, or it can be a virtual device (for example, it can run in an app on a smartphone). Options C is invalid because these options are not available Option D is invalid because there is not root access for users For more information on IAM best practices, please visit the below URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html The correct answer is: Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block. omit your Feedback/Queries to our Experts"},{"question":"You are hosting a web site via website hosting on an S3 bucket - http://demo.s3-website-us-east-l .amazonaws.com. You have some web pages that use Javascript that access resources in another bucket which has web site hosting also enabled. But when users access the web pages , they are getting a blocked Javascript error. How can you rectify this?","choices":{"A":"Enable CORS for the bucket","B":"Enable versioning for the bucket","C":"Enable MFA for the bucket","D":"Enable CRR for the bucket"},"answer":"A","explanation":"Such a scenario is also given in the AWS Documentation Cross-Origin Resource Sharing: Use-case Scenarios The following are example scenarios for using CORS:  Scenario 1: Suppose that you are hosting a website in an Amazon S3 bucket named website as described in Hosting a Static Website on Amazon S3. Your users load the website endpoint http://website.s3-website-us-east-1 .amazonaws.com. Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket website.s3.amazonaws.com. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross-origin requests from website.s3-website-us-east-1 .amazonaws.com.  Scenario 2: Suppose that you want to host a web font from your S3 bucket. Again, browsers require a CORS check (also called a preflight check) for loading web fonts. You would configure the bucket that is hosting the web font to allow any origin to make these requests. Option Bis invalid because versioning is only to create multiple versions of an object and can help in accidental deletion of objects Option C is invalid because this is used as an extra measure of caution for deletion of objects Option D is invalid because this is used for Cross region replication of objects For more information on Cross Origin Resource sharing, please visit the following URL  ittps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html The correct answer is: Enable CORS for the bucket"},{"question":"You have an Ec2 Instance in a private subnet which needs to access the KMS service. Which of the following methods can help fulfil this requirement, keeping security in perspective","choices":{"A":"Use a VPC endpoint","B":"Attach an Internet gateway to the subnet","C":"Attach a VPN connection to the VPC","D":"Use VPC Peering"},"answer":"A","explanation":"The AWS Documentation mentions the following You can connect directly to AWS KMS through a private endpoint in your VPC instead of connecting over the internet. When you use a VPC endpoint communication between your VPC and AWS KMS is conducted entirely within the AWS network. Option B is invalid because this could open threats from the internet Option C is invalid because this is normally used for communication between on-premise environments and AWS. Option D is invalid because this is normally used for communication between VPCs For more information on accessing KMS via an endpoint, please visit the following URL https://docs.aws.amazon.com/kms/latest/developerguide/kms-vpc- endpoint.htmll The correct answer is: Use a VPC endpoint"},{"question":"You are deivising a policy to allow users to have the ability to access objects in a bucket called appbucket. You define the below custom bucket policy But when you try to apply the policy you get the error \\"Action does not apply to any resource(s) in statement.\\" What should be done to rectify the error","choices":{"A":"Change the IAM permissions by applying PutBucketPolicy permissions.","B":"Verify that the policy has the same name as the bucket nam","C":"If no","D":"make it the same.","E":"Change the Resource section to \\"arn:aws:s3:::appbucket/*'.","F":"Create the bucket \\"appbucket\\" and then apply the polic"},"answer":"C","explanation":"When you define access to objects in a bucket you need to ensure that you specify to which objects in the bucket access needs to be given to. In this case, the * can be used to assign the permission to all objects in the bucket Option A is invalid because the right permissions are already provided as per the question requirement Option B is invalid because it is not necessary that the policy has the same name as the bucket Option D is invalid because this should be the default flow for applying the policy For more information on bucket policies please visit the below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.htmll The correct answer is: Change the Resource section to \\"arn:aws:s3:::appbucket/\\""},{"question":"You have an instance setup in a test environment in AWS. You installed the required application and the promoted the server to a production environment. Your IT Security team has advised that there maybe traffic flowing in from an unknown IP address to port 22. How can this be mitigated immediately?","choices":{"A":"Shutdown the instance","B":"Remove the rule for incoming traffic on port 22 for the Security Group","C":"Change the AMI for the instance","D":"Change the Instance type for the instance"},"answer":"B","explanation":"In the test environment the security groups might have been opened to all IP addresses for testing purpose. Always to ensure to remove this rule once all testing is completed. Option A, C and D are all invalid because this would affect the application running on the server. The easiest way is just to remove the rule for access on port 22. For more information on authorizing access to an instance, please visit the below URL: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing- access-to-an-instance.htmll The correct answer is: Remove the rule for incoming traffic on port 22 for the Security Group"},{"question":"Your IT Security team has advised to carry out a penetration test on the resources in their company's AWS Account. This is as part of their capability to analyze the security of the Infrastructure. What should be done first in this regard?","choices":{"A":"Turn on Cloud trail and carry out the penetration test","B":"Turn on VPC Flow Logs and carry out the penetration test","C":"Submit a request to AWS Support","D":"Use a custom AWS Marketplace solution for conducting the penetration test"},"answer":"C","explanation":"This concept is given in the AWS Documentation How do I submit a penetration testing request for my AWS resources? Issue I want to run a penetration test or other simulated event on my AWS architecture. How do I get permission from AWS to do that? Resolution Before performing security testing on AWS resources, you must obtain approval from AWS. After you submit your request AWS will reply in about two business days. AWS might have additional questions about your test which can extend the approval process, so plan accordingly and be sure that your initial request is as detailed as possible. If your request is approved, you'll receive an authorization number. Option A.B and D are all invalid because the first step is to get prior authorization from AWS for penetration tests For more information on penetration testing, please visit the below URL * https://aws.amazon.com/security/penetration-testing/ * https://aws.amazon.com/premiumsupport/knowledge-center/penetration-testing/ ( The correct answer is: Submit a request to AWS Support"},{"question":"You have just recently set up a web and database tier in a VPC and hosted the application. When testing the app , you are not able to reach the home page for the app. You have verified the security groups. What can help you diagnose the issue.","choices":{"A":"Use the AWS Trusted Advisor to see what can be done.","B":"Use VPC Flow logs to diagnose the traffic","C":"Use AWS WAF to analyze the traffic","D":"Use AWS Guard Duty to analyze the traffic"},"answer":"B","explanation":"Option A is invalid because this can be used to check for security issues in your account, but not verify as to why you cannot reach the home page for your application Option C is invalid because this used to protect your app against application layer attacks, but not verify as to why you cannot reach the home page for your application Option D is invalid because this used to protect your instance against attacks, but not verify as to why you cannot reach the home page for your application The AWS Documentation mentions the following VPC Flow Logs capture network flow information for a VPC, subnet or network interface and stores it in Amazon CloudWatch Logs. Flow log data can help customers troubleshoot network issues; for example, to diagnose why specific traffic is not reaching an instance, which might be a result of overly restrictive security group rules. Customers can also use flow logs as a security toi to monitor the traffic that reaches their instances, to profile network traffic, and to look for abnormal traffic behaviors. For more information on AWS Security, please visit the following URL: https://aws.amazon.com/answers/networking/vpc-security-capabilities> The correct answer is: Use VPC Flow logs to diagnose the traffic"},{"question":"Which of the following is used as a secure way to log into an EC2 Linux Instance?","choices":{"A":"IAM User name and password","B":"Key pairs","C":"AWS Access keys","D":"AWS SDK keys"},"answer":"B","explanation":"The AWS Documentation mentions the following Key pairs consist of a public key and a private key. You use the private key to create a digital signature, and then AWS uses the corresponding public key to validate the signature. Key pairs are used only for Amazon EC2 and Amazon CloudFront. Option A.C and D are all wrong because these are not used to log into EC2 Linux Instances For more information on AWS Security credentials, please visit the below URL: https://docs.aws.amazon.com/eeneral/latest/er/aws-sec-cred-types.html The correct answer is: Key pairs"},{"question":"You want to ensure that you keep a check on the Active EBS Volumes, Active snapshots and Elastic IP addresses you use so that you don't go beyond the service limit. Which of the below services can help in this regard?","choices":{"A":"AWS Cloudwatch","B":"AWS EC2","C":"AWS Trusted Advisor","D":"AWS SNS"},"answer":"C","explanation":"Below is a snapshot of the service limits that the Trusted Advisor can monitor Option A is invalid because even though you can monitor resources, it cannot be checked against the service limit. Option B is invalid because this is the Elastic Compute cloud service Option D is invalid because it can be send notification but not check on service limit For more information on the Trusted Advisor monitoring, please visit the below URL: https://aws.amazon.com/premiumsupport/ta-faqs> The correct answer is: AWS Trusted Advisor"},{"question":"Every application in a company's portfolio has a separate AWS account for development and production. The security team wants to prevent the root user and all IAM users in the production accounts from accessing a specific set of unneeded services. How can they control this functionality?","choices":{"A":"Create a Service Control Policy that denies access to the service","B":"Assemble all production accounts in an organizational uni","C":"Apply the policy to that organizational unit.","D":"Create a Service Control Policy that denies access to the service","E":"Apply the policy to the root account.","F":"Create an IAM policy that denies access to the service","G":"Associate the policy with an IAM group and enlist all users and the root users in this group.","H":"Create an IAM policy that denies access to the service","I":"Create a Config Rule that checks that all users have the policy m assigne","J":"Trigger a Lambda function that adds the policy when found missing."},"answer":"A","explanation":"As an administrator of the master account of an organization, you can restrict which AWS services and individual API actions the users and roles in each member account can access. This restriction even overrides the administrators of member accounts in the organization. When AWS Organizations blocks access to a service or API action for a member account a user or role in that account can't access any prohibited service or API action, even if an administrator of a member account explicitly grants such permissions in an IAM policy. Organization permissions overrule account permissions. Option B is invalid because service policies cannot be assigned to the root account at the account level. Option C and D are invalid because IAM policies alone at the account level would not be able to suffice the requirement For more information, please visit the below URL id=docs_orgs_console https://docs.aws.amazon.com/IAM/latest/UserGi manage attach-policy.html The correct answer is: Create a Service Control Policy that denies access to the services. Assemble all production accounts in an organizational unit. Apply the policy to that organizational unit"},{"question":"You are building a large-scale confidential documentation web server on AWSand all of the documentation for it will be stored on S3. One of the requirements is that it cannot be publicly accessible from S3 directly, and you will need to use Cloud Front to accomplish this. Which of the methods listed below would satisfy the requirements as outlined? Choose an answer from the options below","choices":{"A":"Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM User.","B":"Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAl.","C":"Create individual policies for each bucket the documents are stored in and in that policy grant access to only CloudFront.","D":"Create an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN)."},"answer":"B","explanation":"If you want to use CloudFront signed URLs or signed cookies to provide access to objects in your Amazon S3 bucket you probably also want to prevent users from accessing your Amazon S3 objects using Amazon S3 URLs. If users access your objects directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies, for example, control over the date and time that a user can no longer access your content and control over which IP addresses can be used to access content. In addition, if user's access objects both through CloudFront and directly by using Amazon S3 URLs, CloudFront ace logs are less useful because they're incomplete. Option A is invalid because you need to create a Origin Access Identity for Cloudfront and not an IAM user Option C and D are invalid because using policies will not help fulfil the requirement For more information on Origin Access Identity please see the below Link: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-contentrestrictine- access-to-s3.htmll The correct answer is: Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI. ("},{"question":"Your current setup in AWS consists of the following architecture. 2 public subnets, one subnet which has the web servers accessed by users across the internet and the other subnet for the database server. Which of the following changes to the architecture would add a better security boundary to the resources hosted in your setup","choices":{"A":"Consider moving the web server to a private subnet","B":"Consider moving the database server to a private subnet","C":"Consider moving both the web and database server to a private subnet","D":"Consider creating a private subnet and adding a NAT instance to that subnet"},"answer":"B","explanation":"The ideal setup is to ensure that the web server is hosted in the public subnet so that it can be accessed by users on the internet. The database server can be hosted in the private subnet. The below diagram from the AWS Documentation shows how this can be setup Option A and C are invalid because if you move the web server to a private subnet, then it cannot be accessed by users Option D is invalid because NAT instances should be present in the public subnet For more information on public and private subnets in AWS, please visit the following url .com/AmazonVPC/latest/UserGuide/VPC Scenario2. The correct answer is: Consider moving the database server to a private subnet"},{"question":"You need to have a cloud security device which would allow to generate encryption keys based on FIPS 140-2 Level 3. Which of the following can be used for this purpose.","choices":{"A":"AWS KMS","B":"AWS Customer Keys","C":"AWS managed keys","D":"AWS Cloud HSM"},"answer":"AD","explanation":"AWS Key Management Service (KMS) now uses FIPS 140-2 validated hardware security modules (HSM) and supports FIPS 140-2 validated endpoints, which provide independent assurances about the confidentiality and integrity of your keys. All master keys in AWS KMS regardless of their creation date or origin are automatically protected using FIPS 140-2 validated HSMs. defines four levels of security, simply named \\"Level 1'' to \\"Level 4\\". It does not specify in detail what level of security is required by any particular application.  FIPS 140-2 Level 1 the lowest, imposes very limited requirements; loosely, all components must be \\"production-grade\\" anc various egregious kinds of insecurity must be absent  FIPS 140-2 Level 2 adds requirements for physical tamper-evidence and role-based authentication.  FIPS 140-2 Level 3 adds requirements for physical tamper-resistance (making it difficult for attackers to gain access to sensitive information contained in the module) and identity-based authentication, and for a physical or logical separation between the interfaces by which \\"critical security parameters\\" enter and leave the module, and its other interfaces.  FIPS 140-2 Level 4 makes the physical security requirements more stringent and requires robustness against environmental attacks. AWSCIoudHSM provides you with a FIPS 140-2 Level 3 validated single-tenant HSM cluster in your Amazon Virtual Private Cloud (VPQ to store and use your keys. You have exclusive control over how your keys are used via an authentication mechanism independent from AWS. You interact with keys in your AWS CloudHSM cluster similar to the way you interact with your applications running in Amazon EC2. AWS KMS allows you to create and control the encryption keys used by your applications and supported AWS services in multiple regions around the world from a single console. The service uses a FIPS 140-2 validated HSM to protect the security of your keys. Centralized management of all your keys in AWS KMS lets you enforce who can use your keys under which conditions, when they get rotated, and who can manage them. AWS KMS HSMs are validated at level 2 overall and at level 3 in the following areas:  Cryptographic Module Specification  Roles, Services, and Authentication  Physical Security  Design Assurance So I think that we can have 2 answers for this question. Both A & D.  https://aws.amazon.com/blo15s/security/aws-key-management-service- now-ffers-flps-140-2- validated-cryptographic-m< enabling-easier-adoption-of-the-service- for-regulated-workloads/  https://a ws.amazon.com/cloudhsm/faqs/  https://aws.amazon.com/kms/faqs/  https://en.wikipedia.org/wiki/RPS The AWS Documentation mentions the following AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys using FIPS 140-2 Level 3 validated HSMs. CloudHSM offers you the filexibility to integrate with your applications using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions ()CE). and Microsoft CryptoNG (CNG) libraries. CloudHSM is also standardscompliant and enables you to export all of your keys to most other commercially-available HSMs. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. CloudHSM also enables you to scale quickly by adding and removing HSM capacity on-demand, with no up-front costs. All other options are invalid since AWS Cloud HSM is the prime service that offers FIPS 140-2 Level 3 compliance For more information on CloudHSM, please visit the following url https://aws.amazon.com/cloudhsm; The correct answers are: AWS KMS, AWS Cloud HSM"},{"question":"A company stores critical data in an S3 bucket. There is a requirement to ensure that an extra level of security is added to the S3 bucket. In addition , it should be ensured that objects are available in a secondary region if the primary one goes down. Which of the following can help fulfil these requirements? Choose 2 answers from the options given below","choices":{"A":"Enable bucket versioning and also enable CRR","B":"Enable bucket versioning and enable Master Pays","C":"For the Bucket policy add a condition for {\\"Null\\": {\\"aws:MultiFactorAuthAge\\": true}}","D":"Enable the Bucket ACL and add a condition for {\\"Null\\": {\\"aws:MultiFactorAuthAge\\": true}}"},"answer":"AC","explanation":"The AWS Documentation mentions the following Adding a Bucket Policy to Require MFA Amazon S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security you can apply to your AWS environment. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code. For more information, go to AWS Multi-Factor Authentication. You can require MFA authentication for any requests to access your Amazoi. S3 resources. You can enforce the MFA authentication requirement using the aws:MultiFactorAuthAge key in a bucket policy. IAM users car access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service (STS). You provide the MFA code at the time of the STS request. When Amazon S3 receives a request with MFA authentication, the aws:MultiFactorAuthAge key provides a numeric value indicating how long ago (in seconds) the temporary credential was created. If the temporary credential provided in the request was not created using an MFA device, this key value is null (absent). In a bucket policy, you can add a condition to check this value, as shown in the following example bucket policy. The policy denies any Amazon S3 operation on the /taxdocuments folder in the examplebucket bucket if the request is not MFA authenticated. To learn more about MFA authentication, see Using Multi-Factor Authentication (MFA) in AWS in the IAM User Guide. Option B is invalid because just enabling bucket versioning will not guarantee replication of objects Option D is invalid because the condition for the bucket policy needs to be set accordingly For more information on example bucket policies, please visit the following URL:  https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html Also versioning and Cross Region replication can ensure that objects will be available in the destination region in case the primary region fails. For more information on CRR, please visit the following URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html The correct answers are: Enable bucket versioning and also enable CRR, For the Bucket policy add a condition for {\\"Null\\": { \\"aws:MultiFactorAuthAge\\": true}}"},{"question":"You are trying to use the Systems Manager to patch a set of EC2 systems. Some of the systems are not getting covered in the patching process. Which of the following can be used to troubleshoot the issue? Choose 3 answers from the options given below.","choices":{"A":"Check to see if the right role has been assigned to the EC2 instances","B":"Check to see if the IAM user has the right permissions for EC2","C":"Ensure that agent is running on the instances.","D":"Check the Instance status by using the Health AP"},"answer":"ACD","explanation":"For ensuring that the instances are configured properly you need to ensure the followi . 1) You installed the latest version of the SSM Agent on your instance 2) Your instance is configured with an AWS Identity and Access Management (IAM) role that enables the instance to communicate with the Systems Manager API 3) You can use the Amazon EC2 Health API to quickly determine the following information about Amazon EC2 instances The status of one or more instances The last time the instance sent a heartbeat value The version of the SSM Agent The operating system The version of the EC2Config service (Windows) The status of the EC2Config service (Windows) Option B is invalid because IAM users are not supposed to be directly granted permissions to EC2 Instances For more information on troubleshooting AWS SSM, please visit the following URL: https://docs.aws.amazon.com/systems-manager/latest/userguide/troubleshooting-remotecommands. html The correct answers are: Check to see if the right role has been assigned to the EC2 Instances, Ensure that agent is running on the Instances., Check the Instance status by using the Health API."},{"question":"You have a requirement to serve up private content using the keys available with Cloudfront. How can this be achieved?","choices":{"A":"Add the keys to the backend distribution.","B":"Add the keys to the S3 bucket","C":"Create pre-signed URL's","D":"Use AWS Access keys"},"answer":"C","explanation":"Option A and B are invalid because you will not add keys to either the backend distribution or the S3 bucket. Option D is invalid because this is used for programmatic access to AWS resources You can use Cloudfront key pairs to create a trusted pre-signed URL which can be distributed to users Specifying the AWS Accounts That Can Create Signed URLs and Signed Cookies (Trusted Signers) Topics  Creating CloudFront Key Pairs for Your Trusted Signers  Reformatting the CloudFront Private Key (.NET and Java Only)  Adding Trusted Signers to Your Distribution  Verifying that Trusted Signers Are Active (Optional) 1 Rotating CloudFront Key Pairs To create signed URLs or signed cookies, you need at least one AWS account that has an active CloudFront key pair. This accou is known as a trusted signer. The trusted signer has two purposes:  As soon as you add the AWS account ID for your trusted signer to your distribution, CloudFront starts to require that users us signed URLs or signed cookies to access your objects. ' When you create signed URLs or signed cookies, you use the private key from the trusted signer's key pair to sign a portion of the URL or the cookie. When someone requests a restricted object CloudFront compares the signed portion of the URL or cookie with the unsigned portion to verify that the URL or cookie hasn't been tampered with. CloudFront also verifies that the URL or cookie is valid, meaning, for example, that the expiration date and time hasn't passed. For more information on Cloudfront private trusted content please visit the following URL:  https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-contenttrusted- s The correct answer is: Create pre-signed URL's"},{"question":"You are building a system to distribute confidential training videos to employees. Using CloudFront, what method could be used to serve content that is stored in S3, but not publicly accessible from S3 directly?","choices":{"A":"Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAl.","B":"Add the CloudFront account security group \\"amazon-cf/amazon-cf-sg\\" to the appropriate S3 bucket policy.","C":"Create an Identity and Access Management (IAM) User for CloudFront and grant access to the objects in your S3 bucket to that IAM User.","D":"Create a S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN)."},"answer":"AExplanation:","explanation":"Explanation: You can optionally secure the content in your Amazon S3 bucket so users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents anyone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. This step isn't required to use signed URLs, but we recommend it To require that users access your content through CloudFront URLs, you perform the following tasks: Create a special CloudFront user called an origin access identity. Give the origin access identity permission to read the objects in your bucket. Remove permission for anyone else to use Amazon S3 URLs to read the objects. Option B,C and D are all automatically invalid, because the right way is to ensure to create Origin Access Identity (OAI) for CloudFront and grant access accordingly. For more information on serving private content via Cloudfront, please visit the following URL: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.htmll The correct answer is: Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket t that OAI. You can optionally secure the content in your Amazon S3 bucket so users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents anyone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. This step isn't required to use signed URLs, but we recommend it To require that users access your content through CloudFront URLs, you perform the following tasks: Create a special CloudFront user called an origin access identity. Give the origin access identity permission to read the objects in your bucket. Remove permission for anyone else to use Amazon S3 URLs to read the objects. Option B,C and D are all automatically invalid, because the right way is to ensure to create Origin Access Identity (OAI) for CloudFront and grant access accordingly. For more information on serving private content via Cloudfront, please visit the following URL: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.htmll The correct answer is: Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket t that OAI."},{"question":"A company has an existing AWS account and a set of critical resources hosted in that account. The employee who was in-charge of the root account has left the company. What must be now done to secure the account. Choose 3 answers from the options given below.","choices":{"A":"Change the access keys for all IAM users.","B":"Delete all custom created IAM policies","C":"Delete the access keys for the root account","D":"Confirm MFAtoa secure device","E":"Change the password for the root account","F":"Change the password for all IAM users"},"answer":"CDE","explanation":"Now if the root account has a chance to be compromised, then you have to carry out the below steps 1. Delete the access keys for the root account 2. Confirm MFA to a secure device 3. Change the password for the root account This will ensure the employee who has left has no change to compromise the resources in AWS. Option A is invalid because this would hamper the working of the current IAM users Option B is invalid because this could hamper the current working of services in your AWS account Option F is invalid because this would hamper the working of the current IAM users For more information on IAM root user, please visit the following URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/id root-user.html The correct answers are: Delete the access keys for the root account Confirm MFA to a secure device. Change the password for the root account"},{"question":"Your company has a requirement to work with a DynamoDB table. There is a security mandate that all data should be encrypted at rest. What is the easiest way to accomplish this for DynamoDB.","choices":{"A":"Use the AWS SDK to encrypt the data before sending it to the DynamoDB table","B":"Encrypt the DynamoDB table using KMS during its creation","C":"Encrypt the table using AWS KMS after it is created","D":"Use S3 buckets to encrypt the data before sending it to DynamoDB"},"answer":"B","explanation":"The most easiest option is to enable encryption when the DynamoDB table is created. The AWS Documentation mentions the following Amazon DynamoDB offers fully managed encryption at rest. DynamoDB encryption at rest provides enhanced security by encrypting your data at rest using an AWS Key Management Service (AWS KMS) managed encryption key for DynamoDB. This functionality eliminates the operational burden and complexity involved in protecting sensitive data. Option A is partially correct, you can use the AWS SDK to encrypt the data, but the easier option would be to encrypt the table before hand. Option C is invalid because you cannot encrypt the table after it is created Option D is invalid because encryption for S3 buckets is for the objects in S3 only. For more information on securing data at rest for DynamoDB please refer to below URL: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.htmll The correct answer is: Encrypt the DynamoDB table using KMS during its creation"},{"question":"A company has a large set of keys defined in AWS KMS. Their developers frequently use the keys for the applications being developed. What is one of the ways that can be used to reduce the cost of accessing the keys in the AWS KMS service.","choices":{"A":"Enable rotation of the keys","B":"Use Data key caching","C":"Create an alias of the key","D":"Use the right key policy"},"answer":"B","explanation":"The AWS Documentation mentions the following Data key caching stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generatir a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales. Option A.C and D are all incorrect since these options will not impact how the key is used. For more information on data key caching, please refer to below URL: https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-cachine.htmll The correct answer is: Use Data key caching"},{"question":"In order to encrypt data in transit for a connection to an AWS RDS instance, which of the following would you implement","choices":{"A":"Transparent data encryption","B":"SSL from your application","C":"Data keys from AWS KMS","D":"Data Keys from CloudHSM"},"answer":"B","explanation":"This is mentioned in the AWS Documentation You can use SSL from your application to encrypt a connection to a DB instance running MySQL MariaDB, Amazon Aurora, SQL Server, Oracle, or PostgreSQL. Option A is incorrect since Transparent data encryption is used for data at rest and not in transit Options C and D are incorrect since keys can be used for encryption of data at rest For more information on working with RDS and SSL, please refer to below URL: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html The correct answer is: SSL from your application"},{"question":"Your company has just started using AWS and created an AWS account. They are aware of the potential issues when root access is enabled. How can they best safeguard the account when it comes to root access? Choose 2 answers fro the options given below","choices":{"A":"Delete the root access account","B":"Create an Admin IAM user with the necessary permissions","C":"Change the password for the root account.","D":"Delete the root access keys"},"answer":"BD","explanation":"The AWS Documentation mentions the following All AWS accounts have root user credentials (that is, the credentials of the account owner). These credentials allow full access to all resources in the account. Because you cant restrict permissions for root user credentials, we recommend that you delete your root user access keys. Then create AWS Identity and Access Management (IAM) user credentials for everyday interaction with AWS. Option A is incorrect since you cannot delete the root access account Option C is partially correct but cannot be used as the ideal solution for safeguarding the account For more information on root access vs admin IAM users, please refer to below URL: https://docs.aws.amazon.com/eeneral/latest/er/root-vs-iam.html The correct answers are: Create an Admin IAM user with the necessary permissions. Delete the root access keys"},{"question":"An auditor needs access to logs that record all API events on AWS. The auditor only needs read-only access to the log files and does not need access to each AWS account. The company has multiple AWS accounts, and the auditor needs access to all the logs for all the accounts. What is the best way to configure access for the auditor to view event logs from all accounts? Choose the correct answer from the options below","choices":{"A":"Configure the CloudTrail service in each AWS account, and have the logs delivered to an AWS bucket on each account, while granting the auditor permissions to the bucket via roles in the secondary accounts and a single primary IAM account that can assume a read-only role in the secondary AWS accounts.","B":"Configure the CloudTrail service in the primary AWS account and configure consolidated billing for all the secondary account","C":"Then grant the auditor access to the S3 bucket that receives theCloudTrail log files.","D":"Configure the CloudTrail service in each AWS account and enable consolidated logging inside of CloudTrail.","E":"Configure the CloudTrail service in each AWS account and have the logs delivered to a single AWS bucket in the primary account and erant the auditor access to that single bucket in the orimarvaccoun"},"answer":"D","explanation":"Given the current requirements, assume the method of \\"least privilege\\" security design and only allow the auditor access to the minimum amount of AWS resources as possibli AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting only be granted access in one location Option Option A is incorrect since the auditor should B is incorrect since consolidated billing is not a key requirement as part of the question Option C is incorrect since there is not consolidated logging For more information on Cloudtrail please refer to the below URL: https://aws.amazon.com/cloudtrail ( The correct answer is: Configure the CloudTrail service in each AWS account and have the logs delivered to a single AWS bud in the primary account and grant the auditor access to that single bucket in the primary account."},{"question":"You have been given a new brief from your supervisor for a client who needs a web application set up on AWS. The a most important requirement is that MySQL must be used as the database, and this database must not be hosted in the public cloud, but rather at the client's data center due to security risks. Which of the following solutions would be the best to assure that the client's requirements are met? Choose the correct answer from the options below","choices":{"A":"Build the application server on a public subnet and the database at the client's data cente","B":"Connect them with a VPN connection which uses IPsec.","C":"Use the public subnet for the application server and use RDS with a storage gateway to access and synchronize the data securely from the local data center.","D":"Build the application server on a public subnet and the database on a private subnet with a NAT instance between them.","E":"Build the application server on a public subnet and build the database in a private subnet with a secure ssh connection to the private subnet from the client's data center"},"answer":"A","explanation":"Since the database should not be hosted on the cloud all other options are invalid. The best option is to create a VPN connection for securing traffic as shown below. Option B is invalid because this is the incorrect use of the Storage gateway Option C is invalid since this is the incorrect use of the NAT instance Option D is invalid since this is an incorrect configuration For more information on VPN connections, please visit the below URL http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.htmll The correct answer is: Build the application server on a public subnet and the database at the client's data center. Connect them with a VPN connection which uses IPsec"},{"question":"A company has been using the AW5 KMS service for managing its keys. They are planning on carrying out housekeeping activities and deleting keys which are no longer in use. What are the ways that can be incorporated to see which keys are in use? Choose 2 answers from the options given below","choices":{"A":"Determine the age of the master key","B":"See who is assigned permissions to the master key","C":"See Cloudtrail for usage of the key","D":"Use AWS cloudwatch events for events generated for the key"},"answer":"BC","explanation":"The direct ways that can be used to see how the key is being used is to see the current access permissions and cloudtrail logs Option A is invalid because seeing how long ago the key was created would not determine the usage of the key Option D is invalid because Cloudtrail Event is better for seeing for events generated by the key This is also mentioned in the AWS Documentation Examining CMK Permissions to Determine the Scope of Potential Usage Determining who or what currently has access to a customer master key (CMK) might help you determine how widely the CM was used and whether it is still needed. To learn how to determine who or what currently has access to a CMK, go to Determining Access to an AWS KMS Customer Master Key. Examining AWS CloudTrail Logs to Determine Actual Usage AWS KMS is integrated with AWS CloudTrail, so all AWS KMS API activity is recorded in CloudTrail log files. If you have CloudTrail turned on in the region where your customer master key (CMK) is located, you can examine your CloudTrail log files to view a history of all AWS KMS API activity for a particular CMK, and thus its usage history. You might be able to use a CMK's usage history to help you determine whether or not you still need it For more information on determining the usage of CMK keys, please visit the following URL: https://docs.aws.amazon.com/kms/latest/developerguide/deleting- keys-determining-usage.html The correct answers are: See who is assigned permissions to the master key. See Cloudtrail for usage of the key"},{"question":"A company wants to use Cloudtrail for logging all API activity. They want to segregate the logging of data events and management events. How can this be achieved? Choose 2 answers from the options given below","choices":{"A":"Create one Cloudtrail log group for data events","B":"Create one trail that logs data events to an S3 bucket","C":"Create another trail that logs management events to another S3 bucket","D":"Create another Cloudtrail log group for management events"},"answer":"BC","explanation":"The AWS Documentation mentions the following You can configure multiple trails differently so that the trails process and log only the events that you specify. For example, one trail can log read-only data and management events, so that all read-only events are delivered to one S3 bucket. Another trail can log only write-only data and management events, so that all write-only events are delivered to a separate S3 bucket Options A and D are invalid because you have to create a trail and not a log group For more information on managing events with cloudtrail, please visit the following URL: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/loHEing-manasement-and-dataevents- with-cloudtrai The correct answers are: Create one trail that logs data events to an S3 bucket. Create another trail that logs management events to another S3 bucket"},{"question":"An application is designed to run on an EC2 Instance. The applications needs to work with an S3 bucket. From a security perspective , what is the ideal way for the EC2 instance/ application to be configured?","choices":{"A":"Use the AWS access keys ensuring that they are frequently rotated.","B":"Assign an IAM user to the application that has specific access to only that S3 bucket","C":"Assign an IAM Role and assign it to the EC2 Instance","D":"Assign an IAM group and assign it to the EC2 Instance"},"answer":"C","explanation":"The below diagram from the AWS whitepaper shows the best security practicse of allocating a role that has access to the S3 bucket Options A,B and D are invalid because using users, groups or access keys is an invalid security practise when giving access to resources from other AWS resources. For more information on the Security Best practices, please visit the following URL: https://d1.awsstatic.com/whitepapers/Security/AWS Security Best Practices.pdl The correct answer is: Assign an IAM Role and assign it to the EC2 Instance"},{"question":"A company hosts critical data in an S3 bucket. Even though they have assigned the appropriate permissions to the bucket, they are still worried about data deletion. What measures can be taken to restrict the risk of data deletion on the bucket. Choose 2 answers from the options given below","choices":{"A":"Enable versioning on the S3 bucket","B":"Enable data at rest for the objects in the bucket","C":"Enable MFA Delete in the bucket policy","D":"Enable data in transit for the objects in the bucket"},"answer":"AC","explanation":"One of the AWS Security blogs mentions the followinj Versioning keeps multiple versions of an object in the same bucket. When you enable it on a bucket Amazon S3 automatically adds a unique version ID to every object stored in the bucket. At that point, a simple DELETE action does not permanently delete an object version; it merely associates a delete marker with the object. If you want to permanently delete an object version, you must specify its version ID in your DELETE request. You can add another layer of protection by enabling MFA Delete on a versioned bucket. Once you do so, you must provide your AWS accounts access keys and a valid code from the account's MFA device in order to permanently delete an object version or suspend or reactivate versioning on the bucket. Option B is invalid because enabling encryption does not guarantee risk of data deletion. Option D is invalid because this option does not guarantee risk of data deletion. For more information on AWS S3 versioning and MFA please refer to the below URL: https://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfa- part-3/"},{"question":"The correct answers are: Enable versioning on the S3 bucket Enable MFA Delete in the bucket policy You company has mandated that all data in AWS be encrypted at rest. How can you achieve this for EBS volumes? Choose 2 answers from the options given below","choices":{"A":"Use Windows bit locker for EBS volumes on Windows instances","B":"Use TrueEncrypt for EBS volumes on Linux instances","C":"Use AWS Systems Manager to encrypt the existing EBS volumes","D":"Boot EBS volume can be encrypted during launch without using custom AMI"},"answer":"AB","explanation":"EBS encryption can also be enabled when the volume is created and not for existing volumes. One can use existing tools for OS level encryption. Option C is incorrect. AWS Systems Manager is a management service that helps you automatically collect software inventory, apply OS patches, create system images, and configure Windows and Linux operating systems. Option D is incorrect You cannot choose to encrypt a non-encrypted boot volume on instance launch. To have encrypted boot volumes during launch , your custom AMI must have it's boot volume encrypted before launch. For more information on the Security Best practices, please visit the following URL: .com/whit Security Practices. The correct answers are: Use Windows bit locker for EBS volumes on Windows instances. Use TrueEncrypt for EBS volumes on Linux instances"},{"question":"A company has a set of EC2 instances hosted in AWS. These instances have EBS volumes for storing critical information. There is a business continuity requirement and in order to boost the agility of the business and to ensure data durability which of the following options are not required.","choices":{"A":"Use lifecycle policies for the EBS volumes","B":"Use EBS Snapshots","C":"Use EBS volume replication","D":"Use EBS volume encryption"},"answer":"CD","explanation":"Data stored in Amazon EBS volumes is redundantly stored in multiple physical locations as part of normal operation of those services and at no additional charge. However, Amazon EBS replication is stored within the same availability zone, not across multiple zones; therefore, it is highly recommended that you conduct regular snapshots to Amazon S3 for long-term data durability. You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. With lifecycle management, you can be sure that snapshots are cleaned up regularly and keep costs under control. EBS Lifecycle Policies A lifecycle policy consists of these core settings:  Resource typeThe AWS resource managed by the policy, in this case, EBS volumes.  Target tagThe tag that must be associated with an EBS volume for it to be managed by the policy.  ScheduleDefines how often to create snapshots and the maximum number of snapshots to keep. Snapshot creation starts within an hour of the specified start time. If creating a new snapshot exceeds the maximum number of snapshots to keep for the volume, the oldest snapshot is deleted. Option C is correct. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability. But it does not have an explicit feature like that. Option D is correct Encryption does not ensure data durability For information on security for Compute Resources, please visit the below URL https://d1.awsstatic.com/whitepapers/Security/Security Compute Services Whitepaper.pdl The correct answers are: Use EBS volume replication. Use EBS volume encryption"},{"question":"Your company has the following setup in AWS\\n- A set of EC2 Instances hosting a web application\\n- An application load balancer placed in front of the EC2 InstancesThere seems to be a set of malicious requests coming from a set of IP addresse\\n- Which of the following can be used to protect against these requests?","choices":{"A":"Use Security Groups to block the IP addresses","B":"Use VPC Flow Logs to block the IP addresses","C":"Use AWS inspector to block the IP addresses","D":"Use AWS WAF to block the IP addresses"},"answer":"D","explanation":"The AWS Documentation mentions the following on AWS WAF which can be used to protect Application Load Balancers and Cloud front A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distributions or Application Load Balancers respond to. You can allow or block the following types of requests: Originate from an IP address or a range of IP addresses Originate from a specific country or countries Contain a specified string or match a regular expression (regex) pattern in a particular part of requests Exceed a specified length Appear to contain malicious SQL code (known as SQL injection) Appear to contain malicious scripts (known as cross-site scripting) Option A is invalid because by default Security Groups have the Deny policy Options B and C are invalid because these services cannot be used to block IP addresses For information on AWS WAF, please visit the below URL: https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html The correct answer is: Use AWS WAF to block the IP addresses"},{"question":"You need to establish a secure backup and archiving solution for your company, using AWS. Documents should be immediately accessible for three months and available for five years for compliance reasons. Which AWS service fulfills these requirements in the most cost-effective way? Choose the correct answer","choices":{"A":"Upload data to S3 and use lifecycle policies to move the data into Glacier for long-term archiving.","B":"Upload the data on EBS, use lifecycle policies to move EBS snapshots into S3 and later into Glacier for long-term archiving.","C":"Use Direct Connect to upload data to S3 and use IAM policies to move the data into Glacier for long-term archiving.","D":"Use Storage Gateway to store data to S3 and use lifecycle policies to move the data into Redshift for long-term archiving."},"answer":"A","explanation":"amazon Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup. Customers can reliably store large or small amounts of data for as little as $0,004 per gigabyte per month, a significant savings compared to on-premises solutions. With Amazon lifecycle policies you can create transition actions in which you define when objects transition to another Amazon S3 storage class. For example, you may choose to transition objects to the STANDARDJA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation. Option B is invalid because lifecycle policies are not available for EBS volumes Option C is invalid because IAM policies cannot be used to move data to Glacier Option D is invalid because lifecycle policies is not used to move data to Redshif For more information on S3 lifecycle policies, please visit the URL: http://docs.aws.amazon.com/AmazonS3/latest/dev/obiect-lifecycle-mgmt.html The correct answer is: Upload data to S3 and use lifecycle policies to move the data into Glacier for long-term archiving."},{"question":"Your company is planning on developing an application in AWS. This is a web based application. The application user will use their facebook or google identities for authentication. You want to have the ability to manage user profiles without having to add extra coding to manage this. Which of the below would assist in this.","choices":{"A":"Create an OlDC identity provider in AWS","B":"Create a SAML provider in AWS","C":"Use AWS Cognito to manage the user profiles","D":"Use IAM users to manage the user profiles"},"answer":"C","explanation":"The AWS Documentation mentions the following A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Facebook or Amazon, and through SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. User pools provide: Sign-up and sign-in services. A built-in, customizable web Ul to sign in users. Social sign-in with Facebook, Google, and Login with Amazon, as well as sign-in with SAML identity providers from your user pool. User directory management and user profiles. Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification. Customized workflows and user migration through AWS Lambda triggers. Options A and B are invalid because these are not used to manage users Option D is invalid because this would be a maintenance overhead For more information on Cognito User Identity pools, please refer to the below Link: https://docs.aws.amazon.com/coenito/latest/developerguide/cognito-user- identity-pools.html The correct answer is: Use AWS Cognito to manage the user profiles"},{"question":"Your company has defined a set of S3 buckets in AWS. They need to monitor the S3 buckets and know the source IP address and the person who make requests to the S3 bucket. How can this be achieved?","choices":{"A":"Enable VPC flow logs to know the source IP addresses","B":"Monitor the S3 API calls by using Cloudtrail logging","C":"Monitor the S3 API calls by using Cloudwatch logging","D":"Enable AWS Inspector for the S3 bucket"},"answer":"B","explanation":"The AWS Documentation mentions the following Amazon S3 is integrated with AWS CloudTrail. CloudTrail is a service that captures specific API calls made to Amazon S3 from your AWS account and delivers the log files to an Amazon S3 bucket that you specify. It captures API calls made from the Amazon S3 console or from the Amazon S3 API. Using the information collected by CloudTrail, you can determine what request was made to Amazon S3, the source IP address from which the request was made, who made the request when it was made, and so on Options A,C and D are invalid because these services cannot be used to get the source IP address of the calls to S3 buckets For more information on Cloudtrail logging, please refer to the below Link: https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logeins.htmll The correct answer is: Monitor the S3 API calls by using Cloudtrail logging"},{"question":"A company has set up EC2 instances on the AW5 Cloud. There is a need to see all the IP addresses which are accessing the EC2 Instances. Which service can help achieve this?","choices":{"A":"Use the AWS Inspector service","B":"Use AWS VPC Flow Logs","C":"Use Network ACL's","D":"Use Security Groups"},"answer":"B","explanation":"The AWS Documentation mentions the foil A flow log record represents a network flow in your flow log. Each record captures the network flow for a specific 5-tuple, for a specific capture window. A 5-tuple is a set of five different values that specify the source, destination, and protocol for an internet protocol (IP) flow. Options A,C and D are all invalid because these services/tools cannot be used to get the the IP addresses which are accessing the EC2 Instances For more information on VPC Flow Logs please visit the URL https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html The correct answer is: Use AWS VPC Flow Logs Submit vour Feedback/Queries to our Experts"},{"question":"An enterprise wants to use a third-party SaaS application. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise's account. The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege and there must be controls in place to ensure that the credentials used by the SaaS vendor cannot be used by any other third party. Which of the following would meet all of these conditions?","choices":{"A":"From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account.","B":"Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application.","C":"Create a new access and secret key for the user and provide these credentials to the SaaS provider.","D":"Create an IAM role for cross-account access allows the SaaS provider's account to assume the role and assign it a policy that allows only the actions required by the SaaS application.","E":"Create an IAM role for EC2 instances, assign it a policy that allows only the actions required tor the Saas application to work, provide the role ARN to the SaaS provider to use when launching their application instances."},"answer":"C","explanation":"The below diagram from an AWS blog shows how access is given to other accounts for the services in your own account Options A and B are invalid because you should not user IAM users or IAM Access keys Options D is invalid because you need to create a role for cross account access For more information on Allowing access to external accounts, please visit the below URL: |https://aws.amazon.com/blogs/apn/how-to-best-architect-your-aws-marketplace-saassubscription- across-multiple-aws-accounts; The correct answer is: Create an IAM role for cross-account access allows the SaaS provider's account to assume the role and assign it a policy that allows only the actions required by the SaaS application."},{"question":"Your company has a set of resources defined in the AWS Cloud. Their IT audit department has requested to get a list of resources that have been defined across the account. How can this be achieved in the easiest manner?","choices":{"A":"Create a powershell script using the AWS CL","B":"Query for all resources with the tag of production.","C":"Create a bash shell script with the AWS CL","D":"Query for all resources in all region","E":"Store the results in an S3 bucket.","F":"Use Cloud Trail to get the list of all resources","G":"Use AWS Config to get the list of all resources"},"answer":"D","explanation":"The most feasible option is to use AWS Config. When you turn on AWS Config, you will get a list of resources defined in your AWS Account. A sample snapshot of the resources dashboard in AWS Config is shown below Option A is incorrect because this would give the list of production based resources and now all resources Option B is partially correct But this will just add more maintenance overhead. Option C is incorrect because this can be used to log API activities but not give an account of all resou For more information on AWS Config, please visit the below URL: https://docs.aws.amazon.com/config/latest/developereuide/how-does-confie-work.html The correct answer is: Use AWS Config to get the list of all resources"},{"question":"A Lambda function reads metadata from an S3 object and stores the metadata in a DynamoDB table. The function is triggered whenever an object is stored within the S3 bucket. How should the Lambda function be given access to the DynamoDB table?","choices":{"A":"Create a VPC endpoint for DynamoDB within a VP","B":"Configure the Lambda function to access resources in the VPC.","C":"Create a resource policy that grants the Lambda function permissions to write to the DynamoDB tabl","D":"Attach the poll to the DynamoDB table.","E":"Create an IAM user with permissions to write to the DynamoDB tabl","F":"Store an access key for that user in the Lambda environment variables.","G":"Create an IAM service role with permissions to write to the DynamoDB tabl","H":"Associate that role with the Lambda function."},"answer":"D","explanation":"The ideal way is to create an IAM role which has the required permissions and then associate it with the Lambda function The AWS Documentation additionally mentions the following Each Lambda function has an IAM role (execution role) associated with it. You specify the IAM role when you create your Lambda function. Permissions you grant to this role determine what AWS Lambda can do when it assumes the role. There are two types of permissions that you grant to the IAM role: If your Lambda function code accesses other AWS resources, such as to read an object from an S3 bucket or write logs to CloudWatch Logs, you need to grant permissions for relevant Amazon S3 and CloudWatch actions to the role. If the event source is stream-based (Amazon Kinesis Data Streams and DynamoDB streams), AWS Lambda polls these streams on your behalf. AWS Lambda needs permissions to poll the stream and read new records on the stream so you need to grant the relevant permissions to this role. Option A is invalid because the VPC endpoint allows access instances in a private subnet to access DynamoDB Option B is invalid because resources policies are present for resources such as S3 and KMS, but not AWS Lambda Option C is invalid because AWS Roles should be used and not IAM Users For more information on the Lambda permission model, please visit the below URL: https://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html The correct answer is: Create an IAM service role with permissions to write to the DynamoDB table. Associate that role with the Lambda function."},{"question":"You have an S3 bucket hosted in AWS. This is used to host promotional videos uploaded by yourself. You need to provide access to users for a limited duration of time. How can this be achieved?","choices":{"A":"Use versioning and enable a timestamp for each version","B":"Use Pre-signed URL's","C":"Use IAM Roles with a timestamp to limit the access","D":"Use IAM policies with a timestamp to limit the access"},"answer":"B","explanation":"The AWS Documentation mentions the following All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL using their own security credentials, to grant time-limited permission to download the objects. Option A is invalid because this can be used to prevent accidental deletion of objects Option C is invalid because timestamps are not possible for Roles Option D is invalid because policies is not the right way to limit access based on time For more information on pre-signed URL's, please visit the URL: https://docs.aws.ama2on.com/AmazonS3/latest/dev/ShareObiectPreSisnedURL.html The correct answer is: Use Pre-signed URL's"},{"question":"A company hosts a critical web application on the AWS Cloud. This is a key revenue generating application for the company. The IT Security team is worried about potential DDos attacks against the web site. The senior management has also specified that immediate action needs to be taken in case of a potential DDos attack. What should be done in this regard?","choices":{"A":"Consider using the AWS Shield Service","B":"Consider using VPC Flow logs to monitor traffic for DDos attack and quickly take actions on a trigger of a potential attack.","C":"Consider using the AWS Shield Advanced Service","D":"Consider using Cloudwatch logs to monitor traffic for DDos attack and quickly take actions on a trigger of a potential attack."},"answer":"C","explanation":"Option A is invalid because the normal AWS Shield Service will not help in immediate action against a DDos attack. This can be done via the AWS Shield Advanced Service Option B is invalid because this is a logging service for VPCs traffic flow but cannot specifically protect against DDos attacks. Option D is invalid because this is a logging service for AWS Services but cannot specifically protect against DDos attacks. The AWS Documentation mentions the following AWS Shield Advanced provides enhanced protections for your applications running on Amazon EC2. Elastic Load Balancing (ELB), Amazon CloudFront and Route 53 against larger and more sophisticated attacks. AWS Shield Advanced is available to AWS Business Support and AWS Enterprise Support customers. AWS Shield Advanced protection provides always-on, flow-based monitoring of network traffic and active application monitoring to provide near real-time notifications of DDoS attacks. AWS Shield Advanced also gives customers highly filexible controls over attack mitigations to take actions instantly. Customers can also engage the DDoS Response Team (DRT) 24X7 to manage and mitigate their application layer DDoS attacks. For more information on AWS Shield, please visit the below URL: https://aws.amazon.com/shield/faqs; The correct answer is: Consider using the AWS Shield Advanced Service"},{"question":"A company has a set of resources defined in AWS. It is mandated that all API calls to the resources be monitored. Also all API calls must be stored for lookup purposes. Any log data greater than 6 months must be archived. Which of the following meets these requirements? Choose 2 answers from the options given below. Each answer forms part of the solution.","choices":{"A":"Enable CloudTrail logging in all accounts into S3 buckets","B":"Enable CloudTrail logging in all accounts into Amazon Glacier","C":"Ensure a lifecycle policy is defined on the S3 bucket to move the data to EBS volumes after 6 months.","D":"Ensure a lifecycle policy is defined on the S3 bucket to move the data to Amazon Glacier after 6 months."},"answer":"AD","explanation":"Cloudtrail publishes the trail of API logs to an S3 bucket Option B is invalid because you cannot put the logs into Glacier from CloudTrail Option C is invalid because lifecycle policies cannot be used to move data to EBS volumes For more information on Cloudtrail logging, please visit the below URL: https://docs.aws.amazon.com/awscloudtrail/latest/usereuide/cloudtrail-find-log-files.htmll You can then use Lifecycle policies to transfer data to Amazon Glacier after 6 months For more information on S3 lifecycle policies, please visit the below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html The correct answers are: Enable CloudTrail logging in all accounts into S3 buckets. Ensure a lifecycle policy is defined on the bucket to move the data to Amazon Glacier after 6 months."},{"question":"You want to launch an EC2 Instance with your own key pair in AWS. How can you achieve this? Choose 3 answers from the options given below.","choices":{"A":"Use a third party tool to create the Key pair","B":"Create a new key pair using the AWS CLI","C":"Import the public key into EC2","D":"Import the private key into EC2"},"answer":"ABC","explanation":"This is given in the AWS Documentation Creating a Key Pair You can use Amazon EC2 to create your key pair. For more information, see Creating a Key Pair Using Amazon EC2. Alternatively, you could use a third-party tool and then import the public key to Amazon EC2. For more information, see Importing Your Own Public Key to Amazon EC2. Option B is Correct, because you can use the AWS CLI to create a new key pair 1 https://docs.aws.amazon.com/cli/latest/userguide/cli-ec2-keypairs.html Option D is invalid because the public key needs to be stored in the EC2 Instance For more information on EC2 Key pairs, please visit the below URL: * https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs The correct answers are: Use a third party tool to create the Key pair. Create a new key pair using the AWS CLI, Import the public key into EC2"},{"question":"Your company makes use of S3 buckets for storing data. There is a company policy that all services should have logging enable. How can you ensure thatlogging is always enabled for created S3 buckets in the AWS Account?","choices":{"A":"Use AWS Inspector to inspect all S3 buckets and enable logging for those where it is not enabled","B":"Use AWS Config Rules to check whether logging is enabled for buckets","C":"Use AWS Cloudwatch metrics to check whether logging is enabled for buckets","D":"Use AWS Cloudwatch logs to check whether logging is enabled for buckets"},"answer":"B","explanation":"This is given in the AWS Documentation as an example rule in AWS Config Example rules with triggers Example rule with configuration change trigger 1. You add the AWS Config managed rule, S3_BUCKET_LOGGING_ENABLED, to your account to check whether your Amazon S3 buckets have logging enabled. 2. The trigger type for the rule is configuration changes. AWS Config runs the evaluations for the rule when an Amazon S3 bucket is created, changed, or deleted. 3. When a bucket is updated, the configuration change triggers the rule and AWS Config evaluates whether the bucket is compliant against the rule. Option A is invalid because AWS Inspector cannot be used to scan all buckets Option C and D are invalid because Cloudwatch cannot be used to check for logging enablement for buckets. For more information on Config Rules please see the below Link: https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config-rules.html The correct answer is: Use AWS Config Rules to check whether logging is enabled for buckets"},{"question":"A company has external vendors that must deliver files to the company. These vendors have crossaccount that gives them permission to upload objects to one of the company's S3 buckets. What combination of steps must the vendor follow to successfully deliver a file to the company? Select 2 answers from the options given below","choices":{"A":"Attach an IAM role to the bucket that grants the bucket owner full permissions to the object","B":"Add a grant to the objects ACL giving full permissions to bucket owner.","C":"Encrypt the object with a KMS key controlled by the company.","D":"Add a bucket policy to the bucket that grants the bucket owner full permissions to the object","E":"Upload the file to the company's S3 bucket"},"answer":"BE","explanation":"This scenario is given in the AWS Documentation A bucket owner can enable other AWS accounts to upload objects. These objects are owned by the accounts that created them. The bucket owner does not own objects that were not created by the bucket owner. Therefore, for the bucket owner to grant access to these objects, the object owner must first grant permission to the bucket owner using an object ACL. The bucket owner can then delegate those permissions via a bucket policy. In this example, the bucket owner delegates permission to users in its own account. Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) Option A and D are invalid because bucket ACL's are used to give grants to bucket Option C is not required since encryption is not part of the requirement For more information on this scenario please see the below Link: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroushs-manaeing-accessexample3. htmll The correct answers are: Add a grant to the objects ACL giving full permissions to bucket owner., Upload the file to the company's S3 bucket"},{"question":"Your company manages thousands of EC2 Instances. There is a mandate to ensure that all servers don't have any critical security flaws. Which of the following can be done to ensure this? Choose 2 answers from the options given below.","choices":{"A":"Use AWS Config to ensure that the servers have no critical flaws.","B":"Use AWS inspector to ensure that the servers have no critical flaws.","C":"Use AWS inspector to patch the servers","D":"Use AWS SSM to patch the servers"},"answer":"BD","explanation":"The AWS Documentation mentions the following on AWS Inspector Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for vulnerabilities or deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports which are available via the Amazon Inspector console or API. Option A is invalid because the AWS Config service is not used to check the vulnerabilities on servers Option C is invalid because the AWS Inspector service is not used to patch servers For more information on AWS Inspector, please visit the following URL: https://aws.amazon.com/inspector> Once you understand the list of servers which require critical updates, you can rectify them by installing the required patches via the SSM tool. For more information on the Systems Manager, please visit the following URL: https://docs.aws.amazon.com/systems-manager/latest/APIReference/Welcome.html The correct answers are: Use AWS Inspector to ensure that the servers have no critical flaws.. Use AWS SSM to patch the servers ("},{"question":"You are working for a company and been allocated the task for ensuring that there is a federated authentication mechanism setup between AWS and their On- premise Active Directory. Which of the following are important steps that need to be covered in this process? Choose 2 answers from the options given below.","choices":{"A":"Ensure the right match is in place for On-premise AD Groups and IAM Roles.","B":"Ensure the right match is in place for On-premise AD Groups and IAM Groups.","C":"Configure AWS as the relying party in Active Directory","D":"Configure AWS as the relying party in Active Directory Federation services"},"answer":"AD","explanation":"The AWS Documentation mentions some key aspects with regards to the configuration of Onpremise AD with AWS One is the Groups configuration in AD Active Directory Configuration Determining how you will create and delineate your AD groups and IAM roles in AWS is crucial to how you secure access to your account and manage resources. SAML assertions to the AWS environment and the respective IAM role access will be managed through regular expression (regex) matching between your on- premises AD group name to an AWS IAM role. One approach for creating the AD groups that uniquely identify the AWS IAM role mapping is by selecting a common group naming convention. For example, your AD groups would start with an identifier, for example, AWS-, as this will distinguish your AWS groups from others within the organization. Next include the 12-digitAWS account number. Finally, add the matching role name within the AWS account. Here is an example: Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) And next is the configuration of the relying party which is AWS ADFS federation occurs with the participation of two parties; the identity or claims provider (in this case the owner of the identity repository - Active Directory) and the relying party, which is another application that wishes to outsource authentication to the identity provider; in this case Amazon Secure Token Service (STS). The relying party is a federation partner that is represented by a claims provider trust in the federation service. Option B is invalid because AD groups should not be matched to IAM Groups Option C is invalid because the relying party should be configured in Active Directory Federation services For more information on the federated access, please visit the following URL: 1 https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directoryfederation- services-ad-fs/ The correct answers are: Ensure the right match is in place for On-premise AD Groups and IAM Roles., Configure AWS as the relying party in Active Directory Federation services"},{"question":"You work as an administrator for a company. The company hosts a number of resources using AWS. There is an incident of a suspicious API activity which occurred 11 days ago. The Security Admin has asked to get the API activity from that point in time. How can this be achieved?","choices":{"A":"Search the Cloud Watch logs to find for the suspicious activity which occurred 11 days ago","B":"Search the Cloudtrail event history on the API events which occurred 11 days ago.","C":"Search the Cloud Watch metrics to find for the suspicious activity which occurred 11 days ago","D":"Use AWS Config to get the API calls which were made 11 days ag"},"answer":"B","explanation":"The Cloud Trail event history allows to view events which are recorded for 90 days. So one can use a metric filter to gather the API calls from 11 days ago. Option A and C is invalid because Cloudwatch is used for logging and not for monitoring API activity Option D is invalid because AWSConfig is a configuration service and not for monitoring API activity For more information on AWS Cloudtrail, please visit the following URL: https://docs.aws.amazon.com/awscloudtrail/latest/usereuide/how-cloudtrail-works.html Note: In this question we assume that the customer has enabled cloud trail service. AWS CloudTrail is enabled by default for ALL CUSTOMERS and will provide visibility into the past seven days of account activity without the need for you to configure a trail in the service to get started. So for an activity that happened 11 days ago to be stored in the cloud trail we need to configure the trail manually to ensure that it is stored in the events history.  https://aws.amazon.com/blogs/aws/new-amazon-web-services-extends-cloudtrail-to-all-awscustomers/ The correct answer is: Search the Cloudtrail event history on the API events which occurred 11 days ago."},{"question":"You are planning on hosting a web application on AWS. You create an EC2 Instance in a public subnet. This instance needs to connect to an EC2 Instance that will host an Oracle database. Which of the following steps should be followed to ensure a secure setup is in place? Select 2 answers.","choices":{"A":"Place the EC2 Instance with the Oracle database in the same public subnet as the Web server for faster communication","B":"Place the EC2 Instance with the Oracle database in a separate private subnet","C":"Create a database security group and ensure the web security group to allowed incoming access","D":"Ensure the database security group allows incoming traffic from 0.0.0.0/0"},"answer":"BC","explanation":"The best secure option is to place the database in a private subnet. The below diagram from the AWS Documentation shows this setup. Also ensure that access is not allowed from all sources but just from the web servers. Option A is invalid because databases should not be placed in the public subnet Option D is invalid because the database security group should not allow traffic from the internet For more information on this type of setup, please refer to the below URL: https://docs.aws.amazon.com/AmazonVPC/latest/UserGuideA/PC Scenario2. The correct answers are: Place the EC2 Instance with the Oracle database in a separate private subnet Create a database security group and ensure the web security group to allowed incoming access"},{"question":"A company has set up the following structure to ensure that their S3 buckets always have logging enabled If there are any changes to the configuration to an S3 bucket, a config rule gets checked. If logging is disabled , then Lambda function is invoked. This Lambda function will again enable logging on the S3 bucket. Now there is an issue being encoutered with the entire flow. You have verified that the Lambda function is being invoked. But when logging is disabled for the bucket, the lambda function does not enable it again. Which of the following could be an issue","choices":{"A":"The AWS Config rule is not configured properly","B":"The AWS Lambda function does not have appropriate permissions for the bucket","C":"The AWS Lambda function should use Node.js instead of python.","D":"You need to also use the API gateway to invoke the lambda function"},"answer":"B","explanation":"The most probable cause is that you have not allowed the Lambda functions to have the appropriate permissions on the S3 bucket to make the relevant changes. Option A is invalid because this is more of a permission instead of a configuration rule issue. Option C is invalid because changing the language will not be the core solution. Option D is invalid because you don't necessarily need to use the API gateway service For more information on accessing resources from a Lambda function, please refer to below URL https://docs.aws.amazon.com/lambda/latest/ds/accessing- resources.htmll The correct answer is: The AWS Lambda function does not have appropriate permissions for the bucket"},{"question":"Your team is designing a web application. The users for this web application would need to sign in via an external ID provider such asfacebook or Google. Which of the following AWS service would you use for authentication?","choices":{"A":"AWS Cognito","B":"AWS SAML","C":"AWS IAM","D":"AWS Config"},"answer":"A","explanation":"The AWS Documentation mentions the following Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users ca sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google. Option B is incorrect since this is used for identity federation Option C is incorrect since this is pure Identity and Access management Option D is incorrect since AWS is a configuration service For more information on AWS Cognito please refer to the below Link: https://docs.aws.amazon.com/coenito/latest/developerguide/what-is-amazon-cognito.html The correct answer is: AWS Cognito"},{"question":"You are designing a connectivity solution between on-premises infrastructure and Amazon VPC. Your server's on-premises will be communicating with your VPC instances. You will be establishing IPSec tunnels over the internet. Yo will be using VPN gateways and terminating the IPsec tunnels on AWSsupported customer gateways. Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above? Choose 4 answers form the options below","choices":{"A":"End-to-end protection of data in transit","B":"End-to-end Identity authentication","C":"Data encryption across the internet","D":"Protection of data in transit over the Internet","E":"Peer identity authentication between VPN gateway and customer gateway","F":"Data integrity protection across the Internet"},"answer":"CDEF","explanation":"Since the Web server needs to talk to the database server on port 3306 that means that the database server should allow incoming traffic on port 3306. The below table from the aws documentation shows how the security groups should be set up. Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) Option B is invalid because you need to allow incoming access for the database server from the WebSecGrp security group. Options C and D are invalid because you need to allow Outbound traffic and not inbound traffic For more information on security groups please visit the below Link: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC Scenario2.html The correct answer is: Allow Inbound on port 3306 for Source Web Server Security Group WebSecGrp."},{"question":"Your developer is using the KMS service and an assigned key in their Java program. They get the below error when running the code arn:aws:iam::113745388712:user/UserB is not authorized to perform: kms:DescribeKey Which of the following could help resolve the issue?","choices":{"A":"Ensure that UserB is given the right IAM role to access the key","B":"Ensure that UserB is given the right permissions in the IAM policy","C":"Ensure that UserB is given the right permissions in the Key policy","D":"Ensure that UserB is given the right permissions in the Bucket policy"},"answer":"C","explanation":"You need to ensure that UserB is given access via the Key policy for the Key Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) Option is invalid because you don't assign roles to IAM users For more information on Key policies please visit the below Link: https://docs.aws.amazon.com/kms/latest/developerguide/key-poli The correct answer is: Ensure that UserB is given the right permissions in the Key policy"},{"question":"An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) outside. How can it achieve this?","choices":{"A":"Create an IAM policy with the security group and use that security group for AWS console login","B":"Create an IAM policy with a condition which denies access when the IP address range is not from the organization","C":"Configure the EC2 instance security group which allows traffic only from the organization's IP range","D":"Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console"},"answer":"B","explanation":"You can actually use a Deny condition which will not allow the person to log in from outside. The below example shows the Deny condition to ensure that any address specified in the source address is not allowed to access the resources in aws. Option A is invalid because you don't mention the security group in the IAM policy Option C is invalid because security groups by default don't allow traffic Option D is invalid because the IAM policy does not have such an option For more information on IAM policy conditions, please visit the URL: http://docs.aws.amazon.com/IAM/latest/UserGuide/access pol examples.htm l#iam-policy-example-ec2-two-condition! The correct answer is: Create an IAM policy with a condition which denies access when the IP address range is not from the organization"},{"question":"You are creating a Lambda function which will be triggered by a Cloudwatch Event. The data from these events needs to be stored in a DynamoDB table. How should the Lambda function be given access to the DynamoDB table?","choices":{"A":"Put the AWS Access keys in the Lambda function since the Lambda function by default is secure","B":"Use an IAM role which has permissions to the DynamoDB table and attach it to the Lambda function.","C":"Use the AWS Access keys which has access to DynamoDB and then place it in an S3 bucket.","D":"Create a VPC endpoint for the DynamoDB tabl","E":"Access the VPC endpoint from the Lambda function."},"answer":"B","explanation":"AWS Lambda functions uses roles to interact with other AWS services. So use an IAM role which has permissions to the DynamoDB table and attach it to the Lambda function. Options A and C are all invalid because you should never use AWS keys for access. Option D is invalid because the VPC endpoint is used for VPCs For more information on Lambda function Permission model, please visit the URL https://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html The correct answer is: Use an IAM role which has permissions to the DynamoDB table and attach it to the Lambda function."},{"question":"What is the result of the following bucket policy? Choose the correct answer","choices":{"A":"It will allow all access to the bucket mybucket","B":"It will allow the user mark from AWS account number 111111111 all access to the bucket but deny everyone else all access to the bucket","C":"It will deny all access to the bucket mybucket","D":"None of these"},"answer":"C","explanation":"The policy consists of 2 statements, one is the allow for the user mark to the bucket and the next is the deny policy for all other users. The deny permission will override the allow and hence all users Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) will not have access to the bucket. Options A,B and D are all invalid because this policy is used to deny all access to the bucket mybucket For examples on S3 bucket policies, please refer to the below Link: http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.htmll The correct answer is: It will deny all access to the bucket mybucket Submit your FeedbacK/Quenes to our Experts"},{"question":"You have an Amazon VPC that has a private subnet and a public subnet in which you have a NAT instance server. You have created a group of EC2 instances that configure themselves at startup by downloading a bootstrapping script from S3 that deploys an application via GIT. Which one of the following setups would give us the highest level of security? Choose the correct answer from the options given below.","choices":{"A":"EC2 instances in our public subnet, no EIPs, route outgoing traffic via the IGW","B":"EC2 instances in our public subnet, assigned EIPs, and route outgoing traffic via the NAT","C":"EC2 instance in our private subnet, assigned EIPs, and route our outgoing traffic via our IGW","D":"EC2 instances in our private subnet, no EIPs, route outgoing traffic via the NAT"},"answer":"D","explanation":"The below diagram shows how the NAT instance works. To make EC2 instances very secure, they need to be in a private sub such as the database server shown below with no EIP and all traffic routed via the NAT. Options A and B are invalid because the instances need to be in the private subnet Option C is invalid because since the instance needs to be in the private subnet, you should not attach an EIP to the instance For more information on NAT instance, please refer to the below Link: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuideA/PC lnstance.html! Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) The correct answer is: EC2 instances in our private subnet no EIPs, route outgoing traffic via the NAT"},{"question":"In your LAMP application, you have some developers that say they would like access to your logs. However, since you are using an AWS Auto Scaling group, your instances are constantly being recreated. What would you do to make sure that these developers can access these log files? Choose the correct answer from the options below","choices":{"A":"Give only the necessary access to the Apache servers so that the developers can gain access to thelog files.","B":"Give root access to your Apache servers to the developers.","C":"Give read-only access to your developers to the Apache servers.","D":"Set up a central logging server that you can use to archive your logs; archive these logs to an S3 bucket for developer-access."},"answer":"D","explanation":"One important security aspect is to never give access to actual servers, hence Option A.B and C are just totally wrong from a security perspective. The best option is to have a central logging server that can be used to archive logs. These logs can then be stored in S3. Options A,B and C are all invalid because you should not give access to the developers on the Apache se For more information on S3, please refer to the below link https://aws.amazon.com/documentation/s3j The correct answer is: Set up a central logging server that you can use to archive your logs; archive these logs to an S3 bucket for developer-access. Submit vour Feedback/Queries to our Experts"},{"question":"Your company is planning on developing an application in AWS. This is a web based application. The application users will use their facebook or google identities for authentication. You want to have the ability to manage user profiles without having to add extra coding to manage this. Which of the below would assist in this.","choices":{"A":"Create an OlDC identity provider in AWS","B":"Create a SAML provider in AWS","C":"Use AWS Cognito to manage the user profiles","D":"Use IAM users to manage the user profiles"},"answer":"B","explanation":"The AWS Documentation mentions the following The AWS Documentation mentions the following OIDC identity providers are entities in IAM that describe an identity provider (IdP) service that supports the OpenID Connect (OIDC) standard. You use an OIDC identity provider when you want to establish trust between an OlDC-compatible IdPsuch as Google, Salesforce, and many othersand your AWS account This is useful if you are creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities Option A is invalid because in the security groups you would not mention this information/ Option C is invalid because SAML is used for federated authentication Option D is invalid because you need to use the OIDC identity provider in AWS For more information on ODIC identity providers, please refer to the below Link: https://docs.aws.amazon.com/IAM/latest/UserGuide/id roles providers create oidc.htmll The correct answer is: Create an OIDC identity provider in AWS"},{"question":"Your company has many AWS accounts defined and all are managed via AWS Organizations. One AWS account has a S3 bucket that has critical data. How can we ensure that all the users in the AWS organisation have access to this bucket?","choices":{"A":"Ensure the bucket policy has a condition which involves aws:PrincipalOrglD","B":"Ensure the bucket policy has a condition which involves aws:AccountNumber","C":"Ensure the bucket policy has a condition which involves aws:PrincipaliD","D":"Ensure the bucket policy has a condition which involves aws:OrglD"},"answer":"A","explanation":"The AWS Documentation mentions the following AWS Identity and Access Management (IAM) now makes it easier for you to control access to your AWS resources by using the AWS organization of IAM principals (users and roles). For some services, you grant permissions using resource-based policies to specify the accounts and principals that can access the resource and what actions they can perform on it. Now, you can use a new condition key, aws:PrincipalOrglD, in these policies to require all principals accessing the resource to be from an account in the organization Option B.C and D are invalid because the condition in the bucket policy has to mention aws:PrincipalOrglD For more information on controlling access via Organizations, please refer to the below Link: https://aws.amazon.com/blogs/security/control-access-to-aws- resources-by-usins-the-awsorganization- of-iam-principal ( The correct answer is: Ensure the bucket policy has a condition which involves aws:PrincipalOrglD"},{"question":"Your team is experimenting with the API gateway service for an application. There is a need to implement a custom module which can be used for authentication/authorization for calls made to the API gateway. How can this be achieved?","choices":{"A":"Use the request parameters for authorization","B":"Use a Lambda authorizer","C":"Use the gateway authorizer","D":"Use CORS on the API gateway"},"answer":"B","explanation":"The AWS Documentation mentions the following An Amazon API Gateway Lambda authorizer (formerly known as a custom authorize?) is a Lambda function that you provide to control access to your API methods. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. It can also use information described by headers, paths, query strings, stage variables, or context variables request parameters. Options A,C and D are invalid because these cannot be used if you need a custom authentication/authorization for calls made to the API gateway For more information on using the API gateway Lambda authorizer please visit the URL: https://docs.aws.amazon.com/apisateway/latest/developerguide/apieateway-use-lambdaauthorizer. htmll The correct answer is: Use a Lambda authorizer"},{"question":"You have a set of 100 EC2 Instances in an AWS account. You need to ensure that all of these instances are patched and kept to date. All of the instances are in a private subnet. How can you achieve this. Choose 2 answers from the options given below","choices":{"A":"Ensure a NAT gateway is present to download the updates","B":"Use the Systems Manager to patch the instances","C":"Ensure an internet gateway is present to download the updates","D":"Use the AWS inspector to patch the updates"},"answer":"AB","explanation":"Option C is invalid because the instances need to remain in the private: Option D is invalid because AWS inspector can only detect the patches One of the AWS Blogs mentions how patching of Linux servers can be accomplished. Below is the diagram representation of the architecture setup For more information on patching Linux workloads in AWS, please refer to the Lin. https://aws.amazon.com/blogs/security/how-to-patch-linux-workloads-on-awsj The correct answers are: Ensure a NAT gateway is present to download the updates. Use the Systems Manager to patch the instances"},{"question":"You have an EC2 instance with the following security configured: 1. ICMP inbound allowed on Security Group 2. ICMP outbound not configured on Security Group 3. ICMP inbound allowed on Network ACL 4. ICMP outbound denied on Network ACL If Flow logs is enabled for the instance, which of the following flow records will be recorded? Choose 3 answers from the options give below","choices":{"A":"An ACCEPT record for the request based on the Security Group","B":"An ACCEPT record for the request based on the NACL","C":"A REJECT record for the response based on the Security Group","D":"A REJECT record for the response based on the NACL"},"answer":"ABD","explanation":"This example is given in the AWS documentation as well For example, you use the ping command from your home computer (IP address is 203.0.113.12) to your instance (the network interface's private IP address is 172.31.16.139). Your security group's inbound rules allow ICMP traffic and the outbound rules do not allow ICMP traffic however, because security groups are stateful, the response ping from your instance is allowed. Your network ACL permits inbound ICMP traffic but does not permit outbound ICMP traffic. Because network ACLs are stateless, the response ping is dropped and will not reach your home computer. In a flow log, this is displayed as 2 flow log records: An ACCEPT record for the originating ping that was allowed by both the network ACL and the security group, and therefore was allowed to reach your instance. A REJECT record for the response ping that the network ACL denied. Option C is invalid because the REJECT record would not be present For more information on Flow Logs, please refer to the below URL: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-loes.html The correct answers are: An ACCEPT record for the request based on the Security Group, An ACCEPT record for the request based on the NACL, A REJECT record for the response based on the NACL Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As)"},{"question":"Your company is planning on using bastion hosts for administering the servers in AWS. Which of the following is the best description of a bastion host from a security perspective?","choices":{"A":"A Bastion host should be on a private subnet and never a public subnet due to security concerns","B":"A Bastion host sits on the outside of an internal network and is used as a gateway into the private network and is considered the critical strong point of the","C":"Bastion hosts allow users to log in using RDP or SSH and use that session to S5H into internal network to access private subnet resources.","D":"A Bastion host should maintain extremely tight security and monitoring as it is available to the public"},"answer":"C","explanation":"A bastion host is a special purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example a proxy server, and all other services are removed or limited to reduce the threat to the computer. In AWS, A bastion host is kept on a public subnet. Users log on to the bastion host via SSH or RDP and then use that session to manage other hosts in the private subnets. Options A and B are invalid because the bastion host needs to sit on the public network. Option D is invalid because bastion hosts are not used for monitoring For more information on bastion hosts, just browse to the below URL: https://docsaws.amazon.com/quickstart/latest/linux-bastion/architecture.htl The correct answer is: Bastion hosts allow users to log in using RDP or SSH and use that session to SSH into internal network to access private subnet resources."},{"question":"Your CTO is very worried about the security of your AWS account. How best can you prevent hackers from completely hijacking your account?","choices":{"A":"Use short but complex password on the root account and any administrators.","B":"Use AWS IAM Geo-Lock and disallow anyone from logging in except for in your city.","C":"Use MFA on all users and accounts, especially on the root account.","D":"Don't write down or remember the root account password after creating the AWS accoun"},"answer":"C","explanation":"Multi-factor authentication can add one more layer of security to your AWS account Even when you go to your Security Credentials dashboard one of the items is to enable MFA on your root account Option A is invalid because you need to have a good password policy Option B is invalid because there is no IAM Geo-Lock Option D is invalid because this is not a recommended practices For more information on MFA, please visit the below URL http://docs.aws.amazon.com/IAM/latest/UserGuide/id credentials mfa.htmll The correct answer is: Use MFA on all users and accounts, especially on the root account."},{"question":"A company hosts a popular web application that connects to an Amazon RDS MySQL DB instance running in a private VPC subnet that was created with default ACL settings. The IT Security department has a suspicion that a DDos attack is coming from a suspecting IP. How can you protect the subnets from this attack?","choices":{"A":"Change the Inbound Security Groups to deny access from the suspecting IP","B":"Change the Outbound Security Groups to deny access from the suspecting IP","C":"Change the Inbound NACL to deny access from the suspecting IP","D":"Change the Outbound NACL to deny access from the suspecting IP"},"answer":"C","explanation":"Option A and B are invalid because by default the Security Groups already block traffic. You can use NACL's as an additional security layer for the subnet to deny traffic. Option D is invalid since just changing the Inbound Rules is sufficient The AWS Documentation mentions the following A network access control list (ACLJ is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC. The correct answer is: Change the Inbound NACL to deny access from the suspecting IP"},{"question":"You have a vendor that needs access to an AWS resource. You create an AWS user account. You want to restrict access to the resource using a policy for just that user over a brief period. Which of the following would be an ideal policy to use?","choices":{"A":"An AWS Managed Policy","B":"An Inline Policy","C":"A Bucket Policy","D":"A bucket ACL"},"answer":"B","explanation":"The AWS Documentation gives an example on such a case Inline policies are useful if you want to maintain a strict one-to-one relationship between a policy and the principal entity that if s applied to. For example, you want to be sure that the permissions in a policy are not inadvertently assigned to a principal entity other than the one they're intended for. When you use an inline policy, the permissions in the policy cannot be inadvertently attached to the wrong principal entity. In addition, when you use the AWS Management Console to delete that principal entit the policies embedded in the principal entity are deleted as well. That's because they are part of the principal entity. Option A is invalid because AWS Managed Polices are ok for a group of users, but for individual users, inline policies are better. Option C and D are invalid because they are specifically meant for access to S3 buckets For more information on policies, please visit the following URL: https://docs.aws.amazon.com/IAM/latest/UserGuide/access managed-vs-inline The correct answer is: An Inline Policy"},{"question":"Your company has a requirement to monitor all root user activity by notification. How can this best be achieved? Choose 2 answers from the options given below. Each answer forms part of the solution","choices":{"A":"Create a Cloudwatch Events Rule s","B":"Create a Cloudwatch Logs Rule","C":"Use a Lambda function","D":"Use Cloudtrail API call"},"answer":"AC","explanation":"Below is a snippet from the AWS blogs on a solution Option B is invalid because you need to create a Cloudwatch Events Rule and there is such thing as a Cloudwatch Logs Rule Option D is invalid because Cloud Trail API calls can be recorded but cannot be used to send across notifications For more information on this blog article, please visit the following URL: https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activityy The correct answers are: Create a Cloudwatch Events Rule, Use a Lambda function"},{"question":"Your company has an EC2 Instance that is hosted in an AWS VPC. There is a requirement to ensure that logs files from the EC2 Instance are stored accordingly. The access should also be limited for the destination of the log files. How can this be accomplished? Choose 2 answers from the options given below. Each answer forms part of the solution","choices":{"A":"Stream the log files to a separate Cloudtrail trail","B":"Stream the log files to a separate Cloudwatch Log group","C":"Create an IAM policy that gives the desired level of access to the Cloudtrail trail","D":"Create an IAM policy that gives the desired level of access to the Cloudwatch Log group"},"answer":"BD","explanation":"You can create a Log group and send all logs from the EC2 Instance to that group. You can then limit the access to the Log groups via an IAM policy. Option A is invalid because Cloudtrail is used to record API activity and not for storing log files Option C is invalid because Cloudtrail is the wrong service to be used for this requirement For more information on Log Groups and Log Streams, please visit the following URL: * https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Workinj For more information on Access to Cloudwatch logs, please visit the following URL: * https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/auth-and-access-control-cwl.html The correct answers are: Stream the log files to a separate Cloudwatch Log group. Create an IAM policy that gives the desired level of access to the Cloudwatch Log group"},{"question":"You have a web site that is sitting behind AWS Cloudfront. You need to protect the web site against threats such as SQL injection and Cross site scripting attacks. Which of the following service can help in such a scenario","choices":{"A":"AWS Trusted Advisor","B":"AWS WAF","C":"AWS Inspector","D":"AWS Config"},"answer":"B","explanation":"The AWS Documentation mentions the following AWS WAF is a web application firewall that helps detect and block malicious web requests targeted at your web applications. AWS WAF allows you to create rules that can help protect against common web explogts like SQL injection and cross-site scripting. With AWS WAF you first identify the resource (either an Amazon CloudFront distribution or an Application Load Balancer) that you need to protect. Option A is invalid because this will only give advise on how you can better the security in your AWS account but not protect against threats mentioned in the question. Option C is invalid because this can be used to scan EC2 Instances for vulnerabilities but not protect against threats mentioned in the question. Option D is invalid because this can be used to check config changes but not protect against threats mentioned in the quest For more information on AWS WAF, please visit the following URL: https://aws.amazon.com/waf/details; The correct answer is: AWS WAF"},{"question":"Which of the following is not a best practice for carrying out a security audit?","choices":{"A":"Conduct an audit on a yearly basis","B":"Conduct an audit if application instances have been added to your account","C":"Conduct an audit if you ever suspect that an unauthorized person might have accessed your account","D":"Whenever there are changes in your organization"},"answer":"A","explanation":"A year's time is generally too long a gap for conducting security audits The AWS Documentation mentions the following You should audit your security configuration in the following situations: On a periodic basis. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWor stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. Option B, C and D are all the right ways and recommended best practices when it comes to conducting audits For more information on Security Audit guideline, please visit the below URL: https://docs.aws.amazon.com/eeneral/latest/gr/aws-security-audit-euide.html The correct answer is: Conduct an audit on a yearly basis"},{"question":"A company requires that data stored in AWS be encrypted at rest. Which of the following approaches achieve this requirement? Select 2 answers from the options given below.","choices":{"A":"When storing data in Amazon EBS, use only EBS-optimized Amazon EC2 instances.","B":"When storing data in EBS, encrypt the volume by using AWS KMS.","C":"When storing data in Amazon S3, use object versioning and MFA Delete.","D":"When storing data in Amazon EC2 Instance Store, encrypt the volume by using KMS.","E":"When storing data in S3, enable server-side encryptio"},"answer":"BE","explanation":"The AWS Documentation mentions the following To create an encrypted Amazon EBS volume, select the appropriate box in the Amazon EBS section of the Amazon EC2 console. You can use a custom customer master key (CMK) by choosing one from the list that appears below the encryption box. If you do not specify a custom CMK, Amazon EBS uses the AWS- managed CMK for Amazon EBS in your account. If there is no AWS-managed CMK for Amazon EBS in your account, Amazon EBS creates one. Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options of protecting data at rest in Amazon S3.  Use Server-Side Encryption - You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.  Use Client-Side Encryption - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. Option A is invalid because using EBS-optimized Amazon EC2 instances alone will not guarantee protection of instances at rest. Option C is invalid because this will not encrypt data at rest for S3 objects. Option D is invalid because you don't store data in Instance store. For more information on EBS encryption, please visit the below URL: https://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html For more information on S3 encryption, please visit the below URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsinEEncryption.html The correct answers are: When storing data in EBS, encrypt the volume by using AWS KMS. When storing data in S3, enable server-side encryption."},{"question":"A company has several Customer Master Keys (CMK), some of which have imported key material. Each CMK must be rotated annually. What two methods can the security team use to rotate each key? Select 2 answers from the options given below","choices":{"A":"Enable automatic key rotation for a CMK","B":"Import new key material to an existing CMK","C":"Use the CLI or console to explicitly rotate an existing CMK","D":"Import new key material to a new CMK; Point the key alias to the new CMK.","E":"Delete an existing CMK and a new default CMK will be create"},"answer":"AD","explanation":"The AWS Documentation mentions the following Automatic key rotation is available for all customer managed CMKs with KMS-generated key material. It is not available for CMKs that have imported key material (the value of the Origin field is External), but you can rotate these CMKs manually. Rotating Keys Manually You might want to create a newCMKand use it in place of a current CMK instead of enabling automatic key rotation. When the new CMK has different cryptographic material than the current CMK, using the new CMK has the same effect as changing the backing key in an existing CMK. The process of replacing one CMK with another is known as manual key rotation. When you begin using the new CMK, be sure to keep the original CMK enabled so that AWS KMS can decrypt data that the original CMK encrypted. When decrypting data, KMS identifies the CMK that was used to encrypt the data, and it uses the sam CMK to decrypt the dat A. As long as you keep both the original and new CMKs enabled, AWS KMS can decrypt any data that was encrypted by either CMK. Option B is invalid because you also need to point the key alias to the new key Option C is invalid because existing CMK keys cannot be rotated as they are Option E is invalid because deleting existing keys will not guarantee the creation of a new default CMK key For more information on Key rotation please see the below Link: https://docs.aws.amazon.com/kms/latest/developereuide/rotate-keys.html The correct answers are: Enable automatic key rotation for a CMK, Import new key material to a new CMK; Point the key alias to the new CMK."},{"question":"A new application will be deployed on EC2 instances in private subnets. The application will transfer sensitive data to and from an S3 bucket. Compliance requirements state that the data must not traverse the public internet. Which solution meets the compliance requirement?","choices":{"A":"Access the S3 bucket through a proxy server","B":"Access the S3 bucket through a NAT gateway.","C":"Access the S3 bucket through a VPC endpoint for S3","D":"Access the S3 bucket through the SSL protected S3 endpoint"},"answer":"C","explanation":"The AWS Documentation mentions the following A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. Option A is invalid because using a proxy server is not sufficient enough Option B and D are invalid because you need secure communication which should not traverse the internet For more information on VPC endpoints please see the below link https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.htmll The correct answer is: Access the S3 bucket through a VPC endpoint for S3"},{"question":"Your company has confidential documents stored in the simple storage service. Due to compliance requirements, you have to ensure that the data in the S3 bucket is available in a different geographical location. As an architect what is the change you would make to comply with this requirement.","choices":{"A":"Apply Multi-AZ for the underlying 53 bucket","B":"Copy the data to an EBS Volume in another Region","C":"Create a snapshot of the S3 bucket and copy it to another region","D":"Enable Cross region replication for the S3 bucket"},"answer":"D","explanation":"This is mentioned clearly as a use case for S3 cross-region replication You might configure cross-region replication on a bucket for various reasons, including the following:  Compliance requirements - Although, by default Amazon S3 stores your data across multiple geographically distant Availability Zones, compliance requirements might dictate that you store data at even further distances. Cross-region replication allows you to replicate data between distant AWS Regions to satisfy these compliance requirements. Option A is invalid because Multi-AZ cannot be used to S3 buckets Option B is invalid because copying it to an EBS volume is not a recommended practice Option C is invalid because creating snapshots is not possible in S3 For more information on S3 cross-region replication, please visit the following URL: https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.htmll The correct answer is: Enable Cross region replication for the S3 bucket"},{"question":"A company is planning on extending their on-premise AWS Infrastructure to the AWS Cloud. They need to have a solution that would give core benefits of traffic encryption and ensure latency is kept to a minimum. Which of the following would help fulfil this requirement? Choose 2 answers from the options given below","choices":{"A":"AWS VPN","B":"AWS VPC Peering","C":"AWS NAT gateways","D":"AWS Direct Connect"},"answer":"AD","explanation":"The AWS Document mention the following which supports the requirement Option B is invalid because VPC peering is only used for connection between VPCs and cannot be used to connect On-premise infrastructure to the AWS Cloud. Option C is invalid because NAT gateways is used to connect instances in a private subnet to the internet For more information on VPN Connections, please visit the following url https://docs.aws.amazon.com/AmazonVPC/latest/UserGuideA/pn-connections.html The correct answers are: AWS VPN, AWS Direct Connect"},{"question":"You need to inspect the running processes on an EC2 Instance that may have a security issue. How can you achieve this in the easiest way possible. Also you need to ensure that the process does not interfere with the continuous running of the instance.","choices":{"A":"Use AWS Cloudtrail to record the processes running on the server to an S3 bucket.","B":"Use AWS Cloudwatch to record the processes running on the server","C":"Use the SSM Run command to send the list of running processes information to an S3 bucket.","D":"Use AWS Config to see the changed process information on the server"},"answer":"C","explanation":"The SSM Run command can be used to send OS specific commands to an Instance. Here you can check and see the running processes on an instance and then send the output to an S3 bucket. Option A is invalid because this is used to record API activity and cannot be used to record running processes. Option B is invalid because Cloudwatch is a logging and metric service and cannot be used to record running processes. Option D is invalid because AWS Config is a configuration service and cannot be used to record running processes. For more information on the Systems Manager Run command, please visit the following URL: https://docs.aws.amazon.com/systems- manaEer/latest/usereuide/execute-remote-commands.htmll The correct answer is: Use the SSM Run command to send the list of running processes information to an S3 bucket."},{"question":"You have a requirement to conduct penetration testing on the AWS Cloud for a couple of EC2 Instances. How could you go about doing this? Choose 2 right answers from the options given below.","choices":{"A":"Get prior approval from AWS for conducting the test","B":"Use a pre-approved penetration testing tool.","C":"Work with an AWS partner and no need for prior approval request from AWS","D":"Choose any of the AWS instance type"},"answer":"AB","explanation":"You can use a pre-approved solution from the AWS Marketplace. But till date the AWS Documentation still mentions that you have to get prior approval before conducting a test on the AWS Cloud for EC2 Instances. Option C and D are invalid because you have to get prior approval first. AWS Docs Provides following details: \\"For performing a penetration test on AWS resources first of all we need to take permission from AWS and complete a requisition form and submit it for approval. The form should contain information about the instances you wish to test identify the expected start and end dates/times of your test and requires you to read and agree to Terms and Conditions specific to penetration testing and to the use of appropriate tools for testing. Note that the end date may not be more than 90 days from the start date.\\" ( At this time, our policy does not permit testing small or micro RDS instance types. Testing of ml .small, t1 .micro or t2.nano EC2 instance types is not permitted. For more information on penetration testing please visit the following URL: https://aws.amazon.eom/security/penetration-testine/l The correct answers are: Get prior approval from AWS for conducting the test Use a pre-approved penetration testing tool."},{"question":"A company is using a Redshift cluster to store their data warehouse. There is a requirement from the Internal IT Security team to ensure that data gets encrypted for the Redshift database. How can this be achieved?","choices":{"A":"Encrypt the EBS volumes of the underlying EC2 Instances","B":"Use AWS KMS Customer Default master key","C":"Use SSL/TLS for encrypting the data","D":"Use S3 Encryption"},"answer":"B","explanation":"The AWS Documentation mentions the following Amazon Redshift uses a hierarchy of encryption keys to encrypt the database. You can use either AWS Key Management Servic (AWS KMS) or a hardware security module (HSM) to manage the toplevel encryption keys in this hierarchy. The process that Amazon Redshift uses for encryption differs depending on how you manage keys. Option A is invalid because its the cluster that needs to be encrypted Option C is invalid because this encrypts objects in transit and not objects at rest Option D is invalid because this is used only for objects in S3 buckets For more information on Redshift encryption, please visit the following URL: https://docs.aws.amazon.com/redshift/latest/memt/workine-with-db-encryption.htmll The correct answer is: Use AWS KMS Customer Default master key"},{"question":"A company is planning to run a number of Admin related scripts using the AWS Lambda service. There is a need to understand if there are any errors encountered when the script run. How can this be accomplished in the most effective manner.","choices":{"A":"Use Cloudwatch metrics and logs to watch for errors","B":"Use Cloudtrail to monitor for errors","C":"Use the AWS Config service to monitor for errors","D":"Use the AWS inspector service to monitor for errors"},"answer":"A","explanation":"The AWS Documentation mentions the following AWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function. Lambda logs all requests handled by your function and also automatically stores logs generated by your code through Amazon CloudWatch Logs. Option B,C and D are all invalid because these services cannot be used to monitor for errors. I For more information on Monitoring Lambda functions, please visit the following URL: https://docs.aws.amazon.com/lambda/latest/dg/monitorine-functions- loes.htmll The correct answer is: Use Cloudwatch metrics and logs to watch for errors"},{"question":"A company hosts data in S3. There is now a mandate that going forward all data in the S3 bucket needs to encrypt at rest. How can this be achieved?","choices":{"A":"Use AWS Access keys to encrypt the data","B":"Use SSL certificates to encrypt the data","C":"Enable server side encryption on the S3 bucket","D":"Enable MFA on the S3 bucket"},"answer":"C","explanation":"The AWS Documentation mentions the following Server-side encryption is about data encryption at restthat is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. Options A and B are invalid because neither Access Keys nor SSL certificates can be used to encrypt data. Option D is invalid because MFA is just used as an extra level of security for S3 buckets For more information on S3 server side encryption, please refer to the below Link: https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full AWS-Certified-Security-Specialty dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/AWS-Certified-Security-Specialty-exam-dumps.html (528 New Questions)"},{"question":"Your company has a set of EBS volumes defined in AWS. The security mandate is that all EBS volumes are encrypted. What can be done to notify the IT admin staff if there are any unencrypted volumes in the account.","choices":{"A":"Use AWS Inspector to inspect all the EBS volumes","B":"Use AWS Config to check for unencrypted EBS volumes","C":"Use AWS Guard duty to check for the unencrypted EBS volumes","D":"Use AWS Lambda to check for the unencrypted EBS volumes"},"answer":"B","explanation":"The enc config rule for AWS Config can be used to check for unencrypted volumes. encrypted-volurrn 5 volumes that are in an attached state are encrypted. If you specify the ID of a KMS key for encryptio using the kmsld parameter, the rule checks if the EBS volumes in an attached state are encrypted with that KMS key*1. Options A and C are incorrect since these services cannot be used to check for unencrypted EBS volumes Option D is incorrect because even though this is possible, trying to implement the solution alone with just the Lambda servk would be too difficult For more information on AWS Config and encrypted volumes, please refer to below URL: https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html"},{"question":"You have just developed a new mobile application that handles analytics workloads on large scale datasets that are stored on Amazon Redshift. Consequently, the application needs to access Amazon Redshift tables. Which of the belov methods would be the best both practically and security-wise, to access the tables? Choose the correct answer from the options below","choices":{"A":"Create an IAM user and generate encryption keys for that use","B":"Create a policy for Redshift readonly acces","C":"Embed th keys in the application.","D":"Create an HSM client certificate in Redshift and authenticate using this certificate.","E":"Create a Redshift read-only access policy in IAM and embed those credentials in the application.","F":"Use roles that allow a web identity federated user to assume a role that allows access to the Redshift table by providing temporary credentials."},"answer":"D","explanation":"The AWS Documentation mentions the following \\"When you write such an app, you'll make requests to AWS services that must be signed with an AWS access key. However, we strongly recommend that you do not embed or distribute long-term AWS credentials with apps that a user downloads t device, even in an encrypted store. Instead, build your app so that it requests temporary AWS security credentials dynamica when needed using web identify federation. The supplied temporary credentials map to an AWS role that has only the permissioi needed to perform the tasks required by the mobile app\\". Option A.B and C are all automatically incorrect because you need to use IAM Roles for Secure access to services For more information on web identity federation please refer to the below Link: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html The correct answer is: Use roles that allow a web identity federated user to assume a role that allows access to the RedShift table by providing temporary credentials."},{"question":"You are designing a connectivity solution between on-premises infrastructure and Amazon VPC. Your server's on-premises will be communicating with your VPC instances. You will be establishing IPSec tunnels over the internet. Yo will be using VPN gateways and terminating the IPsec tunnels on AWSsupported customer gateways. Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above? Choose 4 answers form the options below","choices":{"A":"End-to-end protection of data in transit","B":"End-to-end Identity authentication","C":"Data encryption across the internet","D":"Protection of data in transit over the Internet","E":"Peer identity authentication between VPN gateway and customer gateway","F":"Data integrity protection across the Internet"},"answer":"CDEF","explanation":"Since the Web server needs to talk to the database server on port 3306 that means that the database server should allow incoming traffic on port 3306. The below table from the aws documentation shows how the security groups should be set up. Option B is invalid because you need to allow incoming access for the database server from the WebSecGrp security group. Options C and D are invalid because you need to allow Outbound traffic and not inbound traffic For more information on security groups please visit the below Link: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC Scenario2.html The correct answer is: Allow Inbound on port 3306 for Source Web Server Security Group WebSecGrp."},{"question":"Your IT Security team has identified a number of vulnerabilities across critical EC2 Instances in the company's AWS Account. Which would be the easiest way to ensure these vulnerabilities are remediated?","choices":{"A":"Create AWS Lambda functions to download the updates and patch the servers.","B":"Use AWS CLI commands to download the updates and patch the servers.","C":"Use AWS inspector to patch the servers","D":"Use AWS Systems Manager to patch the servers"},"answer":"D","explanation":"The AWS Documentation mentions the following You can quickly remediate patch and association compliance issues by using Systems Manager Run Command. You can tat either instance IDs or Amazon EC2 tags and execute the AWSRefreshAssociation document or the AWS-RunPatchBaseline document. If refreshing the association or re-running the patch baseline fails to resolve the compliance issue, then you need to investigate your associations, patch baselines, or instance configurations to understand why the Run Command executions did not resolve the problem Options A and B are invalid because even though this is possible, still from a maintenance perspective it would be difficult to maintain the Lambda functions Option C is invalid because this service cannot be used to patch servers For more information on using Systems Manager for compliance remediation please visit the below Link: https://docs.aws.amazon.com/systems-manaeer/latest/usereuide/sysman-compliance-fixing.html The correct answer is: Use AWS Systems Manager to patch the servers"},{"question":"The CFO of a company wants to allow one of his employees to view only the AWS usage report page. Which of the below mentioned IAM policy statements allows the user to have access to the AWS usage report page?","choices":{"A":"\\"Effect\\": \\"Allow\\". \\"Action\\": [\\"Describe\\"], \\"Resource\\": \\"Billing\\"","B":"\\"Effect\\": \\"Allow\\", \\"Action\\": [\\"AccountUsage], \\"Resource\\": \\"*\\"","C":"\\"Effect': \\"Allow\\", \\"Action\\": [\\"aws-portal:ViewUsage\\",\\" aws-portal:ViewBilling\\"], \\"Resource\\": \\"*\\"","D":"\\"Effect\\": \\"Allow\\", \\"Action\\": [\\"aws-portal: ViewBilling\\"], \\"Resource\\": \\"*\\""},"answer":"C","explanation":"the aws documentation, below is the access required for a user to access the Usage reports page and as per this, Option C is the right answer."},{"question":"You are designing a custom 1","choices":{"A":"","B":"","C":"","D":""},"answer":"A","explanation":"The Condition clause can be used to ensure users can only work with resources if they are MFA authenticated. Option B and C are wrong since the aws:MultiFactorAuthPresent clause should be marked as true. Here you are saying that onl if the user has been MFA activated, that means it is true, then allow access. Option D is invalid because the \\"boor clause is missing in the evaluation for the condition clause. Boolean conditions let you construct Condition elements that restrict access based on comparing a key to \\"true\\" or \\"false.\\" Here in this scenario the boot attribute in the condition element will return a value True for option A which will ensure that access is allowed on S3 resources. For more information on an example on such a policy, please visit the following URL:"},{"question":"You want to get a list of vulnerabilities for an EC2 Instance as per the guidelines set by the Center of Internet Security. How can you go about doing this?","choices":{"A":"Enable AWS Guard Duty for the Instance","B":"Use AWS Trusted Advisor","C":"Use AWS inspector","D":"UseAWSMacie"},"answer":"C","explanation":"The AWS Inspector service can inspect EC2 Instances based on specific Rules. One of the rules packages is based on the guidelines set by the Center of Internet Security Center for Internet security (CIS) Benchmarks The CIS Security Benchmarks program provides well-defined, un-biased and consensus-based industry best practices to help organizations assess and improve their security. Amazon Web Services is a CIS Security Benchmarks Member company and the list of Amazon Inspector certifications can be viewed nere. Option A is invalid because this can be used to protect an instance but not give the list of vulnerabilities Options B and D are invalid because these services cannot give a list of vulnerabilities For more information on the guidelines, please visit the below URL: * https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cis.html The correct answer is: Use AWS Inspector"},{"question":"Which technique can be used to integrate AWS IAM (Identity and Access Management) with an on Questions & Answers PDF P-63 premise LDAP (Lightweight Directory Access Protocol) directory service?","choices":{"A":"Use an IAM policy that references the LDAP account identifiers and the AWS credentials.","B":"Use SAML (Security Assertion Markup Language) to enable single sign-on between AWS and LDAP.","C":"Use AWS Security Token Service from an identity broker to issue short-lived AWS credentials.","D":"Use IAM roles to automatically rotate the IAM credentials when LDAP credentials are update"},"answer":"B","explanation":"On the AWS Blog site the following information is present to help on this context The newly released whitepaper. Single Sign-On: Integrating AWS, OpenLDAP, and Shibboleth, will help you integrate your existing LDAP-based user directory with AWS. When you integrate your existing directory with AWS, your users can access AWS by using their existing credentials. This means that your users don't need to maintain yet another user name and password just to access AWS resources. Option A.C and D are all invalid because in this sort of configuration, you have to use SAML to enable single sign on. For more information on integrating AWS with LDAP for Single Sign-On, please visit the following URL: https://aws.amazon.eom/blogs/security/new-whitepaper-sinEle-sign-on-inteErating-aws-openldapand- shibboleth/l The correct answer is: Use SAML (Security Assertion Markup Language) to enable single sign-on between AWS and LDAP."},{"question":"You have a set of Customer keys created using the AWS KMS service. These keys have been used for around 6 months. You are now trying to use the new KMS features for the existing set of key's but are not able to do so. What could be the reason for this.","choices":{"A":"You have not explicitly given access via the key policy","B":"You have not explicitly given access via the IAM policy","C":"You have not given access via the IAM roles","D":"You have not explicitly given access via IAM users"},"answer":"A","explanation":"By default, keys created in KMS are created with the default key policy. When features are added to KMS, you need to explii update the default key policy for these keys. Option B,C and D are invalid because the key policy is the main entity used to provide access to the keys For more information on upgrading key policies please visit the following URL: https://docs.aws.ama20n.com/kms/latest/developerguide/key-policy-upgrading.html ( The correct answer is: You have not explicitly given access via the key policy"},{"question":"A customer has an instance hosted in the AWS Public Cloud. The VPC and subnet used to host the Instance have been created with the default settings for the Network Access Control Lists. They need to provide an IT Administrator secure access to the underlying instance. How can this be accomplished.","choices":{"A":"Ensure the Network Access Control Lists allow Inbound SSH traffic from the IT Administrator's Workstation","B":"Ensure the Network Access Control Lists allow Outbound SSH traffic from the IT Administrator's Workstation","C":"Ensure that the security group allows Inbound SSH traffic from the IT Administrator's Workstation","D":"Ensure that the security group allows Outbound SSH traffic from the IT Administrator's Workstation"},"answer":"C","explanation":"Options A & B are invalid as default NACL rule will allow all inbound and outbound traffic. The requirement is that the IT administrator should be able to access this EC2 instance from his workstation. For that we need to enable the Security Group of EC2 instance to allow traffic from the IT administrator's workstation. Hence option C is correct. Option D is incorrect as we need to enable the Inbound SSH traffic on the EC2 instance Security Group since the traffic originate' , from the IT admin's workstation. The correct answer is: Ensure that the security group allows Inbound SSH traffic from the IT Administrator's Workstation"},{"question":"One of your company's EC2 Instances have been compromised. The company has strict po thorough investigation on finding the culprit for the security breach. What would you do in from the options given below.","choices":{"A":"Take a snapshot of the EBS volume","B":"Isolate the machine from the network","C":"Make sure that logs are stored securely for auditing and troubleshooting purpose","D":"Ensure all passwords for all IAM users are changed","E":"Ensure that all access kevs are rotate"},"answer":"ABC","explanation":"Some of the important aspects in such a situation are 1) First isolate the instance so that no further security harm can occur on other AWS resources 2) Take a snapshot of the EBS volume for further investigation. This is incase if you need to shutdown the initial instance and do a separate investigation on the data 3) Next is Option C. This indicates that we have already got logs and we need to make sure that it is stored securely so that n unauthorised person can access it and manipulate it. Option D and E are invalid because they could have adverse effects for the other IAM users. For more information on adopting a security framework, please refer to below URL https://d1 .awsstatic.com/whitepapers/compliance/NIST Cybersecurity Framework Note: In the question we have been asked to take actions to find the culprit and to help the investigation or to further reduce the damage that has happened due to the security breach. So by keeping logs secure is one way of helping the investigation. The correct answers are: Take a snapshot of the EBS volume. Isolate the machine from the network. Make sure that logs are stored securely for auditing and troubleshooting purpose"},{"question":"Your company has an external web site. This web site needs to access the objects in an S3 bucket. Which of the following would allow the web site to access the objects in the most secure manner?","choices":{"A":"Grant public access for the bucket via the bucket policy","B":"Use the aws:Referer key in the condition clause for the bucket policy","C":"Use the aws:sites key in the condition clause for the bucket policy","D":"Grant a role that can be assumed by the web site"},"answer":"B","explanation":"An example of this is given intheAWS Documentatioi Restricting Access to a Specific HTTP Referrer Suppose you have a website with domain name (www.example.com or example.com) with links to photos and videos stored in your S3 bucket examplebucket. By default, all the S3 resources are private, so only the AWS account that created the resources can access them. To allow read access to these objects from your website, you can add a bucket policy that allows s3:GetObject permission with a condition, using the aws:referer key, that the get request must originate from specific webpages. The following policy specifies the StringLike condition with the aws:Referer condition key. Option A is invalid because giving public access is not a secure way to provide access Option C is invalid because aws:sites is not a valid condition key Option D is invalid because IAM roles will not be assigned to web sites For more information on example bucket policies please visit the below Link: 1 https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html The correct answer is: Use the aws:Referer key in the condition clause for the bucket policy"},{"question":"An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this?","choices":{"A":"Create an IAM policy with the security group and use that security group for AWS console login","B":"Create an IAM policy with a condition which denies access when the IP address range is not from the organization","C":"Configure the EC2 instance security group which allows traffic only from the organization's IP range","D":"Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console"},"answer":"B","explanation":"You can actually use a Deny condition which will not allow the person to log in from outside. The below example shows the Deny condition to ensure that any address specified in the source address is not allowed to access the resources in aws. Option A is invalid because you don't mention the security group in the IAM policy Option C is invalid because security groups by default don't allow traffic Option D is invalid because the IAM policy does not have such an option For more information on IAM policy conditions, please visit the URL: http://docs.aws.amazon.com/IAM/latest/UserGuide/access pol examples.htm l#iam-policy-example-ec2-two-condition! The correct answer is: Create an IAM policy with a condition which denies access when the IP address range is not from the organization"},{"question":"Your development team has started using AWS resources for development purposes. The AWS account has just been created. Your IT Security team is worried about possible leakage of AWS keys. What is the first level of measure that should be taken to protect the AWS account.","choices":{"A":"Delete the AWS keys for the root account","B":"Create IAM Groups","C":"Create IAM Roles","D":"Restrict access using IAM policies"},"answer":"A","explanation":"The first level or measure that should be taken is to delete the keys for the IAM root user When you log into your account and go to your Security Access dashboard, this is the first step that can be seen Option B and C are wrong because creation of IAM groups and roles will not change the impact of leakage of AWS root access keys Option D is wrong because the first key aspect is to protect the access keys for the root account For more information on best practises for Security Access keys, please visit the below URL: https://docs.aws.amazon.com/eeneral/latest/gr/aws-access-keys-best-practices.html The correct answer is: Delete the AWS keys for the root account"},{"question":"Your company has a set of 1000 EC2 Instances defined in an AWS Account. They want to effectively automate several administrative tasks on these instances. Which of the following would be an effective way to achieve this?","choices":{"A":"Use the AWS Systems Manager Parameter Store","B":"Use the AWS Systems Manager Run Command","C":"Use the AWS Inspector","D":"Use AWS Config"},"answer":"B","explanation":"The AWS Documentation mentions the following AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any Amazon EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost. Option A is invalid because this service is used to store parameter Option C is invalid because this service is used to scan vulnerabilities in an EC2 Instance. Option D is invalid because this service is used to check for configuration changes For more information on executing remote commands, please visit the below U https://docs.aws.amazon.com/systems-manaEer/latest/usereuide/execute-remote-commands.htmll ( The correct answer is: Use the AWS Systems Manager Run Command"},{"question":"Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) A windows machine in one VPC needs to join the AD domain in another VPC. VPC Peering has been established. But the domain join is not working. What is the other step that needs to be followed to ensure that the AD domain join can work as intended","choices":{"A":"Change the VPC peering connection to a VPN connection","B":"Change the VPC peering connection to a Direct Connect connection","C":"Ensure the security groups for the AD hosted subnet has the right rule for relevant subnets","D":"Ensure that the AD is placed in a public subnet"},"answer":"C","explanation":"In addition to VPC peering and setting the right route tables, the security groups for the AD EC2 instance needs to ensure the right rules are put in place for allowing incoming traffic. Option A and B is invalid because changing the connection type will not help. This is a problem with the Security Groups. Option D is invalid since the AD should not be placed in a public subnet For more information on allowing ingress traffic for AD, please visit the following url |https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/ingress.html| The correct answer is: Ensure the security groups for the AD hosted subnet has the right rule for relevant subnets"},{"question":"You currently have an S3 bucket hosted in an AWS Account. It holds information that needs be accessed by a partner account. Which is the MOST secure way to allow the partner account to access the S3 bucket in your account? Select 3 options.","choices":{"A":"Ensure an IAM role is created which can be assumed by the partner account.","B":"Ensure an IAM user is created which can be assumed by the partner account.","C":"Ensure the partner uses an external id when making the request","D":"Provide the ARN for the role to the partner account","E":"Provide the Account Id to the partner account","F":"Provide access keys for your account to the partner account"},"answer":"ACD","explanation":"Option B is invalid because Roles are assumed and not IAM users Option E is invalid because you should not give the account ID to the partner Option F is invalid because you should not give the access keys to the partner The below diagram from the AWS documentation showcases an example on this wherein an IAM role and external ID is us> access an AWS account resources Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) For more information on creating roles for external ID'S please visit the following URL: The correct answers are: Ensure an IAM role is created which can be assumed by the partner account. Ensure the partner uses an external id when making the request Provide the ARN for the role to the partner account"},{"question":"You have a set of application , database and web servers hosted in AWS. The web servers are placed behind an ELB. There are separate security groups for the application, database and web servers. The network security groups have been defined accordingly. There is an issue with the communication between the application and database servers. In order to troubleshoot the issue between just the application and database server, what is the ideal set of MINIMAL steps you would take?","choices":{"A":"Check the Inbound security rules for the database security group Check the Outbound security rules for the application security group","B":"Check the Outbound security rules for the database security group I Check the inbound security rules for the application security group","C":"Check the both the Inbound and Outbound security rules for the database security group Check the inbound security rules for the application security group","D":"Check the Outbound security rules for the database security groupCheck the both the Inbound and Outbound security rules for the application security group"},"answer":"A","explanation":"Here since the communication would be established inward to the database server and outward from the application server, you need to ensure that just the Outbound rules for application server security groups are checked. And then just the Inbound rules for database server security groups are checked. Option B can't be the correct answer. It says that we need to check the outbound security group which is not needed. We need to check the inbound for DB SG and outbound of Application SG. Because, this two group Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) need to communicate with each other to function properly. Option C is invalid because you don't need to check for Outbound security rules for the database security group Option D is invalid because you don't need to check for Inbound security rules for the application security group For more information on Security Groups, please refer to below URL: The correct answer is: Check the Inbound security rules for the database security group Check the Outbound security rules for the application security group"},{"question":"You need to create a Linux EC2 instance in AWS. Which of the following steps is used to ensure secure authentication the EC2 instance from a windows machine. Choose 2 answers from the options given below.","choices":{"A":"Ensure to create a strong password for logging into the EC2 Instance","B":"Create a key pair using putty","C":"Use the private key to log into the instance","D":"Ensure the password is passed securely using SSL"},"answer":"BC","explanation":"The AWS Documentation mentions the following You can use Amazon EC2 to create your key pair. Alternatively, you could use a third-party tool and then import the public key to Amazon EC2. Each key pair requires a name. Be sure to choose a name that is easy to remember. Amazon EC2 associates the public key with the name that you specify as the key name. Amazon EC2 stores the public key only, and you store the private key. Anyone who possesses your private key can decrypt login information, so it's important that you store your private keys in a secure place. Options A and D are incorrect since you should use key pairs for secure access to Ec2 Instances For more information on EC2 key pairs, please refer to below URL: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html The correct answers are: Create a key pair using putty. Use the private key to log into the instance"},{"question":"You have just developed a new mobile application that handles analytics workloads on large scale datasets that are stored on Amazon Redshift. Consequently, the application needs to access Amazon Redshift tables. Which of the belov methods would be the best both practically and security-wise, to access the tables? Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) Choose the correct answer from the options below","choices":{"A":"Create an IAM user and generate encryption keys for that use","B":"Create a policy for Redshift readonly acces","C":"Embed th keys in the application.","D":"Create an HSM client certificate in Redshift and authenticate using this certificate.","E":"Create a Redshift read-only access policy in IAM and embed those credentials in the application.","F":"Use roles that allow a web identity federated user to assume a role that allows access to the Redshift table by providing temporary credentials."},"answer":"D","explanation":"The AWS Documentation mentions the following \\"When you write such an app, you'll make requests to AWS services that must be signed with an AWS access key. However, we strongly recommend that you do not embed or distribute long-term AWS credentials with apps that a user downloads t device, even in an encrypted store. Instead, build your app so that it requests temporary AWS security credentials dynamica when needed using web identify federation. The supplied temporary credentials map to an AWS role that has only the permissioi needed to perform the tasks required by the mobile app\\". Option A.B and C are all automatically incorrect because you need to use IAM Roles for Secure access to services For more information on web identity federation please refer to the below Link: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html The correct answer is: Use roles that allow a web identity federated user to assume a role that allows access to the RedShift table by providing temporary credentials."},{"question":"An organization has launched 5 instances: 2 for production and 3 for testing. The organization wants that one particular group of IAM users should only access the test instances and not the production ones. How can the organization set that as a part of the policy? Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As)","choices":{"A":"Launch the test and production instances in separate regions and allow region wise access to the group","B":"Define the IAM policy which allows access based on the instance ID","C":"Create an IAM policy with a condition which allows access to only small instances","D":"Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specification tags"},"answer":"D","explanation":"Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type  you can quickly identify a specific resource based on the tags you've assigned to it Option A is invalid because this is not a recommended practices Option B is invalid because this is an overhead to maintain this in policies Option C is invalid because the instance type will not resolve the requirement For information on resource tagging, please visit the below URL: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Usine_Tags.htmll The correct answer is: Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specific tags"},{"question":"There is a set of Ec2 Instances in a private subnet. The application hosted on these EC2 Instances need to access a DynamoDB table. It needs to be ensured that traffic does not flow out to the internet. How can this be achieved?","choices":{"A":"Use a VPC endpoint to the DynamoDB table","B":"Use a VPN connection from the VPC","C":"Use a VPC gateway from the VPC","D":"Use a VPC Peering connection to the DynamoDB table"},"answer":"A","explanation":"The following diagram from the AWS Documentation shows how you can access the DynamoDB service from within a V without going to the Internet This can be done with the help of a VPC endpoint Option B is invalid because this is used for connection between an on-premise solution and AWS Option C is invalid because there is no such option Option D is invalid because this is used to connect 2 VPCs For more information on VPC endpointsfor DynamoDB, please visit the URL: The correct answer is: Use a VPC endpoint to the DynamoDB table"},{"question":"Your company is hosting a set of EC2 Instances in AWS. They want to have the ability to detect if any port scans occur on their AWS EC2 Instances. Which of the following can help in this regard?","choices":{"A":"Use AWS inspector to consciously inspect the instances for port scans","B":"Use AWS Trusted Advisor to notify of any malicious port scans","C":"Use AWS Config to notify of any malicious port scans","D":"Use AWS Guard Duty to monitor any malicious port scans"},"answer":"D","explanation":"The AWS blogs mention the following to support the use of AWS GuardDuty GuardDuty voraciously consumes multiple data streams, including several threat intelligence feeds, staying aware of malicious addresses, devious domains, and more importantly, learning to accurately identify malicious or unauthorized behavior in your AWS accounts. In combination with information gleaned from your VPC Flow Logs, AWS CloudTrail Event Logs, and DNS logs, th allows GuardDuty to detect many different types of dangerous and mischievous behavior including probes for known vulnerabilities, port scans and probes, and access from unusual locations. On the AWS side, it looks for suspicious AWS account activity such as unauthorized deployments, unusual CloudTrail activity, patterns of access to Your Partner of IT Exam visit - https://www.exambible.com We recommend you to try the PREMIUM AWS-Certified-Security-Specialty Dumps From Exambible https://www.exambible.com/AWS-Certified-Security-Specialty-exam/ (191 Q&As) AWS API functions, and attempts to exceed multiple service limits. GuardDuty will also look for compromised EC2 instances talking to malicious entities or services, data exfiltration attempts, and instances that are mining cryptocurrency. Options A, B and C are invalid because these services cannot be used to detect port scans For more information on AWS Guard Duty, please refer to the below Link: https://aws.amazon.com/blogs/aws/amazon-guardduty-continuous-security-monitoring-threatdetection; ( The correct answer is: Use AWS Guard Duty to monitor any malicious port scans"},{"question":"Your CTO thinks your AWS account was hacked. What is the only way to know for certain if there was unauthorized access and what they did, assuming your hackers are very sophisticated AWS engineers and doing everything they can to cover their tracks?","choices":{"A":"Use CloudTrail Log File Integrity Validation.","B":"Use AWS Config SNS Subscriptions and process events in real time.","C":"Use CloudTrail backed up to AWS S3 and Glacier.","D":"Use AWS Config Timeline forensic"},"answer":"A","explanation":"The AWS Documentation mentions the following To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it you can use CloudTrail log file integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. You can use the AWS CLI to validate the files in the location where CloudTrail delivered them Validated log files are invaluable in security and forensic investigations. For example, a validated log file enables you to assert positively that the log file itself has not changed, or that particular user credentials performed specific API activity. The CloudTrail log file integrity validation process also lets you know if a log file has been deleted or changed, or assert positively that no log files were delivered to your account during a given period of time. Options B.C and D is invalid because you need to check for log File Integrity Validation for cloudtrail logs For more information on Cloudtrail log file validation, please visit the below URL: http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file- validation-intro.html The correct answer is: Use CloudTrail Log File Integrity Validation. omit your Feedback/Queries to our Expert"},{"question":"Your development team is using access keys to develop an application that has access to S3 and DynamoDB. A new security policy has outlined that the credentials should not be older than 2 months, and should be rotated. How can you achieve this?","choices":{"A":"Use the application to rotate the keys in every 2 months via the SDK","B":"Use a script to query the creation date of the key","C":"If older than 2 months, create new access key and update all applications to use it inactivate the old key and delete it.","D":"Delete the user associated with the keys after every 2 month","E":"Then recreate the user again.","F":"Delete the IAM Role associated with the keys after every 2 month","G":"Then recreate the IAM Role again."},"answer":"B","explanation":"One can use the CLI command list-access-keys to get the access keys. This command also returns the \\"CreateDate\\" of the keys. If the CreateDate is older than 2 months, then the keys can be deleted. The Returns list-access-keys CLI command returns information about the access key IDs associated with the specified IAM user. If there are none, the action returns an empty list Option A is incorrect because you might as use a script for such maintenance activities Option C is incorrect because you would not rotate the users themselves Option D is incorrect because you don't use IAM roles for such a purpose For more information on the CLI command, please refer to the below Link: http://docs.aws.amazon.com/cli/latest/reference/iam/list-access-keys.htmll The correct answer is: Use a script to query the creation date of the keys. If older than 2 months, create new access key and update all applications to use it inactivate the old key and delete it."},{"question":"A company manages three separate IAM accounts for its production, development, and test environments, Each Developer is assigned a unique IAM user under the development account. A new application hosted on an Amazon EC2 instance in the developer account requires read access to the archived documents stored in an Amazon S3 bucket in the production account.\\nHow should access be granted?","choices":{"A":"Create an IAM role in the production account and allow EC2 instances in the development account to assume that role using the trust policy","B":"Provide read access for the required S3 bucket to this role.","C":"Use a custom identity broker to allow Developer IAM users to temporarily access the S3 bucket.","D":"Create a temporary IAM user for the application to use in the production account.","E":"Create a temporary IAM user in the production account and provide read access to Amazon S3.Generate the temporary IAM user's access key and secret key and store these on the EC2 instance used by the application in the development account"},"answer":"A","explanation":"https://IAM.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/"},{"question":"To meet regulatory requirements, a Security Engineer needs to implement an IAM policy that restricts the use of AWS services to the us-east-1 Region. What policy should the Engineer implement?","choices":{"A":[{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":"*","Resource":"*","Condition":{"StringEquals":{"aws:RequestedRegion":"us-east-1"}}}]}],"B":["A computer code with black text Description automatically generated",{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":"*","Resource":"*","Condition":{"StringEquals":{"ec2:Region":"us-east-1"}}}]}],"C":["A computer code with black text Description automatically generated",{"Version":"2012-10-17","Statement":[{"Effect":"Deny","Action":"*","Resource":"*","Condition":{"StringNotEquals":{"aws:RequestedRegion":"us-east-1"}}}]}],"D":["A computer code with text Description automatically generated",{"Version":"2012-10-17","Statement":[{"Effect":"Deny","Action":"*","Resource":"*","Condition":{"StringEquals":{"aws:RequestedRegion":"us-east-1"}}}]}]},"answer":"C","explanation":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.h"},{"question":"Your CTO thinks your IAM account was hacked. What is the only way to know for certain if there was unauthorized access and what they did, assuming your hackers are very sophisticated IAM engineers and doing everything they can to cover their tracks?","choices":{"A":"Use CloudTrail Log File Integrity Validation.","B":"Use IAM Config SNS Subscriptions and process events in real time.","C":"Use CloudTrail backed up to IAM S3 and Glacier.","D":"Use IAM Config Timeline forensics."},"answer":"A","explanation":"The IAM Documentation mentions the following To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it you can use CloudTrail log file integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. You can use the IAM CLI to validate the files in the location where CloudTrail delivered them Validated log files are invaluable in security and forensic investigations. For example, a validated log file enables you to assert positively that the log file itself has not changed, or that particular user credentials performed specific API activity. The CloudTrail log file integrity validation process also lets you know if a log file has been deleted or changed, or assert positively that no log files were delivered to your account during a given period of time. Options B.C and D is invalid because you need to check for log File Integrity Validation for cloudtrail logs For more information on Cloudtrail log file validation, please visit the below URL: http://docs.IAM.amazon.com/IAMcloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html The correct answer is: Use CloudTrail Log File Integrity Validation. omit your Feedback/Queries to our Expert"},{"question":"What are the MOST secure ways to protect the AWS account root user of a recently opened AWS account? (Select TWO.)","choices":{"A":"Use the AWS account root user access keys instead of the AWS Management Console.","B":"Enable multi-factor authentication for the AWS IAM users with the Adminis-tratorAccess managed policy attached to them.","C":"Enable multi-factor authentication for the AWS account root user.","D":"Use AWS KMS to encrypt all AWS account root user and AWS IAM access keys and set automatic rotation to 30 days.","E":"Do not create access keys for the AWS account root user; instead, create AWS IAM users."},"answer":"CE","explanation":""},{"question":"The Security Engineer is managing a traditional three-tier web application that is running on Amazon EC2 instances. The application has become the target of increasing numbers of malicious attacks from the Internet.\\nWhat steps should the Security Engineer take to check for known vulnerabilities and limit the attack surface? (Choose two.)","choices":{"A":"Use AWS Certificate Manager to encrypt all traffic between the client and application servers.","B":"Review the application security groups to ensure that only the necessary ports are open.","C":"Use Elastic Load Balancing to offload Secure Sockets Layer encryption.","D":"Use Amazon Inspector to periodically scan the backend instances.","E":"Use AWS Key Management Services to encrypt all the traffic between the client and application servers."},"answer":"BD","explanation":"The steps that the Security Engineer should take to check for known vulnerabilities and limit the attack surface are: B. Review the application security groups to ensure that only the necessary ports are open. This is a good practice to reduce the exposure of the EC2 instances to potential attacks from the Internet. Application security groups are a feature of Azure that allow you to group virtual machines and define network security policies based on those groups1. D. Use Amazon Inspector to periodically scan the backend instances. This is a service that helps you to identify vulnerabilities and exposures in your EC2 instances and applications. Amazon Inspector can perform automated security assessments based on predefined or custom rules packages2. Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Security-Specialty dumps https://www.2passeasy.com/dumps/AWS-Certified-Security-Specialty/ (589 New Questions)"}]`),gr={class:"text-center p-4 max-w-2xl mx-auto"},yr={key:0},br={key:1},wr={class:"text-sm text-gray-600 mb-2"},Ar={class:"mb-2"},vr={class:"text-justify"},Sr={class:"block"},Cr=["value","disabled"],kr={class:"ml-1"},Wr={class:"space-x-2"},Ir=["disabled"],Tr={key:0,class:"mt-4"},Dr={key:0,class:"mt-2 text-left"},Mr={class:"text-justify"},Er={key:2},xr={class:"mb-2"},Pr={__name:"App",setup(e){const t=xe(mr),o=xe([]),n=xe(50),s=xe(0),a=xe([]),i=xe(!1),r=xe(0),l=xe(!1),p=xe(0);let h=null;function f(){h=setInterval(()=>{p.value++},1e3)}function C(){h&&clearInterval(h)}function k(B){const E=Math.floor(B/60),$=B%60;return`${E}:${$.toString().padStart(2,"0")}`}const x=Rs(()=>o.value[s.value]);function O(){l.value=!0,o.value=N([...t.value]).slice(0,n.value),r.value=0,s.value=0,a.value=[],i.value=!1,p.value=0,f()}function J(){const B=x.value.answer,E=a.value;(Array.isArray(B)?D(B.sort(),E.sort()):E.length===1&&E[0]===B)&&r.value++,i.value=!0}function z(){s.value<o.value.length-1?(s.value++,a.value=[],i.value=!1):(C(),s.value++)}function j(){C(),l.value=!1,o.value=[],a.value=[],i.value=!1,p.value=0,r.value=0}function N(B){for(let E=B.length-1;E>0;E--){const $=Math.floor(Math.random()*(E+1));[B[E],B[$]]=[B[$],B[E]]}return B}function D(B,E){return B.length===E.length&&B.every(($,ie)=>$===E[ie])}return ms(()=>C()),(B,E)=>(ge(),Se("div",gr,[E[8]||(E[8]=V("h1",{class:"text-center text-2xl font-bold mb-4"},"QCM Quiz App",-1)),l.value?s.value<o.value.length?(ge(),Se("div",br,[V("div",wr,"  Time Elapsed: "+he(k(p.value)),1),V("div",Ar,[V("strong",null,"Question "+he(s.value+1)+":",1),V("p",vr,he(x.value.question),1)]),(ge(!0),Se(We,null,Ja(x.value.choices,($,ie)=>(ge(),Se("div",{class:"mb-4 space-y-2 !text-left",key:ie},[V("label",Sr,[un(V("input",{type:"checkbox",value:ie,"onUpdate:modelValue":E[1]||(E[1]=et=>a.value=et),disabled:i.value},null,8,Cr),[[cr,a.value]]),V("span",kr,[V("strong",null,he(ie)+".",1),Kt(" "+he($),1)])])]))),128)),V("div",Wr,[i.value?zt("",!0):(ge(),Se("button",{key:0,onClick:J,class:"px-4 py-1 rounded",disabled:a.value.length===0||i.value}," Validate ",8,Ir)),i.value?(ge(),Se("button",{key:1,onClick:z,class:"px-4 py-1 rounded"},he(s.value+1===o.value.length?"Finish":"Next"),1)):zt("",!0)]),i.value?(ge(),Se("div",Tr,[V("p",null,[E[3]||(E[3]=V("strong",null,"Your answer:",-1)),Kt(" "+he(a.value.join(", ")),1)]),V("p",null,[E[4]||(E[4]=V("strong",null,"Correct answer:",-1)),Kt(" "+he(Array.isArray(x.value.answer)?x.value.answer.join(", "):x.value.answer),1)]),x.value.explanation?(ge(),Se("div",Dr,[E[5]||(E[5]=V("strong",null,"Explanation:",-1)),E[6]||(E[6]=V("br",null,null,-1)),V("div",Mr,he(x.value.explanation),1)])):zt("",!0)])):zt("",!0)])):(ge(),Se("div",Er,[E[7]||(E[7]=V("h2",{class:"text-xl font-bold mb-2"},"Quiz Complete!",-1)),V("p",xr,"You scored "+he(r.value)+" out of "+he(o.value.length)+".",1),V("p",null,"Total Time: "+he(k(p.value)),1),V("button",{onClick:j,class:"px-4 py-1 rounded mt-4"},"Restart")])):(ge(),Se("div",yr,[E[2]||(E[2]=V("label",null,"How many questions?",-1)),un(V("input",{"onUpdate:modelValue":E[0]||(E[0]=$=>n.value=$),type:"number",min:"1",class:"border p-1 m-2"},null,512),[[rr,n.value,void 0,{number:!0}]]),V("button",{onClick:O,class:"px-4 py-1 rounded"},"Start")]))]))}};dr(Pr).mount("#app");</script>
    <style rel="stylesheet" crossorigin>/*! tailwindcss v4.1.8 | MIT License | https://tailwindcss.com */@layer properties{@supports (((-webkit-hyphens:none)) and (not (margin-trim:inline))) or ((-moz-orient:inline) and (not (color:rgb(from red r g b)))){*,:before,:after,::backdrop{--tw-space-y-reverse:0;--tw-space-x-reverse:0;--tw-border-style:solid;--tw-font-weight:initial;--tw-blur:initial;--tw-brightness:initial;--tw-contrast:initial;--tw-grayscale:initial;--tw-hue-rotate:initial;--tw-invert:initial;--tw-opacity:initial;--tw-saturate:initial;--tw-sepia:initial;--tw-drop-shadow:initial;--tw-drop-shadow-color:initial;--tw-drop-shadow-alpha:100%;--tw-drop-shadow-size:initial}}}@layer theme{:root,:host{--font-sans:ui-sans-serif,system-ui,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--font-mono:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--color-gray-600:oklch(44.6% .03 256.802);--spacing:.25rem;--container-2xl:42rem;--text-sm:.875rem;--text-sm--line-height:calc(1.25/.875);--text-xl:1.25rem;--text-xl--line-height:calc(1.75/1.25);--text-2xl:1.5rem;--text-2xl--line-height:calc(2/1.5);--font-weight-bold:700;--default-transition-duration:.15s;--default-transition-timing-function:cubic-bezier(.4,0,.2,1);--default-font-family:var(--font-sans);--default-mono-font-family:var(--font-mono)}}@layer base{*,:after,:before,::backdrop{box-sizing:border-box;border:0 solid;margin:0;padding:0}::file-selector-button{box-sizing:border-box;border:0 solid;margin:0;padding:0}html,:host{-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;line-height:1.5;font-family:var(--default-font-family,ui-sans-serif,system-ui,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji");font-feature-settings:var(--default-font-feature-settings,normal);font-variation-settings:var(--default-font-variation-settings,normal);-webkit-tap-highlight-color:transparent}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,samp,pre{font-family:var(--default-mono-font-family,ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace);font-feature-settings:var(--default-mono-font-feature-settings,normal);font-variation-settings:var(--default-mono-font-variation-settings,normal);font-size:1em}small{font-size:80%}sub,sup{vertical-align:baseline;font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}:-moz-focusring{outline:auto}progress{vertical-align:baseline}summary{display:list-item}ol,ul,menu{list-style:none}img,svg,video,canvas,audio,iframe,embed,object{vertical-align:middle;display:block}img,video{max-width:100%;height:auto}button,input,select,optgroup,textarea{font:inherit;font-feature-settings:inherit;font-variation-settings:inherit;letter-spacing:inherit;color:inherit;opacity:1;background-color:#0000;border-radius:0}::file-selector-button{font:inherit;font-feature-settings:inherit;font-variation-settings:inherit;letter-spacing:inherit;color:inherit;opacity:1;background-color:#0000;border-radius:0}:where(select:is([multiple],[size])) optgroup{font-weight:bolder}:where(select:is([multiple],[size])) optgroup option{padding-inline-start:20px}::file-selector-button{margin-inline-end:4px}::placeholder{opacity:1}@supports (not ((-webkit-appearance:-apple-pay-button))) or (contain-intrinsic-size:1px){::placeholder{color:currentColor}@supports (color:color-mix(in lab,red,red)){::placeholder{color:color-mix(in oklab,currentcolor 50%,transparent)}}}textarea{resize:vertical}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-date-and-time-value{min-height:1lh;text-align:inherit}::-webkit-datetime-edit{display:inline-flex}::-webkit-datetime-edit-fields-wrapper{padding:0}::-webkit-datetime-edit{padding-block:0}::-webkit-datetime-edit-year-field{padding-block:0}::-webkit-datetime-edit-month-field{padding-block:0}::-webkit-datetime-edit-day-field{padding-block:0}::-webkit-datetime-edit-hour-field{padding-block:0}::-webkit-datetime-edit-minute-field{padding-block:0}::-webkit-datetime-edit-second-field{padding-block:0}::-webkit-datetime-edit-millisecond-field{padding-block:0}::-webkit-datetime-edit-meridiem-field{padding-block:0}:-moz-ui-invalid{box-shadow:none}button,input:where([type=button],[type=reset],[type=submit]){-webkit-appearance:button;-moz-appearance:button;appearance:button}::file-selector-button{-webkit-appearance:button;-moz-appearance:button;appearance:button}::-webkit-inner-spin-button{height:auto}::-webkit-outer-spin-button{height:auto}[hidden]:where(:not([hidden=until-found])){display:none!important}}@layer components;@layer utilities{.visible{visibility:visible}.static{position:static}.isolate{isolation:isolate}.container{width:100%}@media (min-width:40rem){.container{max-width:40rem}}@media (min-width:48rem){.container{max-width:48rem}}@media (min-width:64rem){.container{max-width:64rem}}@media (min-width:80rem){.container{max-width:80rem}}@media (min-width:96rem){.container{max-width:96rem}}.m-2{margin:calc(var(--spacing)*2)}.mx-auto{margin-inline:auto}.mt-2{margin-top:calc(var(--spacing)*2)}.mt-4{margin-top:calc(var(--spacing)*4)}.mb-2{margin-bottom:calc(var(--spacing)*2)}.mb-4{margin-bottom:calc(var(--spacing)*4)}.ml-1{margin-left:calc(var(--spacing)*1)}.block{display:block}.contents{display:contents}.inline{display:inline}.table{display:table}.max-w-2xl{max-width:var(--container-2xl)}:where(.space-y-2>:not(:last-child)){--tw-space-y-reverse:0;margin-block-start:calc(calc(var(--spacing)*2)*var(--tw-space-y-reverse));margin-block-end:calc(calc(var(--spacing)*2)*calc(1 - var(--tw-space-y-reverse)))}:where(.space-x-2>:not(:last-child)){--tw-space-x-reverse:0;margin-inline-start:calc(calc(var(--spacing)*2)*var(--tw-space-x-reverse));margin-inline-end:calc(calc(var(--spacing)*2)*calc(1 - var(--tw-space-x-reverse)))}.rounded{border-radius:.25rem}.border{border-style:var(--tw-border-style);border-width:1px}.p-1{padding:calc(var(--spacing)*1)}.p-4{padding:calc(var(--spacing)*4)}.px-4{padding-inline:calc(var(--spacing)*4)}.py-1{padding-block:calc(var(--spacing)*1)}.\!text-left{text-align:left!important}.text-center{text-align:center}.text-justify{text-align:justify}.text-left{text-align:left}.text-2xl{font-size:var(--text-2xl);line-height:var(--tw-leading,var(--text-2xl--line-height))}.text-sm{font-size:var(--text-sm);line-height:var(--tw-leading,var(--text-sm--line-height))}.text-xl{font-size:var(--text-xl);line-height:var(--tw-leading,var(--text-xl--line-height))}.font-bold{--tw-font-weight:var(--font-weight-bold);font-weight:var(--font-weight-bold)}.text-gray-600{color:var(--color-gray-600)}.filter{filter:var(--tw-blur,)var(--tw-brightness,)var(--tw-contrast,)var(--tw-grayscale,)var(--tw-hue-rotate,)var(--tw-invert,)var(--tw-saturate,)var(--tw-sepia,)var(--tw-drop-shadow,)}.transition{transition-property:color,background-color,border-color,outline-color,text-decoration-color,fill,stroke,--tw-gradient-from,--tw-gradient-via,--tw-gradient-to,opacity,box-shadow,transform,translate,scale,rotate,filter,-webkit-backdrop-filter,backdrop-filter,display,visibility,content-visibility,overlay,pointer-events;transition-timing-function:var(--tw-ease,var(--default-transition-timing-function));transition-duration:var(--tw-duration,var(--default-transition-duration))}}:root{color-scheme:light dark;color:#ffffffde;font-synthesis:none;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;background-color:#242424;font-family:system-ui,Avenir,Helvetica,Arial,sans-serif;font-weight:400;line-height:1.5}a{color:#646cff;-webkit-text-decoration:inherit;text-decoration:inherit;font-weight:500}a:hover{color:#535bf2}body{place-items:center;min-width:320px;min-height:100vh;margin:0;display:flex}h1{font-size:3.2em;line-height:1.1}button{cursor:pointer;background-color:#1a1a1a;border:1px solid #0000;border-radius:8px;padding:.6em 1.2em;font-family:inherit;font-size:1em;font-weight:500;transition:border-color .25s}button:hover{border-color:#646cff}button:focus,button:focus-visible{outline:4px auto -webkit-focus-ring-color}.card{padding:2em}#app{max-width:1280px;margin:0 auto;padding:2rem}@media (prefers-color-scheme:light){:root{color:#213547;background-color:#fff}a:hover{color:#747bff}button{background-color:#f9f9f9}}@property --tw-space-y-reverse{syntax:"*";inherits:false;initial-value:0}@property --tw-space-x-reverse{syntax:"*";inherits:false;initial-value:0}@property --tw-border-style{syntax:"*";inherits:false;initial-value:solid}@property --tw-font-weight{syntax:"*";inherits:false}@property --tw-blur{syntax:"*";inherits:false}@property --tw-brightness{syntax:"*";inherits:false}@property --tw-contrast{syntax:"*";inherits:false}@property --tw-grayscale{syntax:"*";inherits:false}@property --tw-hue-rotate{syntax:"*";inherits:false}@property --tw-invert{syntax:"*";inherits:false}@property --tw-opacity{syntax:"*";inherits:false}@property --tw-saturate{syntax:"*";inherits:false}@property --tw-sepia{syntax:"*";inherits:false}@property --tw-drop-shadow{syntax:"*";inherits:false}@property --tw-drop-shadow-color{syntax:"*";inherits:false}@property --tw-drop-shadow-alpha{syntax:"<percentage>";inherits:false;initial-value:100%}@property --tw-drop-shadow-size{syntax:"*";inherits:false}body{font-family:sans-serif}</style>
  </head>
  <body>
    <div id="app"></div>
  </body>
</html>
